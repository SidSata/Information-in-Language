{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..') #This line adds '..' to the path so we can import the net_framework python file\n",
    "from net_framework import *\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell allows you to toggle warnings on and off. It helps the readability of the results but warnings in general are important and should not be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show_err=false; \n",
       "function code_toggle_err() {\n",
       " if (code_show_err){\n",
       " $('div.output_stderr').hide();\n",
       " } else {\n",
       " $('div.output_stderr').show();\n",
       " }\n",
       " code_show_err = !code_show_err\n",
       "} \n",
       "$( document ).ready(code_toggle_err);\n",
       "</script>\n",
       "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show_err=false; \n",
    "function code_toggle_err() {\n",
    " if (code_show_err){\n",
    " $('div.output_stderr').hide();\n",
    " } else {\n",
    " $('div.output_stderr').show();\n",
    " }\n",
    " code_show_err = !code_show_err\n",
    "} \n",
    "$( document ).ready(code_toggle_err);\n",
    "</script>\n",
    "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi! This notebook will serve as a tutorial to walk through the basics of this project. In this notebook, we will model the truthfulness of a class of mathematical statements that amount to the addition of two integers between -50 and 50. We need to limit the range of integers because it would take an infinite amount of compute power to fully conceptualize the addition of any two integers with no bounds. \n",
    "\n",
    "To model the information content of the chosen set of statements, we will go through the following procedure-\n",
    "\n",
    "1. Defining a \"sub-language\" to constrain the set of statements. This will require going through the process of formalizing what the set of statements you want to model are and how to interpret them. \n",
    "2. Finding an efficient representation that can be inputted into neural networks for the characters in the language and the statements you want to model. For the most part, we will be using one-hot vectors for this task (https://en.wikipedia.org/wiki/One-hot). \n",
    "3. Generating training and validation datasets for our statements. These datasets should consist of input vectors that represent legal statements in the \"sub-language\" and the associated truth values for each of these statements. \n",
    "4. Use the data to train a set of neural networks with different structures neural complexities. The ones with training error below a certain threshold will be validated. We will be using a threshold value of 0.05 for this notebok. The networks with validation error below that threshold will be classified as \"good\" networks and the \"good\" network with minimal neural complexity will be used as the final model to represent the statements. \n",
    "5. (optional) Try finding the information complexity (the amount data it takes to train the network) of that network structure by training it with different amounts of training data and seeing how much training data is required for it to have training and validation errors below the threshold. \n",
    "6. Save the statistics from the final network. This will include things such as the final neural complexity (the number of hidden layer nodes in the network), information complexity (the amount data it takes to train the network), final validation/training error, final weights of the network and plots of the training error as the number of training cycles increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Defining a Sub-Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a purely conceptual step but is easily the most important and difficult part of this project. We need to define a language to constrain the set of statements we are working with. To do this, we need to define three things:\n",
    "\n",
    "1. The characters in the language\n",
    "2. The way these characters are legally allowed to be combined. \n",
    "3. How to determine the truthfulness of a given statement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, the set of characters we are using can be split into two groups. \n",
    "\n",
    "1. The set of integers from -50 to 50, inclusive. We will call this set, $Z$.\n",
    "2. The symbols '+' and '='.\n",
    "\n",
    "These characters can only legally be combined in a single way.\n",
    "\n",
    "Let $z_1$, $z_2$ and $z_3$ be three elements of $Z$. Then, we define a legal 'sentence' in this language as the following combination of characters: \n",
    "\n",
    "$$z_1 + z_2 = z_3$$.\n",
    "\n",
    "And the veracity of these statements can be understood in the normal way, the statement is true if and only if the first two integers add up to the third. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, it might seem unecessary to model the '+' and '=' symbols since they are in the same positions every time, but this is just a very simple example and modeling these types of symbols will be necessary when dealing with more complex syntaxes in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Finding a Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to represent our language in a way that can be understood by the neural network. To do this, we will be using one-hot vectors (https://en.wikipedia.org/wiki/One-hot) to represent each character in the network. Each vector will have a dimensionality of 103 to represent each of the 101 integers as well the '+' and '=' symbols. \n",
    "\n",
    "We will use the following mapping: \n",
    "\n",
    "'+' maps to [1,0,0,...,0,0]  \n",
    "'=' maps to [0,1,0,...,0,0]\n",
    "'-50' maps to [0,0,1,...,0,0]  \n",
    "...  \n",
    "'49' maps to [0,0,0,...,1,0]  \n",
    "and  \n",
    "'50' maps to [0,0,0,...,0,1]  \n",
    "\n",
    "Each sentence in the langauge can be thought of as a sequence of five of these vectors stacked end to end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generating Training/Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to write functions that will generate training or validation datasets on command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(symbols):\n",
    "    '''\n",
    "    Converts symbol ('+', '=', int. between -50 to 50) array into 'stacked' one-hot vectors as specified above. \n",
    "    '''\n",
    "    vector_stack = []\n",
    "    for symbol in symbols:\n",
    "        vector = np.zeros(103)\n",
    "        if symbol == '+':\n",
    "            vector[0] = 1\n",
    "        elif symbol == '=':\n",
    "            vector[1] = 1\n",
    "        else:\n",
    "            idx = int(symbol) + 52\n",
    "            vector[idx] = 1\n",
    "        vector_stack = np.concatenate((vector_stack, vector))\n",
    "    return np.asarray(vector_stack)\n",
    "    \n",
    "    \n",
    "def gen_data(num_examples):\n",
    "    \n",
    "    '''\n",
    "    Generates statements in this language as well as their veracity.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    num_examples : int\n",
    "        The number of examples in the dataset. \n",
    "    randomize : bool\n",
    "        Whether or not to randomize the output dataset. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : 2D numpy array\n",
    "        Matrix of inputs where each row is a vector that represents a single sentence.\n",
    "    Y : 1D numpy array\n",
    "        Each element 1 if the corresponding statement in X is true and 0 otherwise. \n",
    "    '''\n",
    "    X = []\n",
    "    Y = []\n",
    "    sentences = []\n",
    "    i = 0\n",
    "    while i < num_examples:\n",
    "        if i < num_examples/2:\n",
    "            #Randomly Choosing three integers between -50 and 50\n",
    "            z = np.random.randint(-50, 51, 3)\n",
    "            sentence = np.array([z[0], '+', z[1], '=', z[2]])\n",
    "            sentences.append(sentence)\n",
    "            X.append(one_hot(sentence))\n",
    "            if z[0] + z[1] == z[2]:\n",
    "                Y.append(1)\n",
    "            else:\n",
    "                Y.append(0)\n",
    "        else:\n",
    "            #Choosing values such that the output is true to ensure that our training dataset\n",
    "            #is not skewed with False results.\n",
    "            z = np.random.randint(-50, 51, 2)\n",
    "            z = np.append(z, z[0] + z[1])\n",
    "            #Ensuring the sentence is legal\n",
    "            if z[2] in np.arange(-50, 51, 1):\n",
    "                sentence = np.array([z[0], '+', z[1], '=', z[2]])\n",
    "                sentences.append(sentence)\n",
    "                X.append(one_hot(sentence))\n",
    "                if z[0] + z[1] == z[2]:\n",
    "                    Y.append(1)\n",
    "                else:\n",
    "                    Y.append(0)\n",
    "            else:\n",
    "                i -= 1\n",
    "        i += 1\n",
    "        \n",
    "    return np.asarray(X), np.asarray(Y), np.asarray(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There might be repeats in the training/validation data. This isn't ideal, but \n",
    "#hopefully it won't be a big deal. \n",
    "data = gen_data(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A typical sentence is: ['21' '+' '-24' '=' '-3']\n",
      "The truth value of this sentence is: 1\n",
      "Its one-hot representation is: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "X_train = data[0]\n",
    "Y_train = data[1]\n",
    "sentences = data[2]\n",
    "\n",
    "print('A typical sentence is: ' + str(sentences[6000]))\n",
    "print('The truth value of this sentence is: ' + str(Y_train[6000]))\n",
    "print('Its one-hot representation is: ' + str(X_train[6000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training and Validating the Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to define and train a bunch of different neural networks of various shapes. We will train over 50 iterations with 100 training samples. You will probably have to use much larger values for these. \n",
    "\n",
    "The neural networks we are using will have an input size of 5 * 103 = 515 since that is the size of 5 one-hot vectors stacked on each other. The output size will be 1. We will be trying networks with one hidden layer and varying sizes of 1, 2 and 3, networks with two hidden layers with sizes of [3,2] and [2,1]. We will also be using a learning rate of 0.1 but other learning rates work well as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Shape: (515, [1], 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#49 Loss: 0.0010861869668588042'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Shape: (515, [2], 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#49 Loss: 0.0003228409623261541'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Shape: (515, [3], 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#49 Loss: 0.0003011675726156682'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Shape: (515, [3, 2], 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#49 Loss: 0.0003451051888987422'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Shape: (515, [2, 1], 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#49 Loss: 0.0005270301480777562'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Number of training iterations\n",
    "num_iters = 50\n",
    "#Size of training dataset\n",
    "num_examples = 100\n",
    "#Acquiring Training Data\n",
    "training_data = gen_data(num_examples)\n",
    "X = training_data[0]\n",
    "Y = training_data[1]\n",
    "#Converting training data into pytorch format\n",
    "X = torch.from_numpy(X).float()\n",
    "Y = torch.from_numpy(Y).float()\n",
    "#Listing out the shapes of each model\n",
    "network_shapes = [(515, [1], 1), (515, [2], 1), (515, [3], 1), (515, [3,2], 1), (515, [2,1], 1)]\n",
    "#Learning rate of the network\n",
    "rate = 0.1\n",
    "#Array of losses over training period for each network\n",
    "for net_num, shape in enumerate(network_shapes):\n",
    "    print('Network Shape: ' + str(shape), flush = True)\n",
    "    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2], hiddenSize = shape[1] , learning_rate = rate)\n",
    "    for i in range(num_iters):\n",
    "        loss = torch.mean((Y - NN(X))**2).item()\n",
    "        if i == 0: \n",
    "            dh = display(\"#\" + str(i) + \" Loss: \" + str(loss), display_id=True)\n",
    "        else:\n",
    "            dh.update(\"#\" + str(i) + \" Loss: \" + str(loss))\n",
    "        \n",
    "        NN.train(X, Y)\n",
    "        \n",
    "    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n",
    "    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the way we are storing and loading these networks, the validation dataset has to be the same size as the training dataset. We will validate the network 100 times and display the resulting mean/standard deviation. We will be using a threshold error of 0.05 for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Shape: (515, [1], 1)\n",
      "The validation error is: mean = 0.019764089956879616 and std = 0.0\n",
      "Network Shape: (515, [2], 1)\n",
      "The validation error is: mean = 0.019605722278356552 and std = 0.0\n",
      "Network Shape: (515, [3], 1)\n",
      "The validation error is: mean = 0.019608907401561737 and std = 0.0\n",
      "Network Shape: (515, [3, 2], 1)\n",
      "The validation error is: mean = 0.01960284821689129 and std = 0.0\n",
      "Network Shape: (515, [2, 1], 1)\n",
      "The validation error is: mean = 0.019607605412602425 and std = 0.0\n"
     ]
    }
   ],
   "source": [
    "#Generating validation data\n",
    "validation_data = gen_data(num_examples)\n",
    "X = validation_data[0]\n",
    "Y = validation_data[1]\n",
    "#Converting validation data into pytorch format\n",
    "X = torch.from_numpy(X).float()\n",
    "Y = torch.from_numpy(Y).float()\n",
    "#Array of validation error means\n",
    "validation_error_means = []\n",
    "#Array of validation error stds\n",
    "validation_error_stds = []\n",
    "for net_num, shape in enumerate(network_shapes):\n",
    "    print('Network Shape: ' + str(shape), flush = True)\n",
    "    #Loading the network we trained in the prev. section\n",
    "    NN = torch.load(\"saved_networks/Net \" + str(net_num))\n",
    "    validation_errors = []\n",
    "    for i in range(100):\n",
    "        validation_err = torch.mean((Y - NN(X))**2).item()\n",
    "        validation_errors.append(validation_err)\n",
    "    mean, var = sp.describe(validation_errors)[2:4]\n",
    "    std = np.sqrt(var)\n",
    "    validation_error_means.append(mean)\n",
    "    validation_error_stds.append(std)\n",
    "    print(\"The validation error is: mean = \" + str(mean) + \" and std = \" + str(std), flush = True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, due to the simplicity of this tasks, all our examples are below the 0.05 validation error threshold. Therefore, we can say the \"information content\" of adding two integers between -50 and 50 is 1. This value is meaningless on its own but can become interesting when comparing it between statements and languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Step 5: Attempt to Determine Information Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will quantify the information complexity of modeling this addition by figuring out how many training examples is required by our minimal neural complexity network to reach the threshold validation error of 0.05. To do this, we will retrain our example network with one hidden node and varying amounts of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Shape: (515, [2, 1], 1)\n",
      "Number of Examples: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#178 Validation Error: 0.04905067577958107'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#109 Validation Error: 0.049917827785015106'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#39 Validation Error: 0.04678682877123356'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#45 Validation Error: 0.04785569289326668'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#39 Validation Error: 0.04965037626028061'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#List of varying amounts of training examples\n",
    "num_examples_arr = np.arange(1, 6, 1)\n",
    "#Validation Error Threshold\n",
    "threshold = 0.05\n",
    "print('Network Shape: ' + str(shape), flush = True)\n",
    "for num_examples in num_examples_arr:\n",
    "    #Acquiring Training Data\n",
    "    training_data = gen_data(num_examples)\n",
    "    X = training_data[0]\n",
    "    Y = training_data[1]\n",
    "    #Converting training data into pytorch format\n",
    "    X = torch.from_numpy(X).float()\n",
    "    Y = torch.from_numpy(Y).float()\n",
    "    #Winning Network Shape\n",
    "    shape = (515, [1], 1)\n",
    "    #Learning rate of the network\n",
    "    rate = 0.1\n",
    "    #Number of training examples\n",
    "    print('Number of Examples: ' + str(num_examples), flush = True)\n",
    "    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2], hiddenSize = shape[1] , learning_rate = rate)\n",
    "    validation_err = 1\n",
    "    i = 0\n",
    "    while validation_err > 0.05 and i < 10000:       \n",
    "        #Averaging validation error over 100 tries (so outliers do not skew results)\n",
    "        validation_errors = []\n",
    "        for j in range(250):\n",
    "            #Generating validation data\n",
    "            validation_data = gen_data(num_examples)\n",
    "            X_val = validation_data[0]\n",
    "            Y_val = validation_data[1]\n",
    "            #Converting validation data into pytorch format\n",
    "            X_val = torch.from_numpy(X_val).float()\n",
    "            Y_val = torch.from_numpy(Y_val).float()\n",
    "        \n",
    "            err = torch.mean((Y_val - NN(X_val))**2).item()\n",
    "            validation_errors.append(err)\n",
    "        validation_err = sp.describe(validation_errors)[2]\n",
    "        if i == 0: \n",
    "            dh = display(\"#\" + str(i) + \" Validation Error: \" + str(validation_err), display_id=True)\n",
    "        else:\n",
    "            dh.update(\"#\" + str(i) + \" Validation Error: \" + str(validation_err))\n",
    "        NN.train(X, Y)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we only need 1 training example for the network to learn what is going on with high precision!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see a trend here where the network learns more quickly with a higher number of training examples, which is expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save and Discuss Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this notebook really only takes around a minute to run and is a tutorial, there is no reason for me to save all the results. However, I expect future results to be saved for this project and displayed in the form of graphics or a discussion section which you can use to present your results to me and/or the groups. Formatting code in a Jupyter Notebook the way I have here is preferred, but may not always be possible for more complicated systems. \n",
    "\n",
    "From these results, we see that addition is an extremely simple and easy task for a neural network to learn. In the future, I would want to explore how this compares with the results from learning other arithmetic operations as well as learning these operations simultaneously. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
