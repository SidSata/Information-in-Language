{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "sys.path.insert(0, '..')\n",
    "from net_framework import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Lnum</th>\n",
       "      <th>#snum</th>\n",
       "      <th>#cnum</th>\n",
       "      <th>Term Abbrev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>LE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>WK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>LF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #Lnum  #snum  #cnum Term Abbrev\n",
       "0      1      1      1          LB\n",
       "1      1      1      2          LB\n",
       "2      1      1      3          LE\n",
       "3      1      1      4          WK\n",
       "4      1      1      5          LF"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\n",
    "term_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\n",
    "term_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#cnum</th>\n",
       "      <th>Normalized-L</th>\n",
       "      <th>Normalized-a</th>\n",
       "      <th>Normalized-b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025675</td>\n",
       "      <td>-0.138822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>274</td>\n",
       "      <td>0.876301</td>\n",
       "      <td>-0.025504</td>\n",
       "      <td>-0.138822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129</td>\n",
       "      <td>0.876301</td>\n",
       "      <td>0.070445</td>\n",
       "      <td>-0.106039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230</td>\n",
       "      <td>0.876301</td>\n",
       "      <td>0.070101</td>\n",
       "      <td>-0.089951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>302</td>\n",
       "      <td>0.876301</td>\n",
       "      <td>0.070617</td>\n",
       "      <td>-0.072041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #cnum  Normalized-L  Normalized-a  Normalized-b\n",
       "0    141      1.000000     -0.025675     -0.138822\n",
       "1    274      0.876301     -0.025504     -0.138822\n",
       "2    129      0.876301      0.070445     -0.106039\n",
       "3    230      0.876301      0.070101     -0.089951\n",
       "4    302      0.876301      0.070617     -0.072041"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\n",
    "locations = cnum_data[['#cnum']]\n",
    "locations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\n",
    "locations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\n",
    "locations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\n",
    "display(locations.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = locations.sort_values('#cnum')\n",
    "chip_num = list(locations['#cnum'])\n",
    "lab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = term_data[term_data.get('#Lnum').eq(1)]\n",
    "unique_symbols = list(l1['Term Abbrev'].unique())\n",
    "l1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\n",
    "l1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         LB    LE    WK    LF     F     G     S   GB    FU\n",
      "#cnum                                                     \n",
      "1      0.36  0.00  0.00  0.04  0.08  0.52  0.00  0.0  0.00\n",
      "2      0.12  0.00  0.04  0.12  0.60  0.04  0.00  0.0  0.08\n",
      "3      0.00  0.96  0.04  0.00  0.00  0.00  0.00  0.0  0.00\n",
      "4      0.28  0.00  0.40  0.00  0.08  0.04  0.20  0.0  0.00\n",
      "5      0.04  0.00  0.00  0.20  0.68  0.08  0.00  0.0  0.00\n",
      "...     ...   ...   ...   ...   ...   ...   ...  ...   ...\n",
      "326    0.08  0.20  0.36  0.00  0.20  0.00  0.12  0.0  0.04\n",
      "327    0.04  0.00  0.00  0.68  0.20  0.04  0.04  0.0  0.00\n",
      "328    0.64  0.00  0.08  0.00  0.00  0.20  0.08  0.0  0.00\n",
      "329    0.08  0.08  0.40  0.00  0.12  0.00  0.28  0.0  0.04\n",
      "330    0.04  0.00  0.00  0.72  0.24  0.00  0.00  0.0  0.00\n",
      "\n",
      "[330 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\n",
    "l1_result.index += 1\n",
    "l1_result.index.name = '#cnum'\n",
    "l1_result.columns = unique_symbols\n",
    "print(l1_result)\n",
    "chip_norm = []\n",
    "#pull the percentage for each cnum\n",
    "for x in chip_num:\n",
    "    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_num = range(1, 25, 1)\n",
    "layer_num = range(1,3)\n",
    "def mirror(arr, desired_length):\n",
    "    if desired_length/len(arr) == 2:\n",
    "        return arr + arr[::-1]\n",
    "    else:\n",
    "        return arr[:-1] + arr[::-1]\n",
    "\n",
    "shape_collection = []\n",
    "for node in node_num:\n",
    "    for layer in layer_num:\n",
    "        shape = []\n",
    "        # it gets stuck on this particular one for some reason\n",
    "        if layer == 1:\n",
    "            shape = [node]\n",
    "            shape_collection.append(shape)\n",
    "        elif node//layer < 3:\n",
    "            continue\n",
    "        elif layer%2 == 0:\n",
    "            # you are not leaving until you give me the right number\n",
    "            while sum(shape) != node/2:\n",
    "                shape = []\n",
    "                # let the code randomly assign things that add up to what we want\n",
    "                for l in range (0,int(layer/2)) :\n",
    "                    shape.append(int(random.uniform(3,node+1)))\n",
    "            \n",
    "            shape = mirror(shape, layer)\n",
    "            shape_collection.append(shape)\n",
    "        else:\n",
    "            while sum(shape) != node:\n",
    "                shape = []\n",
    "                # similar logic, but we let it mirror itself first\n",
    "                for l in range (0, math.ceil(layer/2)) :\n",
    "                    shape.append(random.randint(2,node))\n",
    "                shape = mirror(shape, layer)    \n",
    "            shape_collection.append(shape)\n",
    "\n",
    "print(shape_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally designed to find every single permutation\n",
    "Oops misread directions\n",
    "\n",
    "\n",
    "def check(arr):\n",
    "    for a in arr:\n",
    "        if a < 3:\n",
    "            return False\n",
    "    inverse = arr[::-1]\n",
    "    if arr == inverse:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "shape_collection = []\n",
    "\n",
    "def trickle_down(node, layer):\n",
    "    if layer > 1:\n",
    "        for i in range(0, node[-1]):\n",
    "            new_arr = node[:-1] + [node[-1]-i] + [i]\n",
    "            trickle_down(new_arr,layer-1)\n",
    "    else:\n",
    "        if check(node):\n",
    "            global shape_collection\n",
    "            shape_collection.append(node)\n",
    "\n",
    "for node in node_num:\n",
    "    for layer in layer_num:\n",
    "        trickle_down([node], layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of training iterations\n",
    "num_iters = 500\n",
    "\n",
    "#Listing out the shapes of each model\n",
    "colors_num = len(chip_norm[0])\n",
    "input_size = 3\n",
    "\n",
    "network_shapes = []\n",
    "for s in shape_collection:\n",
    "    network_shapes.append((input_size,s,colors_num))\n",
    "\n",
    "#Learning rate of the network\n",
    "rate = 0.001\n",
    "\n",
    "#Generating Training Data\n",
    "input_train, output_train, input_test, output_test = 0, 0, 0, 0\n",
    "def shuffle():\n",
    "    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1, random_state = 3, shuffle = True)\n",
    "    #test run on whole dataset\n",
    "    #lab_train, lab_test = lab_norm, lab_norm\n",
    "    #chip_train, chip_test = chip_norm, chip_norm\n",
    "    \n",
    "    global input_train, output_train, input_test, output_test\n",
    "    input_train = torch.FloatTensor(lab_train)\n",
    "    output_train = torch.FloatTensor(chip_train)\n",
    "    input_test= torch.FloatTensor(lab_test)\n",
    "    output_test = torch.FloatTensor(chip_test)\n",
    "\n",
    "print(network_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array of losses over training period for each network\n",
    "output_file = {}\n",
    "for n in node_num:\n",
    "    output_file[n] = {}\n",
    "    \n",
    "for net_num, shape in enumerate(network_shapes):\n",
    "    shuffle()\n",
    "    print(\"Training: \",shape)\n",
    "    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n",
    "                        hiddenSize = shape[1] , learning_rate = rate)\n",
    "    error_arr = []\n",
    "    prev_error = 0\n",
    "    strike = 0\n",
    "    \n",
    "    for i in range(num_iters):       \n",
    "        NN.train(input_train, output_train)\n",
    "        validation_error = NN.l1error(output_test, NN(input_test))\n",
    "        #zero mistake counter at new training\n",
    "        if i == 0:\n",
    "            strike = 0\n",
    "        #adding error to array\n",
    "        error_arr.append(validation_error)\n",
    "        #wait for them to grow up\n",
    "        if prev_error < validation_error and i > 100:\n",
    "            if strike > 10:\n",
    "                print(\"Complete at iteration \", i, \"\\nFinal error: \", min(error_arr), \"\\n\")\n",
    "                break\n",
    "            else:\n",
    "                strike += 1\n",
    "        prev_error = validation_error\n",
    "\n",
    "    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n",
    "    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n",
    "    output_file[sum(shape[1])][len(shape[1])] = error_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('validation_errors.json', 'w') as f:\n",
    "    json.dump(output_file, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "277px",
    "left": "800px",
    "right": "20px",
    "top": "69px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
