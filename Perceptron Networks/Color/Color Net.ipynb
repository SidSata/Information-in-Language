{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "sys.path.insert(0, '..')\n",
    "from net_framework import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Lnum</th>\n",
       "      <th>#snum</th>\n",
       "      <th>#cnum</th>\n",
       "      <th>Term Abbrev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>LE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>WK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>LF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #Lnum  #snum  #cnum Term Abbrev\n",
       "0      1      1      1          LB\n",
       "1      1      1      2          LB\n",
       "2      1      1      3          LE\n",
       "3      1      1      4          WK\n",
       "4      1      1      5          LF"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\n",
    "term_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\n",
    "term_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-f811cbef4bec>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  locations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\n",
      "<ipython-input-30-f811cbef4bec>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  locations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\n",
      "<ipython-input-30-f811cbef4bec>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  locations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#cnum</th>\n",
       "      <th>Normalized-L</th>\n",
       "      <th>Normalized-a</th>\n",
       "      <th>Normalized-b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025675</td>\n",
       "      <td>-0.138822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>274</td>\n",
       "      <td>0.876301</td>\n",
       "      <td>-0.025504</td>\n",
       "      <td>-0.138822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129</td>\n",
       "      <td>0.876301</td>\n",
       "      <td>0.070445</td>\n",
       "      <td>-0.106039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230</td>\n",
       "      <td>0.876301</td>\n",
       "      <td>0.070101</td>\n",
       "      <td>-0.089951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>302</td>\n",
       "      <td>0.876301</td>\n",
       "      <td>0.070617</td>\n",
       "      <td>-0.072041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #cnum  Normalized-L  Normalized-a  Normalized-b\n",
       "0    141      1.000000     -0.025675     -0.138822\n",
       "1    274      0.876301     -0.025504     -0.138822\n",
       "2    129      0.876301      0.070445     -0.106039\n",
       "3    230      0.876301      0.070101     -0.089951\n",
       "4    302      0.876301      0.070617     -0.072041"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\n",
    "locations = cnum_data[['#cnum']]\n",
    "locations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\n",
    "locations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\n",
    "locations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\n",
    "display(locations.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = locations.sort_values('#cnum')\n",
    "chip_num = list(locations['#cnum'])\n",
    "lab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = term_data[term_data.get('#Lnum').eq(2)]\n",
    "unique_symbols = list(l1['Term Abbrev'].unique())\n",
    "l1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\n",
    "l1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             YN        NR        TK        EP        KA        AA        IT  \\\n",
      "#cnum                                                                         \n",
      "2      0.208333  0.000000  0.166667  0.000000  0.000000  0.000000  0.458333   \n",
      "3      0.416667  0.000000  0.041667  0.208333  0.000000  0.041667  0.125000   \n",
      "4      0.000000  1.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "5      0.000000  0.083333  0.458333  0.000000  0.000000  0.041667  0.083333   \n",
      "6      0.375000  0.000000  0.041667  0.041667  0.000000  0.041667  0.458333   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "327    0.000000  0.000000  0.000000  0.041667  0.000000  0.708333  0.000000   \n",
      "328    0.000000  0.000000  0.041667  0.875000  0.000000  0.000000  0.041667   \n",
      "329    0.583333  0.000000  0.250000  0.000000  0.000000  0.000000  0.041667   \n",
      "330    0.041667  0.083333  0.208333  0.000000  0.083333  0.125000  0.000000   \n",
      "331    0.083333  0.000000  0.041667  0.833333  0.000000  0.000000  0.041667   \n",
      "\n",
      "              *        AT        IR  ...        MP   KT        KM   KU   TO  \\\n",
      "#cnum                                ...                                      \n",
      "2      0.041667  0.000000  0.041667  ...  0.000000  0.0  0.000000  0.0  0.0   \n",
      "3      0.000000  0.041667  0.000000  ...  0.041667  0.0  0.000000  0.0  0.0   \n",
      "4      0.000000  0.000000  0.000000  ...  0.000000  0.0  0.000000  0.0  0.0   \n",
      "5      0.083333  0.000000  0.041667  ...  0.000000  0.0  0.000000  0.0  0.0   \n",
      "6      0.000000  0.000000  0.000000  ...  0.000000  0.0  0.000000  0.0  0.0   \n",
      "...         ...       ...       ...  ...       ...  ...       ...  ...  ...   \n",
      "327    0.083333  0.000000  0.000000  ...  0.083333  0.0  0.000000  0.0  0.0   \n",
      "328    0.000000  0.000000  0.000000  ...  0.000000  0.0  0.000000  0.0  0.0   \n",
      "329    0.000000  0.000000  0.000000  ...  0.000000  0.0  0.000000  0.0  0.0   \n",
      "330    0.041667  0.083333  0.000000  ...  0.041667  0.0  0.041667  0.0  0.0   \n",
      "331    0.000000  0.000000  0.000000  ...  0.000000  0.0  0.000000  0.0  0.0   \n",
      "\n",
      "             TI        KH        UN   AI        YR  \n",
      "#cnum                                               \n",
      "2      0.000000  0.000000  0.000000  0.0  0.000000  \n",
      "3      0.000000  0.000000  0.000000  0.0  0.000000  \n",
      "4      0.000000  0.000000  0.000000  0.0  0.000000  \n",
      "5      0.000000  0.000000  0.000000  0.0  0.000000  \n",
      "6      0.000000  0.000000  0.000000  0.0  0.000000  \n",
      "...         ...       ...       ...  ...       ...  \n",
      "327    0.041667  0.000000  0.000000  0.0  0.000000  \n",
      "328    0.000000  0.000000  0.000000  0.0  0.000000  \n",
      "329    0.000000  0.000000  0.000000  0.0  0.000000  \n",
      "330    0.000000  0.041667  0.041667  0.0  0.041667  \n",
      "331    0.000000  0.000000  0.000000  0.0  0.000000  \n",
      "\n",
      "[330 rows x 28 columns]\n"
     ]
    },
    {
     "ename": "IndexingError",
     "evalue": "Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-0230e62a95bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#pull the percentage for each cnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchip_num\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mchip_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"#cnum\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getbool_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;31m# caller is responsible for ensuring non-None axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m         \u001b[0minds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36mcheck_bool_indexer\u001b[0;34m(index, key)\u001b[0m\n\u001b[1;32m   2264\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2266\u001b[0;31m             raise IndexingError(\n\u001b[0m\u001b[1;32m   2267\u001b[0m                 \u001b[0;34m\"Unalignable boolean Series provided as \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2268\u001b[0m                 \u001b[0;34m\"indexer (index of the boolean Series and of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexingError\u001b[0m: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match)."
     ]
    }
   ],
   "source": [
    "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\n",
    "l1_result.index += 2\n",
    "l1_result.index.name = '#cnum'\n",
    "l1_result.columns = unique_symbols\n",
    "print(l1_result)\n",
    "chip_norm = []\n",
    "#pull the percentage for each cnum\n",
    "for x in chip_num:\n",
    "    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_num = range(1,25)\n",
    "layer_num = range(1,4)\n",
    "\n",
    "\n",
    "shape_collection = []\n",
    "\n",
    "def trickle(arr, iteration_left, check):\n",
    "    if iteration_left == 0:\n",
    "        global shape_collection\n",
    "        #running the int fxn to make sure we don't have floats\n",
    "        mp = map(int, arr)\n",
    "        x = list(mp)\n",
    "        if check == sum(x):\n",
    "            shape_collection.append(x)\n",
    "    else:\n",
    "        new_arr = [0]+ arr + [0]\n",
    "        #recursively expanding the list symmetrically\n",
    "        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n",
    "            new_arr[0] += 1\n",
    "            new_arr[1] -= 1\n",
    "            new_arr[-1] += 1\n",
    "            new_arr[-2] -= 1\n",
    "        trickle(new_arr, iteration_left - 1, check)\n",
    "\n",
    "for node in node_num:\n",
    "    for layer in layer_num:\n",
    "        if node//layer < 3:\n",
    "            continue\n",
    "        if layer%2 == 0:\n",
    "            trickle([node/2, node/2], (layer-2)/2, node)\n",
    "        else:\n",
    "            trickle([node], (layer-1)/2, node)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "print(shape_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Originally designed to find every single permutation\n",
    "#Oops misread directions\n",
    "node_num = range(10, 130, 10)\n",
    "layer_num = range(1,6)\n",
    "\n",
    "def check(arr):\n",
    "    for a in arr:\n",
    "        if a < 3:\n",
    "            return False\n",
    "    inverse = arr[::-1]\n",
    "    if arr == inverse:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "shape_collection = []\n",
    "\n",
    "def trickle_down(node, layer):\n",
    "    if layer > 1:\n",
    "        for i in range(0, node[-1]):\n",
    "            new_arr = node[:-1] + [node[-1]-i] + [i]\n",
    "            trickle_down(new_arr,layer-1)\n",
    "    else:\n",
    "        if check(node):\n",
    "            global shape_collection\n",
    "            shape_collection.append([sum(node), len(node),node])\n",
    "\n",
    "for node in node_num:\n",
    "    for layer in layer_num:\n",
    "        trickle_down([node], layer)\n",
    "\n",
    "print(shape_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of training iterations\n",
    "num_iters = 500\n",
    "\n",
    "#Listing out the shapes of each model\n",
    "colors_num = len(chip_norm[0])\n",
    "input_size = 3\n",
    "\n",
    "network_shapes = []\n",
    "for s in shape_collection:\n",
    "    network_shapes.append((input_size,s,colors_num))\n",
    "\n",
    "#Learning rate of the network\n",
    "rate = 0.001\n",
    "\n",
    "#Generating Training Data\n",
    "input_train, output_train, input_test, output_test = 0, 0, 0, 0\n",
    "def shuffle():\n",
    "    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n",
    "    #test run on whole dataset\n",
    "    #lab_train, lab_test = lab_norm, lab_norm\n",
    "    #chip_train, chip_test = chip_norm, chip_norm\n",
    "    \n",
    "    global input_train, output_train, input_test, output_test\n",
    "    input_train = torch.FloatTensor(lab_train)\n",
    "    output_train = torch.FloatTensor(chip_train)\n",
    "    input_test= torch.FloatTensor(lab_test)\n",
    "    output_test = torch.FloatTensor(chip_test)\n",
    "\n",
    "print(network_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array of losses over training period for each network\n",
    "output_file = {}\n",
    "for n in node_num:\n",
    "    output_file[n] = {}\n",
    "    \n",
    "for net_num, shape in enumerate(network_shapes):\n",
    "    shuffle()\n",
    "    print(\"Training: \",shape)\n",
    "    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n",
    "                        hiddenSize = shape[1] , learning_rate = rate)\n",
    "    error_arr = []\n",
    "    prev_error = 0\n",
    "    strike = 0\n",
    "    \n",
    "    for i in range(num_iters):       \n",
    "        NN.train(input_train, output_train)\n",
    "        validation_error = NN.l1error(output_test, NN(input_test))\n",
    "        #zero mistake counter at new training\n",
    "        if i == 0:\n",
    "            strike = 0\n",
    "        #adding error to array\n",
    "        error_arr.append(validation_error)\n",
    "        #wait for them to grow up\n",
    "        if prev_error < validation_error and i > 100:\n",
    "            if strike > 3:\n",
    "                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n",
    "                break\n",
    "            else:\n",
    "                strike += 1\n",
    "        prev_error = validation_error\n",
    "\n",
    "    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n",
    "    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n",
    "    output_file[sum(shape[1])][len(shape[1])] = error_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('validation_errors.json', 'w') as f:\n",
    "    json.dump(output_file, f)"
   ]
  }
 ],
 "metadata": {
  "history": [
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "25416a549cd04f4487e7bd5f8618d338",
    "idx": 0,
    "time": "2021-01-20T18:41:05.939Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "85a33b45f61648a08e12c86fe4413df7",
    "idx": 1,
    "time": "2021-01-20T18:41:05.946Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "4fd00db43d74435dba582a93ac732f39",
    "idx": 2,
    "time": "2021-01-20T18:41:05.948Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "1d186efc421a4fff8972cbc8e044bfdb",
    "idx": 3,
    "time": "2021-01-20T18:41:05.950Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "797406f9da5c4e788dd5f6fe416ecec9",
    "idx": 4,
    "time": "2021-01-20T18:41:05.953Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "e7dff5ca819048459a6ae624e6d6ce7b",
    "idx": 5,
    "time": "2021-01-20T18:41:05.954Z",
    "type": "execution"
   },
   {
    "id": "25416a549cd04f4487e7bd5f8618d338",
    "time": "2021-01-20T18:41:11.110Z",
    "type": "completion"
   },
   {
    "id": "85a33b45f61648a08e12c86fe4413df7",
    "time": "2021-01-20T18:41:11.504Z",
    "type": "completion"
   },
   {
    "id": "4fd00db43d74435dba582a93ac732f39",
    "time": "2021-01-20T18:41:11.585Z",
    "type": "completion"
   },
   {
    "id": "1d186efc421a4fff8972cbc8e044bfdb",
    "time": "2021-01-20T18:41:11.587Z",
    "type": "completion"
   },
   {
    "id": "797406f9da5c4e788dd5f6fe416ecec9",
    "time": "2021-01-20T18:41:11.595Z",
    "type": "completion"
   },
   {
    "id": "e7dff5ca819048459a6ae624e6d6ce7b",
    "time": "2021-01-20T18:41:11.871Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "25416a549cd04f4487e7bd5f8618d338",
    "idx": 0,
    "time": "2021-01-20T18:42:10.430Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "85a33b45f61648a08e12c86fe4413df7",
    "idx": 1,
    "time": "2021-01-20T18:42:10.435Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "4fd00db43d74435dba582a93ac732f39",
    "idx": 2,
    "time": "2021-01-20T18:42:10.436Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "1d186efc421a4fff8972cbc8e044bfdb",
    "idx": 3,
    "time": "2021-01-20T18:42:10.437Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "797406f9da5c4e788dd5f6fe416ecec9",
    "idx": 4,
    "time": "2021-01-20T18:42:10.438Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "e7dff5ca819048459a6ae624e6d6ce7b",
    "idx": 5,
    "time": "2021-01-20T18:42:10.439Z",
    "type": "execution"
   },
   {
    "code": "#Originally designed to find every single permutation\n#Oops misread directions\nnode_num = range(10, 130, 10)\nlayer_num = range(1,6)\n\ndef check(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\nshape_collection = []\n\ndef trickle_down(node, layer):\n    if layer > 1:\n        for i in range(0, node[-1]):\n            new_arr = node[:-1] + [node[-1]-i] + [i]\n            trickle_down(new_arr,layer-1)\n    else:\n        if check(node):\n            global shape_collection\n            shape_collection.append(sum(node), len(node),node)\n\nfor node in node_num:\n    for layer in layer_num:\n        trickle_down([node], layer)\n\nprint(shape_collection)",
    "id": "413ad261276748d89dd277b8fabceed0",
    "idx": 7,
    "time": "2021-01-20T18:42:10.446Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "2881ec8a2c8b4d1c89126e11a80dfb8b",
    "idx": 8,
    "time": "2021-01-20T18:42:10.447Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "82de63d5a1c94978ab0eb3b95bcb444d",
    "idx": 9,
    "time": "2021-01-20T18:42:10.448Z",
    "type": "execution"
   },
   {
    "code": "with open('validation_errors.json', 'w') as f:\n    json.dump(output_file, f)",
    "id": "2941038e293a41a39f99076758f6b743",
    "idx": 10,
    "time": "2021-01-20T18:42:10.449Z",
    "type": "execution"
   },
   {
    "id": "25416a549cd04f4487e7bd5f8618d338",
    "time": "2021-01-20T18:42:10.515Z",
    "type": "completion"
   },
   {
    "id": "85a33b45f61648a08e12c86fe4413df7",
    "time": "2021-01-20T18:42:10.777Z",
    "type": "completion"
   },
   {
    "id": "4fd00db43d74435dba582a93ac732f39",
    "time": "2021-01-20T18:42:10.813Z",
    "type": "completion"
   },
   {
    "id": "1d186efc421a4fff8972cbc8e044bfdb",
    "time": "2021-01-20T18:42:10.815Z",
    "type": "completion"
   },
   {
    "id": "797406f9da5c4e788dd5f6fe416ecec9",
    "time": "2021-01-20T18:42:10.859Z",
    "type": "completion"
   },
   {
    "id": "e7dff5ca819048459a6ae624e6d6ce7b",
    "time": "2021-01-20T18:42:11.109Z",
    "type": "completion"
   },
   {
    "id": "413ad261276748d89dd277b8fabceed0",
    "time": "2021-01-20T18:42:11.246Z",
    "type": "completion"
   },
   {
    "id": "2881ec8a2c8b4d1c89126e11a80dfb8b",
    "time": "2021-01-20T18:42:11.291Z",
    "type": "completion"
   },
   {
    "id": "82de63d5a1c94978ab0eb3b95bcb444d",
    "time": "2021-01-20T18:42:11.292Z",
    "type": "completion"
   },
   {
    "id": "2941038e293a41a39f99076758f6b743",
    "time": "2021-01-20T18:42:11.293Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "25416a549cd04f4487e7bd5f8618d338",
    "idx": 0,
    "time": "2021-01-20T18:42:23.296Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "85a33b45f61648a08e12c86fe4413df7",
    "idx": 1,
    "time": "2021-01-20T18:42:23.298Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "4fd00db43d74435dba582a93ac732f39",
    "idx": 2,
    "time": "2021-01-20T18:42:23.299Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "1d186efc421a4fff8972cbc8e044bfdb",
    "idx": 3,
    "time": "2021-01-20T18:42:23.300Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "797406f9da5c4e788dd5f6fe416ecec9",
    "idx": 4,
    "time": "2021-01-20T18:42:23.300Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "e7dff5ca819048459a6ae624e6d6ce7b",
    "idx": 5,
    "time": "2021-01-20T18:42:23.301Z",
    "type": "execution"
   },
   {
    "id": "25416a549cd04f4487e7bd5f8618d338",
    "time": "2021-01-20T18:42:23.351Z",
    "type": "completion"
   },
   {
    "id": "85a33b45f61648a08e12c86fe4413df7",
    "time": "2021-01-20T18:42:23.680Z",
    "type": "completion"
   },
   {
    "id": "4fd00db43d74435dba582a93ac732f39",
    "time": "2021-01-20T18:42:23.754Z",
    "type": "completion"
   },
   {
    "id": "1d186efc421a4fff8972cbc8e044bfdb",
    "time": "2021-01-20T18:42:23.760Z",
    "type": "completion"
   },
   {
    "id": "797406f9da5c4e788dd5f6fe416ecec9",
    "time": "2021-01-20T18:42:23.775Z",
    "type": "completion"
   },
   {
    "id": "e7dff5ca819048459a6ae624e6d6ce7b",
    "time": "2021-01-20T18:42:24.101Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "25416a549cd04f4487e7bd5f8618d338",
    "idx": 0,
    "time": "2021-01-20T18:42:40.686Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "85a33b45f61648a08e12c86fe4413df7",
    "idx": 1,
    "time": "2021-01-20T18:42:40.689Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "4fd00db43d74435dba582a93ac732f39",
    "idx": 2,
    "time": "2021-01-20T18:42:40.692Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "1d186efc421a4fff8972cbc8e044bfdb",
    "idx": 3,
    "time": "2021-01-20T18:42:40.694Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "797406f9da5c4e788dd5f6fe416ecec9",
    "idx": 4,
    "time": "2021-01-20T18:42:40.696Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "e7dff5ca819048459a6ae624e6d6ce7b",
    "idx": 5,
    "time": "2021-01-20T18:42:40.697Z",
    "type": "execution"
   },
   {
    "id": "25416a549cd04f4487e7bd5f8618d338",
    "time": "2021-01-20T18:42:40.763Z",
    "type": "completion"
   },
   {
    "id": "85a33b45f61648a08e12c86fe4413df7",
    "time": "2021-01-20T18:42:41.031Z",
    "type": "completion"
   },
   {
    "id": "4fd00db43d74435dba582a93ac732f39",
    "time": "2021-01-20T18:42:41.060Z",
    "type": "completion"
   },
   {
    "id": "1d186efc421a4fff8972cbc8e044bfdb",
    "time": "2021-01-20T18:42:41.062Z",
    "type": "completion"
   },
   {
    "id": "797406f9da5c4e788dd5f6fe416ecec9",
    "time": "2021-01-20T18:42:41.096Z",
    "type": "completion"
   },
   {
    "id": "e7dff5ca819048459a6ae624e6d6ce7b",
    "time": "2021-01-20T18:42:41.368Z",
    "type": "completion"
   },
   {
    "code": "#Originally designed to find every single permutation\n#Oops misread directions\nnode_num = range(10, 130, 10)\nlayer_num = range(1,6)\n\ndef check(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\nshape_collection = []\n\ndef trickle_down(node, layer):\n    if layer > 1:\n        for i in range(0, node[-1]):\n            new_arr = node[:-1] + [node[-1]-i] + [i]\n            trickle_down(new_arr,layer-1)\n    else:\n        if check(node):\n            global shape_collection\n            shape_collection.append([sum(node), len(node),node])\n\nfor node in node_num:\n    for layer in layer_num:\n        trickle_down([node], layer)\n\nprint(shape_collection)",
    "id": "413ad261276748d89dd277b8fabceed0",
    "idx": 7,
    "time": "2021-01-20T18:42:44.847Z",
    "type": "execution"
   },
   {
    "id": "413ad261276748d89dd277b8fabceed0",
    "time": "2021-01-20T18:43:04.794Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\ndef isSym(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\n#mirroring an array\ndef mirror(arr, desired_length):\n    if desired_length/len(arr) == 2:\n        return arr + arr[::-1]\n    else:\n        return arr[:-1] + arr[::-1]\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        shape_collection.append(arr)\n    else:\n        new_arr = [0]+ arr + [0]\n        while new_arr[0] < newarr[1] and new_arr[-1] < new_arr[-2]:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "idx": 6,
    "time": "2021-01-21T06:22:13.009Z",
    "type": "execution"
   },
   {
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "time": "2021-01-21T06:22:13.170Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\ndef isSym(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\n#mirroring an array\ndef mirror(arr, desired_length):\n    if desired_length/len(arr) == 2:\n        return arr + arr[::-1]\n    else:\n        return arr[:-1] + arr[::-1]\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        shape_collection.append(arr)\n    else:\n        new_arr = [0]+ arr + [0]\n        while new_arr[0] < new_arr[1] and new_arr[-1] < new_arr[-2]:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "idx": 6,
    "time": "2021-01-21T06:22:34.197Z",
    "type": "execution"
   },
   {
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "time": "2021-01-21T06:22:34.257Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\ndef isSym(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\n#mirroring an array\ndef mirror(arr, desired_length):\n    if desired_length/len(arr) == 2:\n        return arr + arr[::-1]\n    else:\n        return arr[:-1] + arr[::-1]\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        shape_collection.append(int(arr))\n    else:\n        new_arr = [0]+ arr + [0]\n        while new_arr[0] < new_arr[1] and new_arr[-1] < new_arr[-2]:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "idx": 6,
    "time": "2021-01-21T06:22:46.609Z",
    "type": "execution"
   },
   {
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "time": "2021-01-21T06:22:46.699Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\ndef isSym(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\n#mirroring an array\ndef mirror(arr, desired_length):\n    if desired_length/len(arr) == 2:\n        return arr + arr[::-1]\n    else:\n        return arr[:-1] + arr[::-1]\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        shape_collection.append(map(int, arr))\n    else:\n        new_arr = [0]+ arr + [0]\n        while new_arr[0] < new_arr[1] and new_arr[-1] < new_arr[-2]:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "idx": 6,
    "time": "2021-01-21T06:23:28.317Z",
    "type": "execution"
   },
   {
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "time": "2021-01-21T06:23:28.409Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\ndef isSym(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\n#mirroring an array\ndef mirror(arr, desired_length):\n    if desired_length/len(arr) == 2:\n        return arr + arr[::-1]\n    else:\n        return arr[:-1] + arr[::-1]\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        shape_collection.append(map(int(), arr))\n    else:\n        new_arr = [0]+ arr + [0]\n        while new_arr[0] < new_arr[1] and new_arr[-1] < new_arr[-2]:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "idx": 6,
    "time": "2021-01-21T06:24:13.934Z",
    "type": "execution"
   },
   {
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "time": "2021-01-21T06:24:14.009Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\ndef isSym(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\n#mirroring an array\ndef mirror(arr, desired_length):\n    if desired_length/len(arr) == 2:\n        return arr + arr[::-1]\n    else:\n        return arr[:-1] + arr[::-1]\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        shape_collection.append(int() for a in arr)\n    else:\n        new_arr = [0]+ arr + [0]\n        while new_arr[0] < new_arr[1] and new_arr[-1] < new_arr[-2]:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "idx": 6,
    "time": "2021-01-21T06:24:36.786Z",
    "type": "execution"
   },
   {
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "time": "2021-01-21T06:24:36.857Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\ndef isSym(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\n#mirroring an array\ndef mirror(arr, desired_length):\n    if desired_length/len(arr) == 2:\n        return arr + arr[::-1]\n    else:\n        return arr[:-1] + arr[::-1]\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        shape_collection.append([int() for a in arr])\n    else:\n        new_arr = [0]+ arr + [0]\n        while new_arr[0] < new_arr[1] and new_arr[-1] < new_arr[-2]:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "idx": 6,
    "time": "2021-01-21T06:24:46.105Z",
    "type": "execution"
   },
   {
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "time": "2021-01-21T06:24:46.210Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\ndef isSym(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\n#mirroring an array\ndef mirror(arr, desired_length):\n    if desired_length/len(arr) == 2:\n        return arr + arr[::-1]\n    else:\n        return arr[:-1] + arr[::-1]\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        mp = map(int, arr)\n        shape_collection.append(list(mp))\n    else:\n        new_arr = [0]+ arr + [0]\n        while new_arr[0] < new_arr[1] and new_arr[-1] < new_arr[-2]:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "idx": 6,
    "time": "2021-01-21T06:26:20.137Z",
    "type": "execution"
   },
   {
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "time": "2021-01-21T06:26:20.217Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        shape_collection.append(list(mp))\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-1 and new_arr[-1] < new_arr[-2]-1:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "778aa23fe0964fc68f4f15bb72d70497",
    "idx": 6,
    "time": "2021-01-21T06:28:14.496Z",
    "type": "execution"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "886db2efd9c44203846cf9591e5d8a47",
    "idx": 0,
    "time": "2021-01-21T16:23:24.417Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "80a642188e3c4aafa156162e09d65ca3",
    "idx": 1,
    "time": "2021-01-21T16:23:24.422Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "ac8f69921065472d83f57a6376110a1b",
    "idx": 2,
    "time": "2021-01-21T16:23:24.424Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "56295541b30f497ea0d28bcb5942ff31",
    "idx": 3,
    "time": "2021-01-21T16:23:24.426Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "0e74bb0162df475e83147e0fb05107ad",
    "idx": 4,
    "time": "2021-01-21T16:23:24.427Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "a089d6fe1bc64a7c912cf53edce64f04",
    "idx": 5,
    "time": "2021-01-21T16:23:24.429Z",
    "type": "execution"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        shape_collection.append(list(mp))\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "815d9b5e736f482d8316e2e6a940788a",
    "idx": 6,
    "time": "2021-01-21T16:23:24.432Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "79ed4b747cad49c7a2de4809fcfa1151",
    "idx": 8,
    "time": "2021-01-21T16:23:24.434Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "d4e5d1e9abc2436bab9f7d21808a5d63",
    "idx": 9,
    "time": "2021-01-21T16:23:24.435Z",
    "type": "execution"
   },
   {
    "code": "with open('validation_errors.json', 'w') as f:\n    json.dump(output_file, f)",
    "id": "c554ed2026a9416580e12acdf78759e4",
    "idx": 10,
    "time": "2021-01-21T16:23:24.437Z",
    "type": "execution"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "59adedb13eb742bd898d6dc87106ff22",
    "idx": 0,
    "time": "2021-01-23T02:59:37.965Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "67bf808b7be74c1db1c728676af35131",
    "idx": 1,
    "time": "2021-01-23T02:59:37.972Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "fbe01656aff44e099be9c52ec3b8ae22",
    "idx": 2,
    "time": "2021-01-23T02:59:37.975Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "9bad1c0bb3e5485f841c664cc610e383",
    "idx": 3,
    "time": "2021-01-23T02:59:37.979Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "3dab680f0eb9475b8505e7de56fbc36f",
    "idx": 4,
    "time": "2021-01-23T02:59:37.981Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "69f27a8e26f743e7b098d022ec67f6c6",
    "idx": 5,
    "time": "2021-01-23T02:59:37.982Z",
    "type": "execution"
   },
   {
    "id": "59adedb13eb742bd898d6dc87106ff22",
    "time": "2021-01-23T02:59:40.527Z",
    "type": "completion"
   },
   {
    "id": "67bf808b7be74c1db1c728676af35131",
    "time": "2021-01-23T02:59:40.846Z",
    "type": "completion"
   },
   {
    "id": "fbe01656aff44e099be9c52ec3b8ae22",
    "time": "2021-01-23T02:59:40.886Z",
    "type": "completion"
   },
   {
    "id": "9bad1c0bb3e5485f841c664cc610e383",
    "time": "2021-01-23T02:59:40.889Z",
    "type": "completion"
   },
   {
    "id": "3dab680f0eb9475b8505e7de56fbc36f",
    "time": "2021-01-23T02:59:40.906Z",
    "type": "completion"
   },
   {
    "id": "69f27a8e26f743e7b098d022ec67f6c6",
    "time": "2021-01-23T02:59:41.155Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,2)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        shape_collection.append(list(mp))\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T02:59:43.329Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T02:59:43.412Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,3)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        shape_collection.append(list(mp))\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:00:40.394Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:00:40.495Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        shape_collection.append(list(mp))\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:01:02.834Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:01:02.882Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        shape_collection.append(list(mp))\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3 or not node//layer == node/layer:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:01:47.636Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:01:47.692Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        if sum(arr) == check:\n            shape_collection.append(list(mp))\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, sum[arr])\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:04:01.112Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:04:01.327Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        if sum(arr) == check:\n            shape_collection.append(list(mp))\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, sum(arr)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:04:11.971Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:04:12.020Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        if sum(arr) == check:\n            shape_collection.append(list(mp))\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, sum(arr))\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:04:22.464Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:04:22.532Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        if sum(arr) == check:\n            shape_collection.append(list(mp))\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, sum(arr))\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:04:31.429Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:04:31.504Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        if sum(arr) == check:\n            shape_collection.append(list(mp))\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:04:51.482Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:04:51.532Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        if sum(list(mp)) == check:\n            shape_collection.append(list(mp))\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:05:30.694Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:05:30.752Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        print(list(mp))\n        if sum(list(mp)) == check:\n            shape_collection.append(list(mp))\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:05:42.130Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:05:42.222Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        #print(list(mp))\n        if sum(list(mp)) == check:\n            shape_collection.append(list(mp))\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:06:12.373Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:06:12.457Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:06:37.874Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:06:37.971Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        print(sum(x))\n        shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:06:52.677Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:06:52.756Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int(), arr)\n        x = list(mp)\n        print(sum(x))\n        shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:07:22.733Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:07:22.819Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        #mp = map(int, arr)\n       # x = list(mp)\n        print(arr)\n        shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:07:42.114Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:07:42.187Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        print(sum(x))\n        shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:07:59.327Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:07:59.384Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        print(sum(x), check)\n        shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:09:11.574Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:09:11.677Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        if check == sum(x):\n            shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:09:26.604Z",
    "type": "execution"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:09:26.680Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "59adedb13eb742bd898d6dc87106ff22",
    "idx": 0,
    "time": "2021-01-23T03:09:43.443Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "67bf808b7be74c1db1c728676af35131",
    "idx": 1,
    "time": "2021-01-23T03:09:43.444Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "fbe01656aff44e099be9c52ec3b8ae22",
    "idx": 2,
    "time": "2021-01-23T03:09:43.445Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "9bad1c0bb3e5485f841c664cc610e383",
    "idx": 3,
    "time": "2021-01-23T03:09:43.446Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "3dab680f0eb9475b8505e7de56fbc36f",
    "idx": 4,
    "time": "2021-01-23T03:09:43.447Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "69f27a8e26f743e7b098d022ec67f6c6",
    "idx": 5,
    "time": "2021-01-23T03:09:43.448Z",
    "type": "execution"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        if check == sum(x):\n            shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:09:43.451Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "ee151fd81e224b6dbc1f58e993564c02",
    "idx": 8,
    "time": "2021-01-23T03:09:43.452Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "b8b7900f1ad64169be6e65e68e8a7f1a",
    "idx": 9,
    "time": "2021-01-23T03:09:43.453Z",
    "type": "execution"
   },
   {
    "code": "with open('validation_errors.json', 'w') as f:\n    json.dump(output_file, f)",
    "id": "7333cb2902254eab803d94d93fbf488f",
    "idx": 10,
    "time": "2021-01-23T03:09:43.454Z",
    "type": "execution"
   },
   {
    "id": "59adedb13eb742bd898d6dc87106ff22",
    "time": "2021-01-23T03:09:43.496Z",
    "type": "completion"
   },
   {
    "id": "67bf808b7be74c1db1c728676af35131",
    "time": "2021-01-23T03:09:43.778Z",
    "type": "completion"
   },
   {
    "id": "fbe01656aff44e099be9c52ec3b8ae22",
    "time": "2021-01-23T03:09:43.810Z",
    "type": "completion"
   },
   {
    "id": "9bad1c0bb3e5485f841c664cc610e383",
    "time": "2021-01-23T03:09:43.811Z",
    "type": "completion"
   },
   {
    "id": "3dab680f0eb9475b8505e7de56fbc36f",
    "time": "2021-01-23T03:09:43.842Z",
    "type": "completion"
   },
   {
    "id": "69f27a8e26f743e7b098d022ec67f6c6",
    "time": "2021-01-23T03:09:44.086Z",
    "type": "completion"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:09:44.104Z",
    "type": "completion"
   },
   {
    "id": "ee151fd81e224b6dbc1f58e993564c02",
    "time": "2021-01-23T03:09:44.126Z",
    "type": "completion"
   },
   {
    "id": "b8b7900f1ad64169be6e65e68e8a7f1a",
    "time": "2021-01-23T03:09:51.984Z",
    "type": "completion"
   },
   {
    "id": "7333cb2902254eab803d94d93fbf488f",
    "time": "2021-01-23T03:09:51.986Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "59adedb13eb742bd898d6dc87106ff22",
    "idx": 0,
    "time": "2021-01-23T03:09:59.689Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "67bf808b7be74c1db1c728676af35131",
    "idx": 1,
    "time": "2021-01-23T03:09:59.690Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "fbe01656aff44e099be9c52ec3b8ae22",
    "idx": 2,
    "time": "2021-01-23T03:09:59.692Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "9bad1c0bb3e5485f841c664cc610e383",
    "idx": 3,
    "time": "2021-01-23T03:09:59.693Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "3dab680f0eb9475b8505e7de56fbc36f",
    "idx": 4,
    "time": "2021-01-23T03:09:59.693Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "69f27a8e26f743e7b098d022ec67f6c6",
    "idx": 5,
    "time": "2021-01-23T03:09:59.694Z",
    "type": "execution"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        if check == sum(x):\n            shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "8fc63966a8424c7db16629d9659432f8",
    "idx": 6,
    "time": "2021-01-23T03:09:59.696Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "ee151fd81e224b6dbc1f58e993564c02",
    "idx": 8,
    "time": "2021-01-23T03:09:59.697Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "b8b7900f1ad64169be6e65e68e8a7f1a",
    "idx": 9,
    "time": "2021-01-23T03:09:59.699Z",
    "type": "execution"
   },
   {
    "code": "with open('validation_errors.json', 'w') as f:\n    json.dump(output_file, f)",
    "id": "7333cb2902254eab803d94d93fbf488f",
    "idx": 10,
    "time": "2021-01-23T03:09:59.700Z",
    "type": "execution"
   },
   {
    "id": "59adedb13eb742bd898d6dc87106ff22",
    "time": "2021-01-23T03:10:01.352Z",
    "type": "completion"
   },
   {
    "id": "67bf808b7be74c1db1c728676af35131",
    "time": "2021-01-23T03:10:01.616Z",
    "type": "completion"
   },
   {
    "id": "fbe01656aff44e099be9c52ec3b8ae22",
    "time": "2021-01-23T03:10:01.691Z",
    "type": "completion"
   },
   {
    "id": "9bad1c0bb3e5485f841c664cc610e383",
    "time": "2021-01-23T03:10:01.693Z",
    "type": "completion"
   },
   {
    "id": "3dab680f0eb9475b8505e7de56fbc36f",
    "time": "2021-01-23T03:10:01.694Z",
    "type": "completion"
   },
   {
    "id": "69f27a8e26f743e7b098d022ec67f6c6",
    "time": "2021-01-23T03:10:01.983Z",
    "type": "completion"
   },
   {
    "id": "8fc63966a8424c7db16629d9659432f8",
    "time": "2021-01-23T03:10:02.017Z",
    "type": "completion"
   },
   {
    "id": "ee151fd81e224b6dbc1f58e993564c02",
    "time": "2021-01-23T03:10:02.019Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "5d6a1247ecf143118cd7bad8afa99507",
    "idx": 0,
    "time": "2021-01-27T03:05:54.085Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "549a1ad2f7f942f882696720ee855403",
    "idx": 1,
    "time": "2021-01-27T03:05:54.089Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "dbe767c8e3e34a2d84e7ef11f3420ada",
    "idx": 2,
    "time": "2021-01-27T03:05:54.098Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "47bbdac4cb5248598555458117d6e89b",
    "idx": 3,
    "time": "2021-01-27T03:05:54.100Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(2)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "a2d82634e01241c0b1c41a9a15bb48d4",
    "idx": 4,
    "time": "2021-01-27T03:05:54.101Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "615a1295362c4b708badc33e86f5eb83",
    "idx": 5,
    "time": "2021-01-27T03:05:54.103Z",
    "type": "execution"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        if check == sum(x):\n            shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "c167e6cabab240628e045abec0042014",
    "idx": 6,
    "time": "2021-01-27T03:05:54.104Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "4318238cde4e418d90158e8e780f7080",
    "idx": 8,
    "time": "2021-01-27T03:05:54.106Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "d99d0511e1be4f4c8b24c53d77411acf",
    "idx": 9,
    "time": "2021-01-27T03:05:54.110Z",
    "type": "execution"
   },
   {
    "code": "with open('validation_errors.json', 'w') as f:\n    json.dump(output_file, f)",
    "id": "46107c4abf7c4ea481da589657ff4277",
    "idx": 10,
    "time": "2021-01-27T03:05:54.112Z",
    "type": "execution"
   },
   {
    "id": "5d6a1247ecf143118cd7bad8afa99507",
    "time": "2021-01-27T03:05:56.071Z",
    "type": "completion"
   },
   {
    "id": "549a1ad2f7f942f882696720ee855403",
    "time": "2021-01-27T03:05:56.456Z",
    "type": "completion"
   },
   {
    "id": "dbe767c8e3e34a2d84e7ef11f3420ada",
    "time": "2021-01-27T03:05:56.504Z",
    "type": "completion"
   },
   {
    "id": "47bbdac4cb5248598555458117d6e89b",
    "time": "2021-01-27T03:05:56.506Z",
    "type": "completion"
   },
   {
    "id": "a2d82634e01241c0b1c41a9a15bb48d4",
    "time": "2021-01-27T03:05:56.586Z",
    "type": "completion"
   },
   {
    "id": "615a1295362c4b708badc33e86f5eb83",
    "time": "2021-01-27T03:05:56.776Z",
    "type": "completion"
   },
   {
    "id": "c167e6cabab240628e045abec0042014",
    "time": "2021-01-27T03:05:56.790Z",
    "type": "completion"
   },
   {
    "id": "4318238cde4e418d90158e8e780f7080",
    "time": "2021-01-27T03:05:56.817Z",
    "type": "completion"
   },
   {
    "id": "d99d0511e1be4f4c8b24c53d77411acf",
    "time": "2021-01-27T03:05:56.818Z",
    "type": "completion"
   },
   {
    "id": "46107c4abf7c4ea481da589657ff4277",
    "time": "2021-01-27T03:05:56.819Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "5d6a1247ecf143118cd7bad8afa99507",
    "idx": 0,
    "time": "2021-01-27T03:08:31.655Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "549a1ad2f7f942f882696720ee855403",
    "idx": 1,
    "time": "2021-01-27T03:08:31.659Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "dbe767c8e3e34a2d84e7ef11f3420ada",
    "idx": 2,
    "time": "2021-01-27T03:08:31.661Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "47bbdac4cb5248598555458117d6e89b",
    "idx": 3,
    "time": "2021-01-27T03:08:31.665Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(3)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "a2d82634e01241c0b1c41a9a15bb48d4",
    "idx": 4,
    "time": "2021-01-27T03:08:31.667Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "615a1295362c4b708badc33e86f5eb83",
    "idx": 5,
    "time": "2021-01-27T03:08:31.669Z",
    "type": "execution"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        if check == sum(x):\n            shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "c167e6cabab240628e045abec0042014",
    "idx": 6,
    "time": "2021-01-27T03:08:31.670Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "4318238cde4e418d90158e8e780f7080",
    "idx": 8,
    "time": "2021-01-27T03:08:31.672Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "d99d0511e1be4f4c8b24c53d77411acf",
    "idx": 9,
    "time": "2021-01-27T03:08:31.673Z",
    "type": "execution"
   },
   {
    "code": "with open('validation_errors.json', 'w') as f:\n    json.dump(output_file, f)",
    "id": "46107c4abf7c4ea481da589657ff4277",
    "idx": 10,
    "time": "2021-01-27T03:08:31.674Z",
    "type": "execution"
   },
   {
    "id": "5d6a1247ecf143118cd7bad8afa99507",
    "time": "2021-01-27T03:08:31.744Z",
    "type": "completion"
   },
   {
    "id": "549a1ad2f7f942f882696720ee855403",
    "time": "2021-01-27T03:08:32.048Z",
    "type": "completion"
   },
   {
    "id": "dbe767c8e3e34a2d84e7ef11f3420ada",
    "time": "2021-01-27T03:08:32.080Z",
    "type": "completion"
   },
   {
    "id": "47bbdac4cb5248598555458117d6e89b",
    "time": "2021-01-27T03:08:32.089Z",
    "type": "completion"
   },
   {
    "id": "a2d82634e01241c0b1c41a9a15bb48d4",
    "time": "2021-01-27T03:08:32.301Z",
    "type": "completion"
   },
   {
    "id": "615a1295362c4b708badc33e86f5eb83",
    "time": "2021-01-27T03:08:32.371Z",
    "type": "completion"
   },
   {
    "id": "c167e6cabab240628e045abec0042014",
    "time": "2021-01-27T03:08:32.380Z",
    "type": "completion"
   },
   {
    "id": "4318238cde4e418d90158e8e780f7080",
    "time": "2021-01-27T03:08:32.386Z",
    "type": "completion"
   },
   {
    "id": "d99d0511e1be4f4c8b24c53d77411acf",
    "time": "2021-01-27T03:08:32.387Z",
    "type": "completion"
   },
   {
    "id": "46107c4abf7c4ea481da589657ff4277",
    "time": "2021-01-27T03:08:32.387Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "5d6a1247ecf143118cd7bad8afa99507",
    "idx": 0,
    "time": "2021-01-27T03:08:45.396Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "549a1ad2f7f942f882696720ee855403",
    "idx": 1,
    "time": "2021-01-27T03:08:45.398Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "dbe767c8e3e34a2d84e7ef11f3420ada",
    "idx": 2,
    "time": "2021-01-27T03:08:45.399Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "47bbdac4cb5248598555458117d6e89b",
    "idx": 3,
    "time": "2021-01-27T03:08:45.400Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "a2d82634e01241c0b1c41a9a15bb48d4",
    "idx": 4,
    "time": "2021-01-27T03:08:45.401Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "615a1295362c4b708badc33e86f5eb83",
    "idx": 5,
    "time": "2021-01-27T03:08:45.406Z",
    "type": "execution"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        if check == sum(x):\n            shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "c167e6cabab240628e045abec0042014",
    "idx": 6,
    "time": "2021-01-27T03:08:45.408Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "4318238cde4e418d90158e8e780f7080",
    "idx": 8,
    "time": "2021-01-27T03:08:45.410Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "d99d0511e1be4f4c8b24c53d77411acf",
    "idx": 9,
    "time": "2021-01-27T03:08:45.411Z",
    "type": "execution"
   },
   {
    "code": "with open('validation_errors.json', 'w') as f:\n    json.dump(output_file, f)",
    "id": "46107c4abf7c4ea481da589657ff4277",
    "idx": 10,
    "time": "2021-01-27T03:08:45.415Z",
    "type": "execution"
   },
   {
    "id": "5d6a1247ecf143118cd7bad8afa99507",
    "time": "2021-01-27T03:08:45.453Z",
    "type": "completion"
   },
   {
    "id": "549a1ad2f7f942f882696720ee855403",
    "time": "2021-01-27T03:08:45.766Z",
    "type": "completion"
   },
   {
    "id": "dbe767c8e3e34a2d84e7ef11f3420ada",
    "time": "2021-01-27T03:08:45.805Z",
    "type": "completion"
   },
   {
    "id": "47bbdac4cb5248598555458117d6e89b",
    "time": "2021-01-27T03:08:45.816Z",
    "type": "completion"
   },
   {
    "id": "a2d82634e01241c0b1c41a9a15bb48d4",
    "time": "2021-01-27T03:08:45.929Z",
    "type": "completion"
   },
   {
    "id": "615a1295362c4b708badc33e86f5eb83",
    "time": "2021-01-27T03:08:46.167Z",
    "type": "completion"
   },
   {
    "id": "c167e6cabab240628e045abec0042014",
    "time": "2021-01-27T03:08:46.170Z",
    "type": "completion"
   },
   {
    "id": "4318238cde4e418d90158e8e780f7080",
    "time": "2021-01-27T03:08:46.187Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(2)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "a2d82634e01241c0b1c41a9a15bb48d4",
    "idx": 4,
    "time": "2021-01-27T03:09:49.451Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "615a1295362c4b708badc33e86f5eb83",
    "idx": 5,
    "time": "2021-01-27T03:09:52.197Z",
    "type": "execution"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "5d6a1247ecf143118cd7bad8afa99507",
    "idx": 0,
    "time": "2021-01-27T03:09:54.260Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "549a1ad2f7f942f882696720ee855403",
    "idx": 1,
    "time": "2021-01-27T03:09:54.261Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "dbe767c8e3e34a2d84e7ef11f3420ada",
    "idx": 2,
    "time": "2021-01-27T03:09:54.262Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "47bbdac4cb5248598555458117d6e89b",
    "idx": 3,
    "time": "2021-01-27T03:09:54.262Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(2)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "a2d82634e01241c0b1c41a9a15bb48d4",
    "idx": 4,
    "time": "2021-01-27T03:09:54.263Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "615a1295362c4b708badc33e86f5eb83",
    "idx": 5,
    "time": "2021-01-27T03:09:54.264Z",
    "type": "execution"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        if check == sum(x):\n            shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "c167e6cabab240628e045abec0042014",
    "idx": 6,
    "time": "2021-01-27T03:09:54.267Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "4318238cde4e418d90158e8e780f7080",
    "idx": 8,
    "time": "2021-01-27T03:09:54.268Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "d99d0511e1be4f4c8b24c53d77411acf",
    "idx": 9,
    "time": "2021-01-27T03:09:54.269Z",
    "type": "execution"
   },
   {
    "code": "with open('validation_errors.json', 'w') as f:\n    json.dump(output_file, f)",
    "id": "46107c4abf7c4ea481da589657ff4277",
    "idx": 10,
    "time": "2021-01-27T03:09:54.270Z",
    "type": "execution"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "5d6a1247ecf143118cd7bad8afa99507",
    "idx": 0,
    "time": "2021-01-27T03:10:02.561Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "549a1ad2f7f942f882696720ee855403",
    "idx": 1,
    "time": "2021-01-27T03:10:02.563Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "dbe767c8e3e34a2d84e7ef11f3420ada",
    "idx": 2,
    "time": "2021-01-27T03:10:02.564Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "47bbdac4cb5248598555458117d6e89b",
    "idx": 3,
    "time": "2021-01-27T03:10:02.565Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(2)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "a2d82634e01241c0b1c41a9a15bb48d4",
    "idx": 4,
    "time": "2021-01-27T03:10:02.567Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "615a1295362c4b708badc33e86f5eb83",
    "idx": 5,
    "time": "2021-01-27T03:10:02.568Z",
    "type": "execution"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        if check == sum(x):\n            shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "c167e6cabab240628e045abec0042014",
    "idx": 6,
    "time": "2021-01-27T03:10:02.569Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "4318238cde4e418d90158e8e780f7080",
    "idx": 8,
    "time": "2021-01-27T03:10:02.571Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "d99d0511e1be4f4c8b24c53d77411acf",
    "idx": 9,
    "time": "2021-01-27T03:10:02.573Z",
    "type": "execution"
   },
   {
    "code": "with open('validation_errors.json', 'w') as f:\n    json.dump(output_file, f)",
    "id": "46107c4abf7c4ea481da589657ff4277",
    "idx": 10,
    "time": "2021-01-27T03:10:02.574Z",
    "type": "execution"
   },
   {
    "id": "5d6a1247ecf143118cd7bad8afa99507",
    "time": "2021-01-27T03:10:04.753Z",
    "type": "completion"
   },
   {
    "id": "5d6a1247ecf143118cd7bad8afa99507",
    "time": "2021-01-27T03:10:04.753Z",
    "type": "completion"
   },
   {
    "id": "549a1ad2f7f942f882696720ee855403",
    "time": "2021-01-27T03:10:04.754Z",
    "type": "completion"
   },
   {
    "id": "549a1ad2f7f942f882696720ee855403",
    "time": "2021-01-27T03:10:04.754Z",
    "type": "completion"
   },
   {
    "id": "dbe767c8e3e34a2d84e7ef11f3420ada",
    "time": "2021-01-27T03:10:04.754Z",
    "type": "completion"
   },
   {
    "id": "dbe767c8e3e34a2d84e7ef11f3420ada",
    "time": "2021-01-27T03:10:04.754Z",
    "type": "completion"
   },
   {
    "id": "47bbdac4cb5248598555458117d6e89b",
    "time": "2021-01-27T03:10:04.755Z",
    "type": "completion"
   },
   {
    "id": "47bbdac4cb5248598555458117d6e89b",
    "time": "2021-01-27T03:10:04.755Z",
    "type": "completion"
   },
   {
    "id": "a2d82634e01241c0b1c41a9a15bb48d4",
    "time": "2021-01-27T03:10:04.755Z",
    "type": "completion"
   },
   {
    "id": "a2d82634e01241c0b1c41a9a15bb48d4",
    "time": "2021-01-27T03:10:04.755Z",
    "type": "completion"
   },
   {
    "id": "a2d82634e01241c0b1c41a9a15bb48d4",
    "time": "2021-01-27T03:10:04.755Z",
    "type": "completion"
   },
   {
    "id": "615a1295362c4b708badc33e86f5eb83",
    "time": "2021-01-27T03:10:04.756Z",
    "type": "completion"
   },
   {
    "id": "615a1295362c4b708badc33e86f5eb83",
    "time": "2021-01-27T03:10:04.756Z",
    "type": "completion"
   },
   {
    "id": "615a1295362c4b708badc33e86f5eb83",
    "time": "2021-01-27T03:10:04.756Z",
    "type": "completion"
   },
   {
    "id": "c167e6cabab240628e045abec0042014",
    "time": "2021-01-27T03:10:04.756Z",
    "type": "completion"
   },
   {
    "id": "c167e6cabab240628e045abec0042014",
    "time": "2021-01-27T03:10:04.756Z",
    "type": "completion"
   },
   {
    "id": "4318238cde4e418d90158e8e780f7080",
    "time": "2021-01-27T03:10:04.756Z",
    "type": "completion"
   },
   {
    "id": "4318238cde4e418d90158e8e780f7080",
    "time": "2021-01-27T03:10:04.756Z",
    "type": "completion"
   },
   {
    "id": "d99d0511e1be4f4c8b24c53d77411acf",
    "time": "2021-01-27T03:10:04.757Z",
    "type": "completion"
   },
   {
    "id": "d99d0511e1be4f4c8b24c53d77411acf",
    "time": "2021-01-27T03:10:04.757Z",
    "type": "completion"
   },
   {
    "id": "d99d0511e1be4f4c8b24c53d77411acf",
    "time": "2021-01-27T03:10:04.757Z",
    "type": "completion"
   },
   {
    "id": "46107c4abf7c4ea481da589657ff4277",
    "time": "2021-01-27T03:10:04.757Z",
    "type": "completion"
   },
   {
    "id": "46107c4abf7c4ea481da589657ff4277",
    "time": "2021-01-27T03:10:04.758Z",
    "type": "completion"
   },
   {
    "id": "46107c4abf7c4ea481da589657ff4277",
    "time": "2021-01-27T03:10:04.758Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "5d6a1247ecf143118cd7bad8afa99507",
    "idx": 0,
    "time": "2021-01-27T03:10:06.009Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "549a1ad2f7f942f882696720ee855403",
    "idx": 1,
    "time": "2021-01-27T03:10:06.010Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "dbe767c8e3e34a2d84e7ef11f3420ada",
    "idx": 2,
    "time": "2021-01-27T03:10:06.012Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "47bbdac4cb5248598555458117d6e89b",
    "idx": 3,
    "time": "2021-01-27T03:10:06.014Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(2)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "a2d82634e01241c0b1c41a9a15bb48d4",
    "idx": 4,
    "time": "2021-01-27T03:10:06.015Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "615a1295362c4b708badc33e86f5eb83",
    "idx": 5,
    "time": "2021-01-27T03:10:06.016Z",
    "type": "execution"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        if check == sum(x):\n            shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "c167e6cabab240628e045abec0042014",
    "idx": 6,
    "time": "2021-01-27T03:10:06.017Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "4318238cde4e418d90158e8e780f7080",
    "idx": 8,
    "time": "2021-01-27T03:10:06.019Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "d99d0511e1be4f4c8b24c53d77411acf",
    "idx": 9,
    "time": "2021-01-27T03:10:06.020Z",
    "type": "execution"
   },
   {
    "code": "with open('validation_errors.json', 'w') as f:\n    json.dump(output_file, f)",
    "id": "46107c4abf7c4ea481da589657ff4277",
    "idx": 10,
    "time": "2021-01-27T03:10:06.021Z",
    "type": "execution"
   },
   {
    "id": "5d6a1247ecf143118cd7bad8afa99507",
    "time": "2021-01-27T03:10:06.085Z",
    "type": "completion"
   },
   {
    "id": "549a1ad2f7f942f882696720ee855403",
    "time": "2021-01-27T03:10:06.550Z",
    "type": "completion"
   },
   {
    "id": "dbe767c8e3e34a2d84e7ef11f3420ada",
    "time": "2021-01-27T03:10:06.593Z",
    "type": "completion"
   },
   {
    "id": "47bbdac4cb5248598555458117d6e89b",
    "time": "2021-01-27T03:10:06.595Z",
    "type": "completion"
   },
   {
    "id": "a2d82634e01241c0b1c41a9a15bb48d4",
    "time": "2021-01-27T03:10:06.674Z",
    "type": "completion"
   },
   {
    "id": "615a1295362c4b708badc33e86f5eb83",
    "time": "2021-01-27T03:10:06.767Z",
    "type": "completion"
   },
   {
    "id": "c167e6cabab240628e045abec0042014",
    "time": "2021-01-27T03:10:06.768Z",
    "type": "completion"
   },
   {
    "id": "4318238cde4e418d90158e8e780f7080",
    "time": "2021-01-27T03:10:06.801Z",
    "type": "completion"
   },
   {
    "id": "d99d0511e1be4f4c8b24c53d77411acf",
    "time": "2021-01-27T03:10:06.802Z",
    "type": "completion"
   },
   {
    "id": "46107c4abf7c4ea481da589657ff4277",
    "time": "2021-01-27T03:10:06.803Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "5d6a1247ecf143118cd7bad8afa99507",
    "idx": 0,
    "time": "2021-01-27T03:10:39.584Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "549a1ad2f7f942f882696720ee855403",
    "idx": 1,
    "time": "2021-01-27T03:10:39.587Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "dbe767c8e3e34a2d84e7ef11f3420ada",
    "idx": 2,
    "time": "2021-01-27T03:10:39.590Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "47bbdac4cb5248598555458117d6e89b",
    "idx": 3,
    "time": "2021-01-27T03:10:39.593Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(2)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "a2d82634e01241c0b1c41a9a15bb48d4",
    "idx": 4,
    "time": "2021-01-27T03:10:39.594Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 2\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "615a1295362c4b708badc33e86f5eb83",
    "idx": 5,
    "time": "2021-01-27T03:10:39.597Z",
    "type": "execution"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        if check == sum(x):\n            shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "c167e6cabab240628e045abec0042014",
    "idx": 6,
    "time": "2021-01-27T03:10:39.598Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "4318238cde4e418d90158e8e780f7080",
    "idx": 8,
    "time": "2021-01-27T03:10:39.599Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "d99d0511e1be4f4c8b24c53d77411acf",
    "idx": 9,
    "time": "2021-01-27T03:10:39.601Z",
    "type": "execution"
   },
   {
    "code": "with open('validation_errors.json', 'w') as f:\n    json.dump(output_file, f)",
    "id": "46107c4abf7c4ea481da589657ff4277",
    "idx": 10,
    "time": "2021-01-27T03:10:39.602Z",
    "type": "execution"
   },
   {
    "id": "5d6a1247ecf143118cd7bad8afa99507",
    "time": "2021-01-27T03:10:39.646Z",
    "type": "completion"
   },
   {
    "id": "549a1ad2f7f942f882696720ee855403",
    "time": "2021-01-27T03:10:40.072Z",
    "type": "completion"
   },
   {
    "id": "dbe767c8e3e34a2d84e7ef11f3420ada",
    "time": "2021-01-27T03:10:40.110Z",
    "type": "completion"
   },
   {
    "id": "47bbdac4cb5248598555458117d6e89b",
    "time": "2021-01-27T03:10:40.139Z",
    "type": "completion"
   },
   {
    "id": "a2d82634e01241c0b1c41a9a15bb48d4",
    "time": "2021-01-27T03:10:40.241Z",
    "type": "completion"
   },
   {
    "id": "615a1295362c4b708badc33e86f5eb83",
    "time": "2021-01-27T03:10:40.328Z",
    "type": "completion"
   },
   {
    "id": "c167e6cabab240628e045abec0042014",
    "time": "2021-01-27T03:10:40.329Z",
    "type": "completion"
   },
   {
    "id": "4318238cde4e418d90158e8e780f7080",
    "time": "2021-01-27T03:10:40.335Z",
    "type": "completion"
   },
   {
    "id": "d99d0511e1be4f4c8b24c53d77411acf",
    "time": "2021-01-27T03:10:40.337Z",
    "type": "completion"
   },
   {
    "id": "46107c4abf7c4ea481da589657ff4277",
    "time": "2021-01-27T03:10:40.338Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "56395750006c485f9171f57f79f3f002",
    "idx": 0,
    "time": "2021-02-06T18:29:12.110Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "8a2ddcb5b0694a22a7a00b340bded3f3",
    "idx": 1,
    "time": "2021-02-06T18:29:12.781Z",
    "type": "execution"
   },
   {
    "id": "56395750006c485f9171f57f79f3f002",
    "time": "2021-02-06T18:29:14.285Z",
    "type": "completion"
   },
   {
    "id": "8a2ddcb5b0694a22a7a00b340bded3f3",
    "time": "2021-02-06T18:29:14.681Z",
    "type": "completion"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "277px",
    "left": "708px",
    "right": "20px",
    "top": "94px",
    "width": "418px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
