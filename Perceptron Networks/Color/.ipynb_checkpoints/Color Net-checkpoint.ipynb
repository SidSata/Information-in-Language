{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "sys.path.insert(0, '..')\n",
    "from net_framework import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Lnum</th>\n",
       "      <th>#snum</th>\n",
       "      <th>#cnum</th>\n",
       "      <th>Term Abbrev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>LE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>WK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>LF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #Lnum  #snum  #cnum Term Abbrev\n",
       "0      1      1      1          LB\n",
       "1      1      1      2          LB\n",
       "2      1      1      3          LE\n",
       "3      1      1      4          WK\n",
       "4      1      1      5          LF"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\n",
    "term_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\n",
    "term_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
       "       105, 106, 107, 108, 109, 110], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_data['#Lnum'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#cnum</th>\n",
       "      <th>Normalized-L</th>\n",
       "      <th>Normalized-a</th>\n",
       "      <th>Normalized-b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141</td>\n",
       "      <td>0.853205</td>\n",
       "      <td>-0.025675</td>\n",
       "      <td>-0.138822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>274</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>-0.025504</td>\n",
       "      <td>-0.138822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.070445</td>\n",
       "      <td>-0.106039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.070101</td>\n",
       "      <td>-0.089951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>302</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.070617</td>\n",
       "      <td>-0.072041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #cnum  Normalized-L  Normalized-a  Normalized-b\n",
       "0    141      0.853205     -0.025675     -0.138822\n",
       "1    274      0.747664     -0.025504     -0.138822\n",
       "2    129      0.747664      0.070445     -0.106039\n",
       "3    230      0.747664      0.070101     -0.089951\n",
       "4    302      0.747664      0.070617     -0.072041"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#cnum</th>\n",
       "      <th>Normalized-L</th>\n",
       "      <th>Normalized-a</th>\n",
       "      <th>Normalized-b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141</td>\n",
       "      <td>0.853205</td>\n",
       "      <td>-0.025675</td>\n",
       "      <td>-0.138822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>274</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>-0.025504</td>\n",
       "      <td>-0.138822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.070445</td>\n",
       "      <td>-0.106039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.070101</td>\n",
       "      <td>-0.089951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>302</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.070617</td>\n",
       "      <td>-0.072041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>305</td>\n",
       "      <td>-0.765518</td>\n",
       "      <td>0.567556</td>\n",
       "      <td>-0.362691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>267</td>\n",
       "      <td>-0.765518</td>\n",
       "      <td>0.584752</td>\n",
       "      <td>-0.297579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>243</td>\n",
       "      <td>-0.765518</td>\n",
       "      <td>0.593865</td>\n",
       "      <td>-0.235807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>182</td>\n",
       "      <td>-0.765518</td>\n",
       "      <td>0.601603</td>\n",
       "      <td>-0.171302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>89</td>\n",
       "      <td>-0.871488</td>\n",
       "      <td>-0.024988</td>\n",
       "      <td>-0.139429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>330 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     #cnum  Normalized-L  Normalized-a  Normalized-b\n",
       "0      141      0.853205     -0.025675     -0.138822\n",
       "1      274      0.747664     -0.025504     -0.138822\n",
       "2      129      0.747664      0.070445     -0.106039\n",
       "3      230      0.747664      0.070101     -0.089951\n",
       "4      302      0.747664      0.070617     -0.072041\n",
       "..     ...           ...           ...           ...\n",
       "325    305     -0.765518      0.567556     -0.362691\n",
       "326    267     -0.765518      0.584752     -0.297579\n",
       "327    243     -0.765518      0.593865     -0.235807\n",
       "328    182     -0.765518      0.601603     -0.171302\n",
       "329     89     -0.871488     -0.024988     -0.139429\n",
       "\n",
       "[330 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\n",
    "locations = cnum_data[['#cnum']]\n",
    "locations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).std() * 1/2\n",
    "locations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\n",
    "locations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\n",
    "display(locations.head(5))\n",
    "display(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\n",
    "# debug1['L*'] = (debug1['L*'] - debug1['L*'].mean()) / debug1['L*'].std()\n",
    "# debug1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11742175098860884, -0.1023657233266634, -0.7343880269977279], [0.5389418959675792, -0.36235605857428743, -0.20499626945699717], [-0.09988100059596995, 0.9245273203894813, 0.8971949413402466], [-0.7655181557459678, 0.3904466449403276, -0.35722675478192634], [0.5389418959675792, -0.5927707736746949, -0.06445247144016328], [0.5389418959675792, 0.2310478084492248, 0.33016294415785846], [0.11742175098860884, -0.8322989066111633, 0.8414934576769875], [-0.5460702634744732, 0.7943602014857436, -0.06020276696449227], [-0.3219030616917637, 0.3562283402351178, 0.48451828172062317], [-0.7655181557459678, 0.10655508178303447, -0.6849093248881297], [-0.7655181557459678, -0.3020011593203747, -0.2106119503712767], [0.5389418959675792, -0.21275844503894822, 1.1943707043175262], [-0.7655181557459678, -0.5024963517137144, -0.028481758556805136], [-0.7655181557459678, -0.18834136328950205, -0.3520663993471829], [0.5389418959675792, 0.29759295378046186, -0.33309450436650884], [0.5389418959675792, -0.025503501199885672, -0.13882229976440583], [0.11742175098860884, 0.7728662914950339, 0.39861354124813064], [-0.7655181557459678, -0.05456326750732513, 0.03587090921764149], [0.11742175098860884, -0.31369384635532077, -0.5694084282457856], [-0.09988100059596995, -1.0023587224576582, 0.607304386035546], [0.11742175098860884, 0.8244516754727371, -0.09753945628645895], [0.7476640848044945, -0.11491816676123784, 0.5468978724170795], [-0.09988100059596995, 0.5125320536875584, -0.5941477793005847], [-0.5460702634744732, -0.027223013999142443, 0.26368542414557633], [-0.3219030616917637, -0.4374987679018084, -0.35586077834331775], [-0.5460702634744732, 0.11050996122132505, -0.7982853692926383], [0.7476640848044945, 0.06494287204102057, -0.02332140312206178], [0.11742175098860884, 0.6643650338619316, -0.19194360571029337], [0.32979067899328085, -0.7459793640884734, 0.07927860493342387], [-0.09988100059596995, 0.48175277458086224, 0.8383061793202341], [0.5389418959675792, -0.33879873322446963, -0.24612733777509868], [0.11742175098860884, 0.6132955037240054, -0.4180885938799289], [0.32979067899328085, -0.7172635003408853, 0.6681662251335485], [-0.09988100059596995, -0.2289218653519619, -0.7249779670873134], [0.32979067899328085, 0.6325540470756812, 0.30375406634476004], [-0.7655181557459678, -0.4866768339605521, -0.12121638122234024], [0.11742175098860884, 0.02075139310012151, -0.7390930569529351], [-0.3219030616917637, 0.8794760850489538, -0.14049182652270517], [-0.3219030616917637, -0.6847647084349322, 0.3388141282690459], [0.11742175098860884, -0.5270853847430862, -0.46255871571462903], [-0.5460702634744732, 0.8811955978482106, 0.3772132437099303], [0.11742175098860884, -0.048716923989852105, 0.9471289689293809], [-0.7655181557459678, -0.022752280721074835, -0.5436066510720688], [0.7476640848044945, 0.06889775147931115, -0.11696667674666925], [0.11742175098860884, -0.9115684466569005, 0.25943571966990536], [-0.09988100059596995, -0.02515959864003432, -0.13912585008409661], [-0.7655181557459678, -0.32297921547130737, -0.02650868147881503], [0.11742175098860884, 0.22382585469234634, 1.02787335396713], [0.32979067899328085, -0.40843900159436897, -0.36238711021666964], [-0.7655181557459678, 0.36637346575073276, -0.39243859186605745], [-0.5460702634744732, 0.13888192240906178, 0.42426354326200205], [0.7476640848044945, -0.23597186782891463, 0.0205416180732568], [-0.5460702634744732, 0.7823236118909461, -0.17782851584467183], [-0.3219030616917637, -0.5951780915936544, 0.48482183204031387], [0.11742175098860884, -0.594834189033803, -0.3889477631896135], [0.5389418959675792, 0.337485650723219, 0.20024340733020204], [-0.5460702634744732, -0.41738046815050417, -0.371797170127084], [-0.09988100059596995, -0.0328974062366898, 0.7294833897110874], [0.5389418959675792, 0.1418050941677983, -0.3106317807093907], [-0.7655181557459678, -0.22617064487315106, -0.32231846801748587], [-0.5460702634744732, 0.7677077530972636, 0.17292387855803135], [0.11742175098860884, 0.23809781092617754, -0.5806397900743447], [0.32979067899328085, 0.5286954740005722, 0.05104842520218079], [-0.3219030616917637, -0.06058156230472383, -0.7591273780525268], [-0.09988100059596995, 1.0017334450761104, -0.11651135126713305], [0.7476640848044945, -0.002633980969770596, 0.0789750546137331], [0.5389418959675792, -0.6058390709490463, 0.048620022644654495], [0.11742175098860884, 0.6402918546723367, -0.3162474616236702], [-0.3219030616917637, -0.18473038641106285, 0.5136591124109385], [0.5389418959675792, 0.07543190011648689, -0.34341521523599555], [-0.7655181557459678, 0.6084808678860865, -0.09222732569187019], [-0.3219030616917637, -0.9433794334431509, -0.009054538096594846], [-0.5460702634744732, 0.6098564781254918, -0.6275383144665712], [0.5389418959675792, -0.2519633368620027, -0.31564036098428866], [-0.7655181557459678, -0.3076755515579221, 0.03146942958212508], [-0.09988100059596995, 0.6722747927385128, 0.908122752849115], [-0.09988100059596995, -0.5810780866397488, -0.40852675880966915], [0.5389418959675792, 0.33696979688344203, -0.286803080613664], [-0.5460702634744732, -0.024987647360108643, -0.13927762524394202], [-0.3219030616917637, -0.2058803938419211, -0.7415214595104613], [0.11742175098860884, -0.5427329512163229, 0.8484751150298755], [0.32979067899328085, -0.6988647133888378, -0.20894242361297735], [0.7476640848044945, -0.03255350367683844, 0.31240525045594747], [0.5389418959675792, 0.3651698067912531, 0.12557002868626874], [-0.09988100059596995, -0.9337501617673128, 0.09491144639749936], [0.32979067899328085, 0.645450393070107, -0.30638207623371966], [-0.7655181557459678, -0.10150596692703501, 0.0023285988918096273], [-0.3219030616917637, 0.7823236118909461, -0.5146175955415988], [-0.8714881056796123, -0.024987647360108643, -0.1394294004037874], [-0.7655181557459678, -0.28377432364825295, -0.2476450893735526], [0.32979067899328085, -0.45228657797541666, 1.152632535360043], [0.7476640848044945, 0.029348957096405375, 0.04801292200527291], [-0.3219030616917637, -0.38711704288358495, -0.5678906766473318], [-0.7655181557459678, 0.03760261853283789, 0.028585701545062618], [0.7476640848044945, -0.252307239421854, -0.056560163128202834], [0.5389418959675792, -0.0880937670928322, 1.3807506006076689], [-0.5460702634744732, 0.4630100850689634, -0.7430392111089152], [-0.3219030616917637, 1.0311371139434011, 0.13224813571946606], [0.32979067899328085, 0.2527136697198601, -0.40564303077260666], [0.5389418959675792, -0.38900850696276745, -0.16022259730260624], [-0.5460702634744732, 0.770630924856, -0.28407112773644694], [0.32979067899328085, 0.07955873083470313, -0.4844143387323656], [-0.3219030616917637, -0.1083840181240621, 0.5168463907676917], [0.32979067899328085, -0.2691584648545704, -0.44176551881581017], [-0.3219030616917637, 0.6026345243686133, -0.5305539873253651], [0.7476640848044945, -0.23390845246980652, 0.5569150329668755], [0.7476640848044945, 0.033131885254770274, -0.2040856184979248], [0.7476640848044945, -0.24284991902594175, -0.12106460606249485], [-0.3219030616917637, 0.3331868687250771, -0.8690125937805914], [0.32979067899328085, 0.24394415444365058, 1.237171299393927], [-0.5460702634744732, -0.24886821382334043, 0.19371707545685016], [-0.09988100059596995, 1.003452957875367, 0.02297002063078309], [-0.7655181557459678, -0.026535208879439737, 0.04285256657052955], [-0.5460702634744732, -0.46604268036947083, -0.30501609979511113], [-0.5460702634744732, 0.789373614367899, 0.051200200362026174], [0.5389418959675792, -0.20416088104266436, -0.3338533801657358], [-0.7655181557459678, 0.2037075549410421, 0.13664961535498243], [0.32979067899328085, 0.6638491800221545, -0.1807122438817343], [0.11742175098860884, -0.9167269850546709, -0.05610483764866666], [0.7476640848044945, -0.005385201448581431, -0.22138798672029958], [0.11742175098860884, 0.8204967960344465, 0.7226535075180446], [-0.3219030616917637, -0.9457867513621103, 0.21359962139659666], [-0.3219030616917637, -0.376456063528193, -0.4074643326907514], [0.5389418959675792, -0.5489231972936472, 0.8554567723827635], [0.11742175098860884, 0.7438065251875944, 1.0712810496829126], [0.32979067899328085, 0.5326503534388627, -0.018768148326699992], [-0.7655181557459678, -0.32005604371257085, -0.16204389922075096], [0.5389418959675792, -0.32125970267205056, 1.1957366807561345], [0.7476640848044945, 0.07044531299864225, -0.10603886523780094], [-0.5460702634744732, -0.13538036907239345, 0.26353364898573095], [-0.5460702634744732, 0.659378446744087, -0.564551623130733], [-0.3219030616917637, 0.8533394905002509, 0.752856764327278], [-0.5460702634744732, -0.7059147158657905, 0.02130049387248375], [-0.7655181557459678, 0.25030635180090066, 0.08018925589249623], [-0.3219030616917637, 0.8323614343493183, -0.4029110778953896], [0.11742175098860884, -0.6569086010869726, -0.30516787495495656], [-0.5460702634744732, -0.43663901150218004, 0.27446146049459924], [0.32979067899328085, -0.2318450371106984, -0.5868625716280059], [0.5389418959675792, 0.11618435345887239, -0.3312732024483641], [-0.5460702634744732, -0.1597974508218396, -0.6527329910009063], [0.8532050066007559, -0.025675452479811352, -0.13882229976440583], [0.5389418959675792, 0.030724567335810797, -0.35631610382285395], [0.32979067899328085, -0.6177037092639182, 1.0615674394528074], [-0.7655181557459678, 0.37290761438790854, -0.6152445265190943], [0.32979067899328085, -0.7389293616115206, -0.05762258924712058], [0.32979067899328085, -0.06419253918316305, 1.1600695181924674], [0.11742175098860884, -0.7315354565747164, -0.13684922268641572], [-0.3219030616917637, 0.9984663707575224, 0.31817270653007246], [0.5389418959675792, -0.6857964161144864, 0.37660614307054874], [-0.7655181557459678, 0.4784857002622744, -0.5193226254968061], [0.32979067899328085, 0.5204418125641396, 0.12177564969013391], [0.32979067899328085, 0.4330905623618956, -0.4177850435602381], [0.11742175098860884, -0.2493840676631175, -0.7096486759429287], [-0.3219030616917637, -0.02515959864003432, -0.13927762524394202], [-0.5460702634744732, 0.028661151976702666, 0.2504809852390272], [-0.3219030616917637, -0.9304830874487251, -0.08873649701542616], [-0.5460702634744732, 0.21419658301650843, 0.37326708955395], [0.7476640848044945, -0.031005942157507342, -0.22442348991720745], [0.5389418959675792, 0.23293927252840724, 0.5742174011892502], [0.5389418959675792, 0.38494420398270596, -0.10148561044243916], [0.32979067899328085, -0.7442598512892166, 0.008551380445470741], [-0.3219030616917637, 1.0340602857021377, -0.00875098777690404], [-0.3219030616917637, -0.6493427447702427, -0.346298943273058], [-0.09988100059596995, -0.08327913125491324, -0.8694679192601276], [-0.3219030616917637, -0.2751767596519691, 0.47525999697005417], [-0.7655181557459678, -0.18318282489173174, -0.504600434991803], [0.32979067899328085, 0.2104136548581435, -0.43812291497952077], [-0.5460702634744732, -0.8831964854691639, -0.03440098979077546], [-0.09988100059596995, -0.29495115684342194, -0.5810951155538808], [-0.7655181557459678, -0.25488650862073914, -0.290293909290108], [-0.3219030616917637, -0.3596048380954766, 0.41515703367127854], [0.7476640848044945, -0.0502644855091832, -0.22487881539674365], [0.11742175098860884, 0.8294382625905817, 0.0228182454709377], [0.5389418959675792, 0.23500268788751535, 1.2294307662418118], [0.5389418959675792, -0.30733164899807075, -0.2813391748592299], [0.11742175098860884, 0.8098358166790546, 0.25366826359578043], [0.7476640848044945, -0.3109426258765099, 0.5380949131460468], [-0.5460702634744732, -0.019657157682412645, -0.7928214635382042], [-0.7655181557459678, 0.5797650041384983, 0.09460789607780856], [0.11742175098860884, -0.9225733285721439, 0.10705345918513082], [0.11742175098860884, 0.07629165651611527, 0.9012928706560723], [-0.7655181557459678, 0.6016028166890593, -0.17130218397131994], [-0.3219030616917637, 0.05135872092689207, 0.4738940205314456], [-0.3219030616917637, 0.544171089193883, -0.6026471882519266], [-0.09988100059596995, 0.315647838172658, 0.7519461133682056], [0.32979067899328085, -0.45675731125348423, -0.3071409520329466], [0.32979067899328085, 0.39010274238047626, 1.141097623211793], [-0.09988100059596995, -0.3981219247988283, -0.5439102013917596], [0.5389418959675792, -0.1114791411627243, -0.493065522843553], [0.5389418959675792, 0.3799576168648613, -0.15430336606863593], [-0.3219030616917637, -0.6968012980297297, -0.26616165887469057], [-0.09988100059596995, 0.44650276219609836, -0.6515187897221432], [0.32979067899328085, -0.29942189012148956, 1.1901209998418552], [-0.7655181557459678, 0.6053857448474242, 0.0005072969736649234], [-0.09988100059596995, -0.7148561824219257, 0.6622469938995784], [0.5389418959675792, -0.5779829636010867, -0.11226164679146206], [0.11742175098860884, 0.375830786146645, -0.5008060559956681], [-0.7655181557459678, -0.07777669029729155, 0.02342534611031926], [0.7476640848044945, -0.25041577534267156, -0.02392850376144335], [0.32979067899328085, -0.34051824602372643, -0.4139906645641033], [0.5389418959675792, 0.3739393220674626, 0.05924428383383199], [-0.09988100059596995, -1.112751444169943, 0.29965613702893446], [-0.09988100059596995, 0.8175736242757099, -0.38181433067688], [-0.7655181557459678, 0.1500587556042308, 0.18567299198504436], [-0.3219030616917637, 0.39302591413921273, -0.7167821084556624], [0.7476640848044945, -0.20966332200028603, 0.06334221314965763], [-0.09988100059596995, 0.8557468084192104, -0.2283696440731877], [-0.09988100059596995, 0.20095633446223127, 0.8314762971271914], [-0.5460702634744732, -0.4944146415572076, -0.24172585813958225], [-0.7655181557459678, 0.40781372421282097, 0.11221381461987417], [0.11742175098860884, -0.41978778606946365, -0.5302504370056743], [0.32979067899328085, 0.5409040148752953, 1.0292393304057386], [0.32979067899328085, 0.1516063171235619, -0.467567295989527], [0.5389418959675792, -0.47240487772672085, 1.1599177430326217], [0.32979067899328085, 0.4784857002622744, -0.35145929870780135], [-0.5460702634744732, -0.19745478112556294, 0.23924962341046804], [0.5389418959675792, 0.38236493478382083, 0.003998125650108968], [0.11742175098860884, -0.2657194392560568, 0.9708058938652623], [0.7476640848044945, -0.24749260358393504, -0.10300336204089308], [-0.09988100059596995, 0.21935512141427874, -0.7337809263583462], [0.7476640848044945, -0.21138283479954278, -0.17206105977054686], [-0.3219030616917637, 0.4824405797005649, -0.6566791451568866], [-0.09988100059596995, -0.9186184491338534, -0.07204122943243292], [0.7476640848044945, 0.06201970028228406, -0.157035318945853], [-0.09988100059596995, -0.13469256395269075, 0.7513390127288239], [-0.5460702634744732, -0.36390362009361854, -0.4264362276714255], [-0.7655181557459678, -0.1168096308404203, -0.018616373166854588], [-0.7655181557459678, -0.11491816676123784, -0.5266078331693849], [0.5389418959675792, -0.6029158991903099, -0.00814388713752248], [0.7476640848044945, 0.07010141043879088, -0.08995069829418931], [-0.09988100059596995, 0.5807967118180524, -0.5140104949022173], [-0.3219030616917637, -0.02825472167869651, 0.5033384015414517], [0.11742175098860884, 0.14008558136854152, -0.6047720404897622], [-0.3219030616917637, -0.9545562666383198, 0.07685020237589758], [-0.3219030616917637, 0.09864532290645331, -0.8825205830068313], [0.11742175098860884, -0.3996694863181594, 0.930889026825924], [0.5389418959675792, 0.29381002562209696, 0.2658102763834118], [-0.09988100059596995, -0.6469354268512832, -0.32171136737810435], [0.32979067899328085, 0.5336820611184168, -0.09784300660614972], [-0.5460702634744732, 0.45957105947044985, 0.40741650051916345], [0.7476640848044945, 0.04946725684770961, -0.18253354579987902], [0.7476640848044945, -0.17991575057314385, 0.5575221336062571], [-0.7655181557459678, 0.5938650090924038, -0.23580662690561194], [0.7476640848044945, -0.1333169537132853, -0.21683473192493782], [-0.3219030616917637, 0.9486004995790761, 0.48239342948278763], [0.11742175098860884, -0.6981769082691351, -0.22897674471256924], [-0.09988100059596995, 0.3545088274358611, -0.7016045924711231], [-0.7655181557459678, -0.279991395489888, 0.0917241680407461], [-0.5460702634744732, -0.5245061155442011, -0.16887378141379364], [-0.7655181557459678, 0.43652958796040914, -0.5622749957330522], [0.5389418959675792, -0.7538891229650544, 0.22118837938886632], [0.7476640848044945, 0.06838189763953412, -0.1285015888949191], [0.32979067899328085, -0.7280964309762028, -0.12227880734125798], [0.32979067899328085, -0.01002788600657472, -0.615092751359249], [-0.7655181557459678, 0.0025245574279997222, 0.040727714332694065], [0.11742175098860884, 0.8251394805924398, 0.13695316567467322], [-0.5460702634744732, -0.21172673735939418, -0.5070288375493291], [0.32979067899328085, -0.18816941200957638, 1.1828357921692763], [-0.5460702634744732, 0.6784650388158371, 0.4071129501994727], [0.7476640848044945, -0.27689627245122583, 0.3096732975787304], [0.11742175098860884, 0.48054911562138247, 0.8376990786808527], [0.7476640848044945, 0.02608188277781751, -0.21334390324849378], [0.11742175098860884, -0.025331549919959995, -0.13912585008409661], [0.5389418959675792, -0.1419145177095692, -0.35100397322826515], [-0.5460702634744732, -0.6992086159486891, 0.11221381461987417], [-0.3219030616917637, 0.5319625483191599, 0.5678428444757437], [-0.7655181557459678, 0.5847515912563429, -0.2975791169626869], [0.32979067899328085, -0.9110525928171235, 0.280532466888415], [-0.09988100059596995, 0.7692553146165947, -0.49488682476169776], [0.11742175098860884, 0.3551966325555638, 0.9425757141340192], [-0.7655181557459678, -0.49630610563639, -0.07507673262934078], [0.7476640848044945, -0.08448279021439298, -0.22396816443767129], [0.11742175098860884, 0.5680723171035522, -0.5044486598319574], [0.7476640848044945, -0.025503501199885672, -0.13882229976440583], [0.11742175098860884, -0.16082915850139368, 0.9714129945046437], [-0.3219030616917637, 0.8667516903344538, -0.26236727987855574], [-0.5460702634744732, -0.289620667165726, -0.4736383023833427], [0.32979067899328085, 0.5764979298199104, 0.43003099933612704], [-0.5460702634744732, 0.41813080100836164, -0.8929930690361636], [-0.09988100059596995, -0.4548658471743018, 0.6274904822949833], [-0.3219030616917637, -0.7332549693739733, -0.17115040881147453], [0.11742175098860884, -0.9865392047044957, 0.6068490605560098], [-0.5460702634744732, 0.287963682104624, 0.3196904581285263], [0.7476640848044945, -0.19126453504823857, -0.18860455219369474], [0.5389418959675792, 0.3643100503916247, -0.2233610637982897], [0.7476640848044945, 0.013013585503466034, -0.2145581045272569], [-0.09988100059596995, 0.0670062874001287, 0.6935126768277291], [-0.09988100059596995, 0.9341565920653191, 0.4984815964263993], [0.32979067899328085, -0.4978536671557211, -0.2476450893735526], [0.7476640848044945, 0.049295305567783936, 0.013256410400677932], [-0.09988100059596995, 0.061675797722432694, -0.8735658485759533], [0.5389418959675792, 0.3835685937433005, -0.04593590193902533], [0.7476640848044945, -0.2519633368620027, -0.08175483966253806], [0.7476640848044945, -0.10460108996569721, -0.22078088608091803], [-0.3219030616917637, 0.13544289681054825, 0.4339771534921072], [-0.5460702634744732, 0.5515649942306873, -0.6814184962116856], [0.5389418959675792, 0.10053678698563577, 1.51643759350945], [-0.09988100059596995, 0.9728456300485965, 0.32348483712466114], [-0.09988100059596995, -0.9306550387286507, 0.007488954326552986], [0.32979067899328085, -0.025331549919959995, -0.1389740749242512], [-0.09988100059596995, -0.9050342980197249, -0.16037437246245162], [0.7476640848044945, 0.07061726427856792, -0.07204122943243292], [0.5389418959675792, -0.02877057551847354, -0.5044486598319574], [-0.09988100059596995, -0.34051824602372643, 0.7083866424925778], [-0.7655181557459678, 0.5675564632637752, -0.3626906605363605], [0.5389418959675792, -0.7224220387386555, 0.686834569794532], [-0.09988100059596995, -0.5052475721925253, -0.47728090621963215], [0.11742175098860884, 0.320462474010577, -0.548311681027276], [-0.5460702634744732, -0.08173156973558214, 0.26808690378109273], [0.11742175098860884, -0.9203379619331101, 0.021604044192174557], [-0.3219030616917637, -0.27689627245122583, -0.6055309162889893], [-0.7655181557459678, -0.024987647360108643, -0.1394294004037874], [-0.7655181557459678, 0.38477225270278026, -0.7752155449961385], [0.7476640848044945, 0.06597457972057463, -0.1412507023219321], [0.32979067899328085, -0.8276562220531701, 0.4872502345978403], [-0.09988100059596995, -0.6882037340334458, -0.24597556261525327], [-0.5460702634744732, 0.7469016482262566, -0.39243859186605745], [0.32979067899328085, 0.5027308307317949, 0.5365771615475929], [0.7476640848044945, 0.056173356764811035, -0.1700879826925568], [-0.09988100059596995, -0.22599869359322539, 0.7475446337326891], [-0.09988100059596995, 0.996059052838563, 0.15880878869240983], [0.32979067899328085, -0.112166946282427, -0.6064415672480615], [-0.5460702634744732, -0.6983488595490608, -0.10725306651656408], [0.7476640848044945, 0.06872580019938547, -0.054131760570676556], [-0.5460702634744732, 0.705117487204317, -0.4979223279586057], [0.32979067899328085, 0.08230995131351397, 1.1040644842095173], [0.7476640848044945, -0.16151696362109638, -0.2066657962152965], [-0.5460702634744732, -0.49338293387765353, 0.1742898549966399], [-0.3219030616917637, 0.26836123619309676, 0.5576739087661026], [0.7476640848044945, -0.22995357303151595, -0.14383088003930378]]\n"
     ]
    }
   ],
   "source": [
    "locations = locations.sort_values('#cnum')\n",
    "chip_num = list(locations['#cnum'])\n",
    "lab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]\n",
    "# print(lab_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Neural Network Shape Test\n",
    "# NNtest = Neural_Network(inputSize = 3, outputSize = 9, hiddenSize = [3,3,3] , learning_rate = 0.001)\n",
    "# NNtest(torch.FloatTensor([[1, 1, 1], [1, 1, 1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#cnum\n",
       "1      [LB, LB, F, LB, LF, G, G, G, G, G, F, LB, G, G...\n",
       "2      [LB, F, F, LF, F, F, G, WK, F, F, FU, LF, F, F...\n",
       "3      [LE, LE, WK, LE, LE, LE, LE, LE, LE, LE, LE, L...\n",
       "4      [WK, WK, LB, WK, WK, WK, S, WK, LB, S, WK, F, ...\n",
       "5      [LF, F, F, LF, LF, F, G, F, F, F, LF, LF, F, F...\n",
       "                             ...                        \n",
       "326    [WK, WK, LE, LE, LE, WK, F, WK, S, WK, WK, S, ...\n",
       "327    [LF, F, LF, LF, LF, LF, G, F, LF, LF, LF, LF, ...\n",
       "328    [G, LB, S, LB, LB, LB, LB, G, S, LB, LB, LB, G...\n",
       "329    [WK, WK, F, WK, LE, LE, S, WK, WK, F, S, LB, W...\n",
       "330    [LF, F, LF, LF, LF, LF, LF, F, F, LF, LF, LF, ...\n",
       "Name: Term Abbrev, Length: 330, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[0.36, 0.0, 0.0, 0.04, 0.08, 0.52, 0.0, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.04, 0.12, 0.6, 0.04, 0.0, 0.0, 0.08],\n",
       " [0.0, 0.96, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.28, 0.0, 0.4, 0.0, 0.08, 0.04, 0.2, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.0, 0.2, 0.68, 0.08, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.16, 0.48, 0.08, 0.2, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.24, 0.0, 0.04, 0.04, 0.32, 0.2, 0.16, 0.0, 0.0],\n",
       " [0.0, 0.72, 0.08, 0.0, 0.12, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.12, 0.04, 0.12, 0.0, 0.4, 0.0, 0.32, 0.0, 0.0],\n",
       " [0.52, 0.0, 0.0, 0.0, 0.0, 0.48, 0.0, 0.0, 0.0],\n",
       " [0.64, 0.0, 0.04, 0.0, 0.12, 0.12, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.16, 0.32, 0.04, 0.4, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.8, 0.0, 0.0, 0.04, 0.04, 0.08, 0.04, 0.0, 0.0],\n",
       " [0.8, 0.0, 0.04, 0.0, 0.0, 0.08, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.08, 0.12, 0.12, 0.64, 0.04, 0.0, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.0, 0.8, 0.04, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.88, 0.0, 0.04, 0.0, 0.04, 0.04, 0.0, 0.0, 0.0],\n",
       " [0.16, 0.0, 0.08, 0.08, 0.24, 0.44, 0.0, 0.0, 0.0],\n",
       " [0.36, 0.0, 0.08, 0.04, 0.32, 0.04, 0.16, 0.0, 0.0],\n",
       " [0.0, 0.84, 0.08, 0.04, 0.04, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.08, 0.16, 0.16, 0.48, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.12, 0.04, 0.32, 0.04, 0.2, 0.12, 0.12, 0.0, 0.04],\n",
       " [0.8, 0.0, 0.08, 0.0, 0.04, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.44, 0.0, 0.08, 0.0, 0.08, 0.36, 0.04, 0.0, 0.0],\n",
       " [0.32, 0.0, 0.0, 0.0, 0.04, 0.6, 0.04, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.12, 0.36, 0.44, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.96, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.32, 0.04, 0.04, 0.04, 0.36, 0.04, 0.12, 0.0, 0.04],\n",
       " [0.04, 0.44, 0.4, 0.0, 0.08, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.12, 0.04, 0.04, 0.24, 0.36, 0.12, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.28, 0.2, 0.04, 0.24, 0.04, 0.2, 0.0, 0.0],\n",
       " [0.16, 0.04, 0.16, 0.0, 0.4, 0.12, 0.08, 0.0, 0.04],\n",
       " [0.48, 0.0, 0.04, 0.0, 0.0, 0.48, 0.0, 0.0, 0.0],\n",
       " [0.04, 0.48, 0.32, 0.0, 0.12, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.72, 0.04, 0.0, 0.0, 0.08, 0.08, 0.08, 0.0, 0.0],\n",
       " [0.32, 0.0, 0.12, 0.0, 0.12, 0.36, 0.08, 0.0, 0.0],\n",
       " [0.04, 0.84, 0.08, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.6, 0.04, 0.12, 0.0, 0.08, 0.08, 0.08, 0.0, 0.0],\n",
       " [0.28, 0.0, 0.04, 0.12, 0.16, 0.28, 0.12, 0.0, 0.0],\n",
       " [0.04, 0.88, 0.0, 0.0, 0.04, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.04, 0.04, 0.32, 0.0, 0.4, 0.0, 0.2, 0.0, 0.0],\n",
       " [0.6, 0.0, 0.0, 0.0, 0.04, 0.32, 0.04, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.08, 0.44, 0.44, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.32, 0.0, 0.12, 0.08, 0.12, 0.2, 0.12, 0.0, 0.04],\n",
       " [0.4, 0.0, 0.08, 0.08, 0.24, 0.04, 0.16, 0.0, 0.0],\n",
       " [0.8, 0.0, 0.04, 0.04, 0.0, 0.08, 0.04, 0.0, 0.0],\n",
       " [0.04, 0.16, 0.36, 0.04, 0.24, 0.0, 0.16, 0.0, 0.0],\n",
       " [0.16, 0.04, 0.08, 0.16, 0.28, 0.16, 0.08, 0.0, 0.04],\n",
       " [0.32, 0.04, 0.32, 0.0, 0.04, 0.08, 0.2, 0.0, 0.0],\n",
       " [0.36, 0.04, 0.2, 0.0, 0.28, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.04, 0.6, 0.24, 0.04, 0.04, 0.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.56, 0.0, 0.16, 0.0, 0.08, 0.04, 0.16, 0.0, 0.0],\n",
       " [0.24, 0.0, 0.0, 0.04, 0.16, 0.36, 0.16, 0.0, 0.04],\n",
       " [0.0, 0.24, 0.2, 0.08, 0.44, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.68, 0.0, 0.0, 0.0, 0.04, 0.28, 0.0, 0.0, 0.0],\n",
       " [0.08, 0.0, 0.4, 0.0, 0.12, 0.04, 0.24, 0.04, 0.08],\n",
       " [0.0, 0.0, 0.16, 0.28, 0.44, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.96, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.96, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.16, 0.0, 0.08, 0.0, 0.24, 0.28, 0.2, 0.0, 0.04],\n",
       " [0.0, 0.44, 0.16, 0.04, 0.28, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.48, 0.0, 0.0, 0.0, 0.04, 0.48, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.04, 0.48, 0.4, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.12, 0.0, 0.12, 0.08, 0.4, 0.12, 0.12, 0.0, 0.04],\n",
       " [0.0, 0.56, 0.24, 0.0, 0.12, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.48, 0.0, 0.2, 0.0, 0.12, 0.0, 0.16, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.48, 0.32, 0.0, 0.16, 0.0, 0.04],\n",
       " [0.16, 0.28, 0.28, 0.0, 0.24, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.48, 0.0, 0.0, 0.04, 0.08, 0.28, 0.12, 0.0, 0.0],\n",
       " [0.12, 0.08, 0.16, 0.0, 0.36, 0.12, 0.16, 0.0, 0.0],\n",
       " [0.08, 0.0, 0.0, 0.4, 0.32, 0.12, 0.04, 0.0, 0.04],\n",
       " [0.88, 0.0, 0.0, 0.0, 0.0, 0.04, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.72, 0.24, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.48, 0.0, 0.0, 0.0, 0.04, 0.4, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.08, 0.12, 0.16, 0.6, 0.0, 0.0, 0.0, 0.04],\n",
       " [0.88, 0.0, 0.04, 0.0, 0.04, 0.04, 0.0, 0.0, 0.0],\n",
       " [0.48, 0.0, 0.0, 0.0, 0.04, 0.44, 0.04, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.2, 0.0, 0.44, 0.0, 0.16, 0.04, 0.04],\n",
       " [0.12, 0.0, 0.08, 0.12, 0.24, 0.36, 0.04, 0.0, 0.04],\n",
       " [0.0, 0.04, 0.12, 0.32, 0.44, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.36, 0.28, 0.0, 0.28, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.6, 0.0, 0.0, 0.0, 0.12, 0.24, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.52, 0.2, 0.0, 0.2, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.92, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.36, 0.28, 0.0, 0.24, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.92, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0],\n",
       " [0.88, 0.0, 0.0, 0.0, 0.0, 0.08, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.28, 0.0, 0.56, 0.0, 0.08, 0.0, 0.08],\n",
       " [0.0, 0.0, 0.08, 0.44, 0.36, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.4, 0.0, 0.0, 0.04, 0.0, 0.56, 0.0, 0.0, 0.0],\n",
       " [0.96, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.56, 0.36, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.0, 0.4, 0.4, 0.0, 0.12, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.28, 0.0, 0.0, 0.0, 0.08, 0.44, 0.16, 0.0, 0.04],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.12, 0.12, 0.64, 0.04, 0.0, 0.0, 0.04],\n",
       " [0.04, 0.0, 0.08, 0.28, 0.4, 0.12, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.6, 0.16, 0.0, 0.08, 0.0, 0.16, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.08, 0.28, 0.36, 0.12, 0.08, 0.0, 0.04],\n",
       " [0.44, 0.0, 0.12, 0.0, 0.24, 0.0, 0.2, 0.0, 0.0],\n",
       " [0.16, 0.0, 0.0, 0.12, 0.32, 0.36, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.4, 0.28, 0.0, 0.12, 0.0, 0.2, 0.0, 0.0],\n",
       " [0.0, 0.16, 0.08, 0.16, 0.56, 0.0, 0.0, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.56, 0.32, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.4, 0.44, 0.04, 0.08, 0.0, 0.04],\n",
       " [0.32, 0.0, 0.0, 0.0, 0.04, 0.6, 0.0, 0.04, 0.0],\n",
       " [0.0, 0.56, 0.32, 0.0, 0.04, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.76, 0.0, 0.0, 0.0, 0.04, 0.08, 0.12, 0.0, 0.0],\n",
       " [0.04, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.96, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.6, 0.0, 0.0, 0.0, 0.04, 0.32, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.84, 0.08, 0.0, 0.0, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.08, 0.0, 0.0, 0.48, 0.2, 0.12, 0.08, 0.0, 0.04],\n",
       " [0.68, 0.0, 0.08, 0.0, 0.04, 0.0, 0.2, 0.0, 0.0],\n",
       " [0.0, 0.36, 0.36, 0.0, 0.24, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.28, 0.0, 0.0, 0.04, 0.12, 0.48, 0.04, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.68, 0.28, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.96, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.6, 0.0, 0.0, 0.0, 0.12, 0.24, 0.04, 0.0, 0.0],\n",
       " [0.56, 0.0, 0.0, 0.0, 0.08, 0.28, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.04, 0.28, 0.0, 0.52, 0.0, 0.16, 0.0, 0.0],\n",
       " [0.0, 0.96, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.16, 0.44, 0.0, 0.28, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.88, 0.0, 0.0, 0.0, 0.0, 0.08, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.2, 0.08, 0.48, 0.04, 0.12, 0.0, 0.08],\n",
       " [0.0, 0.0, 0.0, 0.56, 0.4, 0.0, 0.0, 0.0, 0.04],\n",
       " [0.76, 0.0, 0.08, 0.0, 0.08, 0.04, 0.04, 0.0, 0.0],\n",
       " [0.08, 0.08, 0.28, 0.0, 0.12, 0.16, 0.24, 0.04, 0.0],\n",
       " [0.0, 0.88, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04],\n",
       " [0.68, 0.0, 0.0, 0.04, 0.0, 0.28, 0.0, 0.0, 0.0],\n",
       " [0.44, 0.0, 0.08, 0.0, 0.2, 0.0, 0.28, 0.0, 0.0],\n",
       " [0.04, 0.6, 0.2, 0.0, 0.08, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.4, 0.0, 0.0, 0.04, 0.08, 0.48, 0.0, 0.0, 0.0],\n",
       " [0.56, 0.0, 0.0, 0.0, 0.12, 0.16, 0.16, 0.0, 0.0],\n",
       " [0.08, 0.0, 0.04, 0.2, 0.28, 0.36, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.04, 0.56, 0.32, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.44, 0.0, 0.0, 0.04, 0.0, 0.52, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.04, 0.88, 0.08, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.0, 0.52, 0.36, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.2, 0.0, 0.28, 0.0, 0.28, 0.04, 0.2, 0.0, 0.0],\n",
       " [0.56, 0.0, 0.04, 0.0, 0.04, 0.28, 0.08, 0.0, 0.0],\n",
       " [0.16, 0.0, 0.16, 0.0, 0.36, 0.08, 0.2, 0.0, 0.04],\n",
       " [0.0, 0.32, 0.4, 0.04, 0.12, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.36, 0.0, 0.04, 0.0, 0.12, 0.36, 0.08, 0.0, 0.04],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.08, 0.2, 0.48, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.36, 0.04, 0.24, 0.0, 0.04, 0.2, 0.12, 0.0, 0.0],\n",
       " [0.0, 0.24, 0.2, 0.08, 0.28, 0.0, 0.2, 0.0, 0.0],\n",
       " [0.0, 0.12, 0.28, 0.04, 0.44, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.4, 0.0, 0.0, 0.04, 0.04, 0.52, 0.0, 0.0, 0.0],\n",
       " [0.76, 0.0, 0.0, 0.04, 0.04, 0.04, 0.12, 0.0, 0.0],\n",
       " [0.84, 0.0, 0.0, 0.0, 0.08, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.4, 0.0, 0.08, 0.0, 0.04, 0.4, 0.04, 0.04, 0.0],\n",
       " [0.44, 0.04, 0.2, 0.04, 0.08, 0.0, 0.2, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.0, 0.84, 0.12, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.28, 0.36, 0.04, 0.28, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.12, 0.36, 0.0, 0.4, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.32, 0.0, 0.08, 0.0, 0.2, 0.24, 0.12, 0.0, 0.04],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.52, 0.0, 0.04, 0.0, 0.04, 0.36, 0.0, 0.0, 0.04],\n",
       " [0.32, 0.0, 0.0, 0.0, 0.0, 0.68, 0.0, 0.0, 0.0],\n",
       " [0.36, 0.0, 0.12, 0.04, 0.16, 0.0, 0.24, 0.04, 0.04],\n",
       " [0.72, 0.0, 0.04, 0.0, 0.0, 0.24, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.12, 0.12, 0.6, 0.0, 0.12, 0.0, 0.04],\n",
       " [0.64, 0.0, 0.04, 0.0, 0.04, 0.16, 0.12, 0.0, 0.0],\n",
       " [0.44, 0.0, 0.0, 0.0, 0.0, 0.56, 0.0, 0.0, 0.0],\n",
       " [0.8, 0.0, 0.0, 0.0, 0.0, 0.12, 0.08, 0.0, 0.0],\n",
       " [0.32, 0.0, 0.16, 0.0, 0.16, 0.08, 0.2, 0.0, 0.08],\n",
       " [0.0, 0.0, 0.0, 0.88, 0.12, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.96, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.28, 0.44, 0.0, 0.24, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.04, 0.28, 0.36, 0.16, 0.12, 0.0, 0.04],\n",
       " [0.0, 0.96, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.04, 0.16, 0.68, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.44, 0.0, 0.0, 0.0, 0.04, 0.52, 0.0, 0.0, 0.0],\n",
       " [0.12, 0.2, 0.2, 0.0, 0.2, 0.0, 0.28, 0.0, 0.0],\n",
       " [0.36, 0.0, 0.08, 0.0, 0.24, 0.28, 0.0, 0.0, 0.04],\n",
       " [0.04, 0.04, 0.48, 0.0, 0.2, 0.0, 0.2, 0.0, 0.04],\n",
       " [0.24, 0.28, 0.2, 0.0, 0.12, 0.04, 0.12, 0.0, 0.0],\n",
       " [0.36, 0.0, 0.16, 0.0, 0.36, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.16, 0.0, 0.12, 0.0, 0.16, 0.32, 0.24, 0.0, 0.0],\n",
       " [0.04, 0.32, 0.36, 0.0, 0.16, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.12, 0.0, 0.0, 0.08, 0.36, 0.36, 0.04, 0.0, 0.04],\n",
       " [0.0, 0.56, 0.36, 0.0, 0.04, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.44, 0.0, 0.0, 0.0, 0.0, 0.56, 0.0, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.0, 0.32, 0.32, 0.2, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.2, 0.4, 0.0, 0.24, 0.0, 0.16, 0.0, 0.0],\n",
       " [0.64, 0.0, 0.0, 0.0, 0.04, 0.28, 0.04, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.12, 0.0, 0.32, 0.2, 0.2, 0.0, 0.04],\n",
       " [0.04, 0.0, 0.28, 0.0, 0.44, 0.0, 0.16, 0.04, 0.04],\n",
       " [0.24, 0.28, 0.12, 0.0, 0.08, 0.12, 0.12, 0.0, 0.04],\n",
       " [0.44, 0.0, 0.08, 0.0, 0.2, 0.24, 0.0, 0.0, 0.04],\n",
       " [0.08, 0.0, 0.0, 0.28, 0.52, 0.08, 0.04, 0.0, 0.0],\n",
       " [0.04, 0.12, 0.32, 0.0, 0.32, 0.0, 0.2, 0.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.64, 0.28, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.24, 0.0, 0.0, 0.08, 0.12, 0.4, 0.16, 0.0, 0.0],\n",
       " [0.0, 0.4, 0.36, 0.0, 0.12, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.6, 0.0, 0.04, 0.0, 0.12, 0.24, 0.0, 0.0, 0.0],\n",
       " [0.04, 0.68, 0.2, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.96, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.2, 0.0, 0.04, 0.44, 0.16, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.56, 0.4, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.12, 0.2, 0.4, 0.0, 0.08, 0.0, 0.16, 0.0, 0.04],\n",
       " [0.68, 0.0, 0.0, 0.0, 0.0, 0.28, 0.04, 0.0, 0.0],\n",
       " [0.28, 0.08, 0.28, 0.0, 0.16, 0.0, 0.16, 0.0, 0.04],\n",
       " [0.36, 0.0, 0.0, 0.0, 0.12, 0.48, 0.0, 0.0, 0.04],\n",
       " [0.0, 0.76, 0.16, 0.0, 0.04, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.08, 0.12, 0.36, 0.24, 0.16, 0.0, 0.04],\n",
       " [0.08, 0.04, 0.36, 0.0, 0.36, 0.0, 0.16, 0.0, 0.0],\n",
       " [0.0, 0.28, 0.28, 0.0, 0.32, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.84, 0.0, 0.04, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.0, 0.16, 0.36, 0.0, 0.36, 0.04, 0.04, 0.0, 0.04],\n",
       " [0.24, 0.0, 0.24, 0.0, 0.36, 0.0, 0.16, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.72, 0.28, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.28, 0.0, 0.0, 0.0, 0.0, 0.68, 0.04, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.0, 0.64, 0.28, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.12, 0.08, 0.12, 0.0, 0.12, 0.44, 0.08, 0.0, 0.04],\n",
       " [0.44, 0.0, 0.08, 0.0, 0.12, 0.32, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.08, 0.48, 0.4, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.16, 0.0, 0.28, 0.04, 0.24, 0.04, 0.24, 0.0, 0.0],\n",
       " [0.64, 0.0, 0.0, 0.0, 0.0, 0.32, 0.04, 0.0, 0.0],\n",
       " [0.88, 0.0, 0.0, 0.0, 0.0, 0.04, 0.04, 0.04, 0.0],\n",
       " [0.48, 0.0, 0.0, 0.0, 0.0, 0.44, 0.04, 0.04, 0.0],\n",
       " [0.08, 0.0, 0.04, 0.16, 0.64, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.04, 0.24, 0.36, 0.28, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.04, 0.28, 0.44, 0.0, 0.04, 0.04, 0.16, 0.0, 0.0],\n",
       " [0.36, 0.0, 0.28, 0.0, 0.12, 0.0, 0.2, 0.0, 0.04],\n",
       " [0.08, 0.0, 0.08, 0.08, 0.24, 0.4, 0.12, 0.0, 0.0],\n",
       " [0.52, 0.0, 0.04, 0.0, 0.12, 0.28, 0.04, 0.0, 0.0],\n",
       " [0.44, 0.0, 0.0, 0.0, 0.04, 0.52, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.04, 0.28, 0.0, 0.48, 0.0, 0.12, 0.04, 0.04],\n",
       " [0.0, 0.24, 0.44, 0.04, 0.2, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.56, 0.0, 0.0, 0.0, 0.04, 0.36, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.44, 0.28, 0.0, 0.16, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.32, 0.12, 0.24, 0.0, 0.12, 0.0, 0.2, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.04, 0.72, 0.24, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.04, 0.24, 0.16, 0.4, 0.0, 0.12, 0.0, 0.04],\n",
       " [0.24, 0.4, 0.12, 0.0, 0.04, 0.04, 0.16, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.0, 0.72, 0.2, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.04, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.44, 0.0, 0.04, 0.0, 0.16, 0.24, 0.08, 0.04, 0.0],\n",
       " [0.28, 0.0, 0.2, 0.0, 0.04, 0.24, 0.24, 0.0, 0.0],\n",
       " [0.92, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0],\n",
       " [0.6, 0.0, 0.04, 0.0, 0.12, 0.2, 0.04, 0.0, 0.0],\n",
       " [0.2, 0.0, 0.2, 0.0, 0.08, 0.28, 0.24, 0.0, 0.0],\n",
       " [0.08, 0.0, 0.08, 0.2, 0.56, 0.04, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.08, 0.52, 0.4, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.32, 0.0, 0.04, 0.0, 0.32, 0.28, 0.04, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.04, 0.04, 0.36, 0.4, 0.04, 0.0, 0.0],\n",
       " [0.96, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.96, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.52, 0.0, 0.0, 0.0, 0.0, 0.36, 0.08, 0.04, 0.0],\n",
       " [0.0, 0.08, 0.4, 0.0, 0.32, 0.0, 0.16, 0.0, 0.04],\n",
       " [0.04, 0.72, 0.12, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.12, 0.32, 0.52, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.4, 0.32, 0.0, 0.12, 0.0, 0.12, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.72, 0.2, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.32, 0.0, 0.08, 0.12, 0.32, 0.04, 0.12, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.0, 0.28, 0.36, 0.16, 0.08, 0.0, 0.0],\n",
       " [0.72, 0.0, 0.04, 0.0, 0.04, 0.16, 0.0, 0.04, 0.0],\n",
       " [0.04, 0.44, 0.32, 0.0, 0.08, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.2, 0.24, 0.28, 0.0, 0.08, 0.04, 0.16, 0.0, 0.0],\n",
       " [0.32, 0.0, 0.08, 0.08, 0.28, 0.16, 0.04, 0.0, 0.04],\n",
       " [0.04, 0.52, 0.32, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.0, 0.4, 0.2, 0.0, 0.24, 0.0, 0.16, 0.0, 0.0],\n",
       " [0.8, 0.0, 0.0, 0.0, 0.04, 0.12, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.84, 0.16, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.04, 0.16, 0.24, 0.0, 0.28, 0.08, 0.2, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.96, 0.04, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.44, 0.0, 0.24, 0.0, 0.28, 0.0, 0.0],\n",
       " [0.0, 0.92, 0.04, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.56, 0.0, 0.0, 0.0, 0.04, 0.36, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.56, 0.32, 0.0, 0.04, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.48, 0.0, 0.0, 0.0, 0.0, 0.52, 0.0, 0.0, 0.0],\n",
       " [0.28, 0.0, 0.08, 0.0, 0.28, 0.12, 0.2, 0.0, 0.04],\n",
       " [0.48, 0.0, 0.0, 0.0, 0.04, 0.44, 0.04, 0.0, 0.0],\n",
       " [0.2, 0.0, 0.2, 0.0, 0.24, 0.2, 0.12, 0.0, 0.04],\n",
       " [0.24, 0.12, 0.24, 0.0, 0.12, 0.0, 0.24, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.76, 0.24, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.2, 0.24, 0.04, 0.36, 0.0, 0.12, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.92, 0.08, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.08, 0.08, 0.44, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0],\n",
       " [0.04, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.32, 0.0, 0.04, 0.04, 0.24, 0.24, 0.08, 0.0, 0.04],\n",
       " [0.04, 0.0, 0.12, 0.48, 0.28, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.36, 0.0, 0.0, 0.0, 0.0, 0.64, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.24, 0.36, 0.04, 0.28, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.44, 0.48, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.72, 0.2, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.24, 0.0, 0.44, 0.0, 0.0, 0.0, 0.28, 0.04, 0.0],\n",
       " [0.2, 0.0, 0.12, 0.0, 0.08, 0.48, 0.12, 0.0, 0.0],\n",
       " [0.0, 0.36, 0.36, 0.0, 0.24, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.48, 0.0, 0.04, 0.0, 0.04, 0.36, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.4, 0.44, 0.0, 0.12, 0.0, 0.04],\n",
       " [0.48, 0.0, 0.04, 0.0, 0.04, 0.4, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.68, 0.28, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.08, 0.0, 0.04, 0.24, 0.36, 0.16, 0.08, 0.0, 0.04],\n",
       " [0.44, 0.0, 0.24, 0.0, 0.12, 0.04, 0.16, 0.0, 0.0],\n",
       " [0.24, 0.16, 0.2, 0.0, 0.08, 0.08, 0.2, 0.04, 0.0],\n",
       " [0.16, 0.0, 0.12, 0.12, 0.32, 0.08, 0.16, 0.0, 0.04],\n",
       " [0.4, 0.0, 0.0, 0.0, 0.08, 0.48, 0.04, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.2, 0.04, 0.36, 0.12, 0.16, 0.0, 0.0],\n",
       " [0.92, 0.0, 0.0, 0.0, 0.04, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.28, 0.0, 0.08, 0.0, 0.24, 0.32, 0.04, 0.0, 0.04],\n",
       " [0.36, 0.0, 0.0, 0.0, 0.04, 0.6, 0.0, 0.0, 0.0],\n",
       " [0.92, 0.0, 0.0, 0.0, 0.04, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.36, 0.0, 0.04, 0.0, 0.0, 0.52, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.64, 0.28, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.24, 0.0, 0.12, 0.0, 0.32, 0.12, 0.12, 0.0, 0.08],\n",
       " [0.36, 0.0, 0.08, 0.0, 0.04, 0.44, 0.04, 0.04, 0.0],\n",
       " [0.0, 0.76, 0.12, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.0, 0.52, 0.36, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.72, 0.24, 0.0, 0.0, 0.0, 0.04],\n",
       " [0.24, 0.0, 0.28, 0.0, 0.08, 0.0, 0.36, 0.04, 0.0],\n",
       " [0.04, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.2, 0.0, 0.0, 0.16, 0.16, 0.48, 0.0, 0.0, 0.0],\n",
       " [0.6, 0.0, 0.04, 0.0, 0.04, 0.24, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.64, 0.36, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.12, 0.32, 0.2, 0.0, 0.04, 0.12, 0.2, 0.0, 0.0],\n",
       " [0.08, 0.2, 0.36, 0.0, 0.2, 0.0, 0.12, 0.0, 0.04],\n",
       " [0.04, 0.0, 0.0, 0.68, 0.2, 0.04, 0.04, 0.0, 0.0],\n",
       " [0.64, 0.0, 0.08, 0.0, 0.0, 0.2, 0.08, 0.0, 0.0],\n",
       " [0.08, 0.08, 0.4, 0.0, 0.12, 0.0, 0.28, 0.0, 0.04],\n",
       " [0.04, 0.0, 0.0, 0.72, 0.24, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l1 = term_data[term_data.get('#Lnum').eq(1)]\n",
    "unique_symbols = list(l1['Term Abbrev'].unique())\n",
    "l1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\n",
    "display(l1_grouped)\n",
    "l1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) \\\n",
    "                              for abbrev in unique_symbols] for i in range(len(l1_grouped))]\n",
    "#display(l1_chip_abbrev_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         LB    LE    WK    LF     F     G     S   GB    FU\n",
      "#cnum                                                     \n",
      "1      0.36  0.00  0.00  0.04  0.08  0.52  0.00  0.0  0.00\n",
      "2      0.12  0.00  0.04  0.12  0.60  0.04  0.00  0.0  0.08\n",
      "3      0.00  0.96  0.04  0.00  0.00  0.00  0.00  0.0  0.00\n",
      "4      0.28  0.00  0.40  0.00  0.08  0.04  0.20  0.0  0.00\n",
      "5      0.04  0.00  0.00  0.20  0.68  0.08  0.00  0.0  0.00\n",
      "...     ...   ...   ...   ...   ...   ...   ...  ...   ...\n",
      "326    0.08  0.20  0.36  0.00  0.20  0.00  0.12  0.0  0.04\n",
      "327    0.04  0.00  0.00  0.68  0.20  0.04  0.04  0.0  0.00\n",
      "328    0.64  0.00  0.08  0.00  0.00  0.20  0.08  0.0  0.00\n",
      "329    0.08  0.08  0.40  0.00  0.12  0.00  0.28  0.0  0.04\n",
      "330    0.04  0.00  0.00  0.72  0.24  0.00  0.00  0.0  0.00\n",
      "\n",
      "[330 rows x 9 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.36, 0.0, 0.0, 0.04, 0.08, 0.52, 0.0, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.04, 0.12, 0.6, 0.04, 0.0, 0.0, 0.08],\n",
       " [0.0, 0.96, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.28, 0.0, 0.4, 0.0, 0.08, 0.04, 0.2, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.0, 0.2, 0.68, 0.08, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.16, 0.48, 0.08, 0.2, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.24, 0.0, 0.04, 0.04, 0.32, 0.2, 0.16, 0.0, 0.0],\n",
       " [0.0, 0.72, 0.08, 0.0, 0.12, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.12, 0.04, 0.12, 0.0, 0.4, 0.0, 0.32, 0.0, 0.0],\n",
       " [0.52, 0.0, 0.0, 0.0, 0.0, 0.48, 0.0, 0.0, 0.0],\n",
       " [0.64, 0.0, 0.04, 0.0, 0.12, 0.12, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.16, 0.32, 0.04, 0.4, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.8, 0.0, 0.0, 0.04, 0.04, 0.08, 0.04, 0.0, 0.0],\n",
       " [0.8, 0.0, 0.04, 0.0, 0.0, 0.08, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.08, 0.12, 0.12, 0.64, 0.04, 0.0, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.0, 0.8, 0.04, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.88, 0.0, 0.04, 0.0, 0.04, 0.04, 0.0, 0.0, 0.0],\n",
       " [0.16, 0.0, 0.08, 0.08, 0.24, 0.44, 0.0, 0.0, 0.0],\n",
       " [0.36, 0.0, 0.08, 0.04, 0.32, 0.04, 0.16, 0.0, 0.0],\n",
       " [0.0, 0.84, 0.08, 0.04, 0.04, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.08, 0.16, 0.16, 0.48, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.12, 0.04, 0.32, 0.04, 0.2, 0.12, 0.12, 0.0, 0.04],\n",
       " [0.8, 0.0, 0.08, 0.0, 0.04, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.44, 0.0, 0.08, 0.0, 0.08, 0.36, 0.04, 0.0, 0.0],\n",
       " [0.32, 0.0, 0.0, 0.0, 0.04, 0.6, 0.04, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.12, 0.36, 0.44, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.96, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.32, 0.04, 0.04, 0.04, 0.36, 0.04, 0.12, 0.0, 0.04],\n",
       " [0.04, 0.44, 0.4, 0.0, 0.08, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.12, 0.04, 0.04, 0.24, 0.36, 0.12, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.28, 0.2, 0.04, 0.24, 0.04, 0.2, 0.0, 0.0],\n",
       " [0.16, 0.04, 0.16, 0.0, 0.4, 0.12, 0.08, 0.0, 0.04],\n",
       " [0.48, 0.0, 0.04, 0.0, 0.0, 0.48, 0.0, 0.0, 0.0],\n",
       " [0.04, 0.48, 0.32, 0.0, 0.12, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.72, 0.04, 0.0, 0.0, 0.08, 0.08, 0.08, 0.0, 0.0],\n",
       " [0.32, 0.0, 0.12, 0.0, 0.12, 0.36, 0.08, 0.0, 0.0],\n",
       " [0.04, 0.84, 0.08, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.6, 0.04, 0.12, 0.0, 0.08, 0.08, 0.08, 0.0, 0.0],\n",
       " [0.28, 0.0, 0.04, 0.12, 0.16, 0.28, 0.12, 0.0, 0.0],\n",
       " [0.04, 0.88, 0.0, 0.0, 0.04, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.04, 0.04, 0.32, 0.0, 0.4, 0.0, 0.2, 0.0, 0.0],\n",
       " [0.6, 0.0, 0.0, 0.0, 0.04, 0.32, 0.04, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.08, 0.44, 0.44, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.32, 0.0, 0.12, 0.08, 0.12, 0.2, 0.12, 0.0, 0.04],\n",
       " [0.4, 0.0, 0.08, 0.08, 0.24, 0.04, 0.16, 0.0, 0.0],\n",
       " [0.8, 0.0, 0.04, 0.04, 0.0, 0.08, 0.04, 0.0, 0.0],\n",
       " [0.04, 0.16, 0.36, 0.04, 0.24, 0.0, 0.16, 0.0, 0.0],\n",
       " [0.16, 0.04, 0.08, 0.16, 0.28, 0.16, 0.08, 0.0, 0.04],\n",
       " [0.32, 0.04, 0.32, 0.0, 0.04, 0.08, 0.2, 0.0, 0.0],\n",
       " [0.36, 0.04, 0.2, 0.0, 0.28, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.04, 0.6, 0.24, 0.04, 0.04, 0.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.56, 0.0, 0.16, 0.0, 0.08, 0.04, 0.16, 0.0, 0.0],\n",
       " [0.24, 0.0, 0.0, 0.04, 0.16, 0.36, 0.16, 0.0, 0.04],\n",
       " [0.0, 0.24, 0.2, 0.08, 0.44, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.68, 0.0, 0.0, 0.0, 0.04, 0.28, 0.0, 0.0, 0.0],\n",
       " [0.08, 0.0, 0.4, 0.0, 0.12, 0.04, 0.24, 0.04, 0.08],\n",
       " [0.0, 0.0, 0.16, 0.28, 0.44, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.96, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.96, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.16, 0.0, 0.08, 0.0, 0.24, 0.28, 0.2, 0.0, 0.04],\n",
       " [0.0, 0.44, 0.16, 0.04, 0.28, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.48, 0.0, 0.0, 0.0, 0.04, 0.48, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.04, 0.48, 0.4, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.12, 0.0, 0.12, 0.08, 0.4, 0.12, 0.12, 0.0, 0.04],\n",
       " [0.0, 0.56, 0.24, 0.0, 0.12, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.48, 0.0, 0.2, 0.0, 0.12, 0.0, 0.16, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.48, 0.32, 0.0, 0.16, 0.0, 0.04],\n",
       " [0.16, 0.28, 0.28, 0.0, 0.24, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.48, 0.0, 0.0, 0.04, 0.08, 0.28, 0.12, 0.0, 0.0],\n",
       " [0.12, 0.08, 0.16, 0.0, 0.36, 0.12, 0.16, 0.0, 0.0],\n",
       " [0.08, 0.0, 0.0, 0.4, 0.32, 0.12, 0.04, 0.0, 0.04],\n",
       " [0.88, 0.0, 0.0, 0.0, 0.0, 0.04, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.72, 0.24, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.48, 0.0, 0.0, 0.0, 0.04, 0.4, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.08, 0.12, 0.16, 0.6, 0.0, 0.0, 0.0, 0.04],\n",
       " [0.88, 0.0, 0.04, 0.0, 0.04, 0.04, 0.0, 0.0, 0.0],\n",
       " [0.48, 0.0, 0.0, 0.0, 0.04, 0.44, 0.04, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.2, 0.0, 0.44, 0.0, 0.16, 0.04, 0.04],\n",
       " [0.12, 0.0, 0.08, 0.12, 0.24, 0.36, 0.04, 0.0, 0.04],\n",
       " [0.0, 0.04, 0.12, 0.32, 0.44, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.36, 0.28, 0.0, 0.28, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.6, 0.0, 0.0, 0.0, 0.12, 0.24, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.52, 0.2, 0.0, 0.2, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.92, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.36, 0.28, 0.0, 0.24, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.92, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0],\n",
       " [0.88, 0.0, 0.0, 0.0, 0.0, 0.08, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.28, 0.0, 0.56, 0.0, 0.08, 0.0, 0.08],\n",
       " [0.0, 0.0, 0.08, 0.44, 0.36, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.4, 0.0, 0.0, 0.04, 0.0, 0.56, 0.0, 0.0, 0.0],\n",
       " [0.96, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.56, 0.36, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.0, 0.4, 0.4, 0.0, 0.12, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.28, 0.0, 0.0, 0.0, 0.08, 0.44, 0.16, 0.0, 0.04],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.12, 0.12, 0.64, 0.04, 0.0, 0.0, 0.04],\n",
       " [0.04, 0.0, 0.08, 0.28, 0.4, 0.12, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.6, 0.16, 0.0, 0.08, 0.0, 0.16, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.08, 0.28, 0.36, 0.12, 0.08, 0.0, 0.04],\n",
       " [0.44, 0.0, 0.12, 0.0, 0.24, 0.0, 0.2, 0.0, 0.0],\n",
       " [0.16, 0.0, 0.0, 0.12, 0.32, 0.36, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.4, 0.28, 0.0, 0.12, 0.0, 0.2, 0.0, 0.0],\n",
       " [0.0, 0.16, 0.08, 0.16, 0.56, 0.0, 0.0, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.56, 0.32, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.4, 0.44, 0.04, 0.08, 0.0, 0.04],\n",
       " [0.32, 0.0, 0.0, 0.0, 0.04, 0.6, 0.0, 0.04, 0.0],\n",
       " [0.0, 0.56, 0.32, 0.0, 0.04, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.76, 0.0, 0.0, 0.0, 0.04, 0.08, 0.12, 0.0, 0.0],\n",
       " [0.04, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.96, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.6, 0.0, 0.0, 0.0, 0.04, 0.32, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.84, 0.08, 0.0, 0.0, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.08, 0.0, 0.0, 0.48, 0.2, 0.12, 0.08, 0.0, 0.04],\n",
       " [0.68, 0.0, 0.08, 0.0, 0.04, 0.0, 0.2, 0.0, 0.0],\n",
       " [0.0, 0.36, 0.36, 0.0, 0.24, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.28, 0.0, 0.0, 0.04, 0.12, 0.48, 0.04, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.68, 0.28, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.96, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.6, 0.0, 0.0, 0.0, 0.12, 0.24, 0.04, 0.0, 0.0],\n",
       " [0.56, 0.0, 0.0, 0.0, 0.08, 0.28, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.04, 0.28, 0.0, 0.52, 0.0, 0.16, 0.0, 0.0],\n",
       " [0.0, 0.96, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.16, 0.44, 0.0, 0.28, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.88, 0.0, 0.0, 0.0, 0.0, 0.08, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.2, 0.08, 0.48, 0.04, 0.12, 0.0, 0.08],\n",
       " [0.0, 0.0, 0.0, 0.56, 0.4, 0.0, 0.0, 0.0, 0.04],\n",
       " [0.76, 0.0, 0.08, 0.0, 0.08, 0.04, 0.04, 0.0, 0.0],\n",
       " [0.08, 0.08, 0.28, 0.0, 0.12, 0.16, 0.24, 0.04, 0.0],\n",
       " [0.0, 0.88, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04],\n",
       " [0.68, 0.0, 0.0, 0.04, 0.0, 0.28, 0.0, 0.0, 0.0],\n",
       " [0.44, 0.0, 0.08, 0.0, 0.2, 0.0, 0.28, 0.0, 0.0],\n",
       " [0.04, 0.6, 0.2, 0.0, 0.08, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.4, 0.0, 0.0, 0.04, 0.08, 0.48, 0.0, 0.0, 0.0],\n",
       " [0.56, 0.0, 0.0, 0.0, 0.12, 0.16, 0.16, 0.0, 0.0],\n",
       " [0.08, 0.0, 0.04, 0.2, 0.28, 0.36, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.04, 0.56, 0.32, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.44, 0.0, 0.0, 0.04, 0.0, 0.52, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.04, 0.88, 0.08, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.0, 0.52, 0.36, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.2, 0.0, 0.28, 0.0, 0.28, 0.04, 0.2, 0.0, 0.0],\n",
       " [0.56, 0.0, 0.04, 0.0, 0.04, 0.28, 0.08, 0.0, 0.0],\n",
       " [0.16, 0.0, 0.16, 0.0, 0.36, 0.08, 0.2, 0.0, 0.04],\n",
       " [0.0, 0.32, 0.4, 0.04, 0.12, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.36, 0.0, 0.04, 0.0, 0.12, 0.36, 0.08, 0.0, 0.04],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.08, 0.2, 0.48, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.36, 0.04, 0.24, 0.0, 0.04, 0.2, 0.12, 0.0, 0.0],\n",
       " [0.0, 0.24, 0.2, 0.08, 0.28, 0.0, 0.2, 0.0, 0.0],\n",
       " [0.0, 0.12, 0.28, 0.04, 0.44, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.4, 0.0, 0.0, 0.04, 0.04, 0.52, 0.0, 0.0, 0.0],\n",
       " [0.76, 0.0, 0.0, 0.04, 0.04, 0.04, 0.12, 0.0, 0.0],\n",
       " [0.84, 0.0, 0.0, 0.0, 0.08, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.4, 0.0, 0.08, 0.0, 0.04, 0.4, 0.04, 0.04, 0.0],\n",
       " [0.44, 0.04, 0.2, 0.04, 0.08, 0.0, 0.2, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.0, 0.84, 0.12, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.28, 0.36, 0.04, 0.28, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.12, 0.36, 0.0, 0.4, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.32, 0.0, 0.08, 0.0, 0.2, 0.24, 0.12, 0.0, 0.04],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.52, 0.0, 0.04, 0.0, 0.04, 0.36, 0.0, 0.0, 0.04],\n",
       " [0.32, 0.0, 0.0, 0.0, 0.0, 0.68, 0.0, 0.0, 0.0],\n",
       " [0.36, 0.0, 0.12, 0.04, 0.16, 0.0, 0.24, 0.04, 0.04],\n",
       " [0.72, 0.0, 0.04, 0.0, 0.0, 0.24, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.12, 0.12, 0.6, 0.0, 0.12, 0.0, 0.04],\n",
       " [0.64, 0.0, 0.04, 0.0, 0.04, 0.16, 0.12, 0.0, 0.0],\n",
       " [0.44, 0.0, 0.0, 0.0, 0.0, 0.56, 0.0, 0.0, 0.0],\n",
       " [0.8, 0.0, 0.0, 0.0, 0.0, 0.12, 0.08, 0.0, 0.0],\n",
       " [0.32, 0.0, 0.16, 0.0, 0.16, 0.08, 0.2, 0.0, 0.08],\n",
       " [0.0, 0.0, 0.0, 0.88, 0.12, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.96, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.28, 0.44, 0.0, 0.24, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.04, 0.28, 0.36, 0.16, 0.12, 0.0, 0.04],\n",
       " [0.0, 0.96, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.04, 0.16, 0.68, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.44, 0.0, 0.0, 0.0, 0.04, 0.52, 0.0, 0.0, 0.0],\n",
       " [0.12, 0.2, 0.2, 0.0, 0.2, 0.0, 0.28, 0.0, 0.0],\n",
       " [0.36, 0.0, 0.08, 0.0, 0.24, 0.28, 0.0, 0.0, 0.04],\n",
       " [0.04, 0.04, 0.48, 0.0, 0.2, 0.0, 0.2, 0.0, 0.04],\n",
       " [0.24, 0.28, 0.2, 0.0, 0.12, 0.04, 0.12, 0.0, 0.0],\n",
       " [0.36, 0.0, 0.16, 0.0, 0.36, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.16, 0.0, 0.12, 0.0, 0.16, 0.32, 0.24, 0.0, 0.0],\n",
       " [0.04, 0.32, 0.36, 0.0, 0.16, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.12, 0.0, 0.0, 0.08, 0.36, 0.36, 0.04, 0.0, 0.04],\n",
       " [0.0, 0.56, 0.36, 0.0, 0.04, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.44, 0.0, 0.0, 0.0, 0.0, 0.56, 0.0, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.0, 0.32, 0.32, 0.2, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.2, 0.4, 0.0, 0.24, 0.0, 0.16, 0.0, 0.0],\n",
       " [0.64, 0.0, 0.0, 0.0, 0.04, 0.28, 0.04, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.12, 0.0, 0.32, 0.2, 0.2, 0.0, 0.04],\n",
       " [0.04, 0.0, 0.28, 0.0, 0.44, 0.0, 0.16, 0.04, 0.04],\n",
       " [0.24, 0.28, 0.12, 0.0, 0.08, 0.12, 0.12, 0.0, 0.04],\n",
       " [0.44, 0.0, 0.08, 0.0, 0.2, 0.24, 0.0, 0.0, 0.04],\n",
       " [0.08, 0.0, 0.0, 0.28, 0.52, 0.08, 0.04, 0.0, 0.0],\n",
       " [0.04, 0.12, 0.32, 0.0, 0.32, 0.0, 0.2, 0.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.64, 0.28, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.24, 0.0, 0.0, 0.08, 0.12, 0.4, 0.16, 0.0, 0.0],\n",
       " [0.0, 0.4, 0.36, 0.0, 0.12, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.6, 0.0, 0.04, 0.0, 0.12, 0.24, 0.0, 0.0, 0.0],\n",
       " [0.04, 0.68, 0.2, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.96, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.2, 0.0, 0.04, 0.44, 0.16, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.56, 0.4, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.12, 0.2, 0.4, 0.0, 0.08, 0.0, 0.16, 0.0, 0.04],\n",
       " [0.68, 0.0, 0.0, 0.0, 0.0, 0.28, 0.04, 0.0, 0.0],\n",
       " [0.28, 0.08, 0.28, 0.0, 0.16, 0.0, 0.16, 0.0, 0.04],\n",
       " [0.36, 0.0, 0.0, 0.0, 0.12, 0.48, 0.0, 0.0, 0.04],\n",
       " [0.0, 0.76, 0.16, 0.0, 0.04, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.08, 0.12, 0.36, 0.24, 0.16, 0.0, 0.04],\n",
       " [0.08, 0.04, 0.36, 0.0, 0.36, 0.0, 0.16, 0.0, 0.0],\n",
       " [0.0, 0.28, 0.28, 0.0, 0.32, 0.0, 0.08, 0.0, 0.04],\n",
       " [0.84, 0.0, 0.04, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.0, 0.16, 0.36, 0.0, 0.36, 0.04, 0.04, 0.0, 0.04],\n",
       " [0.24, 0.0, 0.24, 0.0, 0.36, 0.0, 0.16, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.72, 0.28, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.28, 0.0, 0.0, 0.0, 0.0, 0.68, 0.04, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.0, 0.64, 0.28, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.12, 0.08, 0.12, 0.0, 0.12, 0.44, 0.08, 0.0, 0.04],\n",
       " [0.44, 0.0, 0.08, 0.0, 0.12, 0.32, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.08, 0.48, 0.4, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.16, 0.0, 0.28, 0.04, 0.24, 0.04, 0.24, 0.0, 0.0],\n",
       " [0.64, 0.0, 0.0, 0.0, 0.0, 0.32, 0.04, 0.0, 0.0],\n",
       " [0.88, 0.0, 0.0, 0.0, 0.0, 0.04, 0.04, 0.04, 0.0],\n",
       " [0.48, 0.0, 0.0, 0.0, 0.0, 0.44, 0.04, 0.04, 0.0],\n",
       " [0.08, 0.0, 0.04, 0.16, 0.64, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.04, 0.24, 0.36, 0.28, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.04, 0.28, 0.44, 0.0, 0.04, 0.04, 0.16, 0.0, 0.0],\n",
       " [0.36, 0.0, 0.28, 0.0, 0.12, 0.0, 0.2, 0.0, 0.04],\n",
       " [0.08, 0.0, 0.08, 0.08, 0.24, 0.4, 0.12, 0.0, 0.0],\n",
       " [0.52, 0.0, 0.04, 0.0, 0.12, 0.28, 0.04, 0.0, 0.0],\n",
       " [0.44, 0.0, 0.0, 0.0, 0.04, 0.52, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.04, 0.28, 0.0, 0.48, 0.0, 0.12, 0.04, 0.04],\n",
       " [0.0, 0.24, 0.44, 0.04, 0.2, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.56, 0.0, 0.0, 0.0, 0.04, 0.36, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.44, 0.28, 0.0, 0.16, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.32, 0.12, 0.24, 0.0, 0.12, 0.0, 0.2, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.04, 0.72, 0.24, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.04, 0.24, 0.16, 0.4, 0.0, 0.12, 0.0, 0.04],\n",
       " [0.24, 0.4, 0.12, 0.0, 0.04, 0.04, 0.16, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.0, 0.72, 0.2, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.04, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.44, 0.0, 0.04, 0.0, 0.16, 0.24, 0.08, 0.04, 0.0],\n",
       " [0.28, 0.0, 0.2, 0.0, 0.04, 0.24, 0.24, 0.0, 0.0],\n",
       " [0.92, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0],\n",
       " [0.6, 0.0, 0.04, 0.0, 0.12, 0.2, 0.04, 0.0, 0.0],\n",
       " [0.2, 0.0, 0.2, 0.0, 0.08, 0.28, 0.24, 0.0, 0.0],\n",
       " [0.08, 0.0, 0.08, 0.2, 0.56, 0.04, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.08, 0.52, 0.4, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.32, 0.0, 0.04, 0.0, 0.32, 0.28, 0.04, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.04, 0.04, 0.36, 0.4, 0.04, 0.0, 0.0],\n",
       " [0.96, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.96, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.52, 0.0, 0.0, 0.0, 0.0, 0.36, 0.08, 0.04, 0.0],\n",
       " [0.0, 0.08, 0.4, 0.0, 0.32, 0.0, 0.16, 0.0, 0.04],\n",
       " [0.04, 0.72, 0.12, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.12, 0.32, 0.52, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.4, 0.32, 0.0, 0.12, 0.0, 0.12, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.72, 0.2, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.32, 0.0, 0.08, 0.12, 0.32, 0.04, 0.12, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.0, 0.28, 0.36, 0.16, 0.08, 0.0, 0.0],\n",
       " [0.72, 0.0, 0.04, 0.0, 0.04, 0.16, 0.0, 0.04, 0.0],\n",
       " [0.04, 0.44, 0.32, 0.0, 0.08, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.2, 0.24, 0.28, 0.0, 0.08, 0.04, 0.16, 0.0, 0.0],\n",
       " [0.32, 0.0, 0.08, 0.08, 0.28, 0.16, 0.04, 0.0, 0.04],\n",
       " [0.04, 0.52, 0.32, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.0, 0.4, 0.2, 0.0, 0.24, 0.0, 0.16, 0.0, 0.0],\n",
       " [0.8, 0.0, 0.0, 0.0, 0.04, 0.12, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.84, 0.16, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.04, 0.16, 0.24, 0.0, 0.28, 0.08, 0.2, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.96, 0.04, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.04, 0.0, 0.44, 0.0, 0.24, 0.0, 0.28, 0.0, 0.0],\n",
       " [0.0, 0.92, 0.04, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.56, 0.0, 0.0, 0.0, 0.04, 0.36, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.56, 0.32, 0.0, 0.04, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.48, 0.0, 0.0, 0.0, 0.0, 0.52, 0.0, 0.0, 0.0],\n",
       " [0.28, 0.0, 0.08, 0.0, 0.28, 0.12, 0.2, 0.0, 0.04],\n",
       " [0.48, 0.0, 0.0, 0.0, 0.04, 0.44, 0.04, 0.0, 0.0],\n",
       " [0.2, 0.0, 0.2, 0.0, 0.24, 0.2, 0.12, 0.0, 0.04],\n",
       " [0.24, 0.12, 0.24, 0.0, 0.12, 0.0, 0.24, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.76, 0.24, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.2, 0.24, 0.04, 0.36, 0.0, 0.12, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.92, 0.08, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.08, 0.08, 0.44, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0],\n",
       " [0.04, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.32, 0.0, 0.04, 0.04, 0.24, 0.24, 0.08, 0.0, 0.04],\n",
       " [0.04, 0.0, 0.12, 0.48, 0.28, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.36, 0.0, 0.0, 0.0, 0.0, 0.64, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.24, 0.36, 0.04, 0.28, 0.0, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.44, 0.48, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.0, 0.0, 0.0, 0.72, 0.2, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.24, 0.0, 0.44, 0.0, 0.0, 0.0, 0.28, 0.04, 0.0],\n",
       " [0.2, 0.0, 0.12, 0.0, 0.08, 0.48, 0.12, 0.0, 0.0],\n",
       " [0.0, 0.36, 0.36, 0.0, 0.24, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.48, 0.0, 0.04, 0.0, 0.04, 0.36, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.4, 0.44, 0.0, 0.12, 0.0, 0.04],\n",
       " [0.48, 0.0, 0.04, 0.0, 0.04, 0.4, 0.04, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.68, 0.28, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.08, 0.0, 0.04, 0.24, 0.36, 0.16, 0.08, 0.0, 0.04],\n",
       " [0.44, 0.0, 0.24, 0.0, 0.12, 0.04, 0.16, 0.0, 0.0],\n",
       " [0.24, 0.16, 0.2, 0.0, 0.08, 0.08, 0.2, 0.04, 0.0],\n",
       " [0.16, 0.0, 0.12, 0.12, 0.32, 0.08, 0.16, 0.0, 0.04],\n",
       " [0.4, 0.0, 0.0, 0.0, 0.08, 0.48, 0.04, 0.0, 0.0],\n",
       " [0.12, 0.0, 0.2, 0.04, 0.36, 0.12, 0.16, 0.0, 0.0],\n",
       " [0.92, 0.0, 0.0, 0.0, 0.04, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.28, 0.0, 0.08, 0.0, 0.24, 0.32, 0.04, 0.0, 0.04],\n",
       " [0.36, 0.0, 0.0, 0.0, 0.04, 0.6, 0.0, 0.0, 0.0],\n",
       " [0.92, 0.0, 0.0, 0.0, 0.04, 0.0, 0.04, 0.0, 0.0],\n",
       " [0.36, 0.0, 0.04, 0.0, 0.0, 0.52, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.64, 0.28, 0.0, 0.04, 0.0, 0.04],\n",
       " [0.24, 0.0, 0.12, 0.0, 0.32, 0.12, 0.12, 0.0, 0.08],\n",
       " [0.36, 0.0, 0.08, 0.0, 0.04, 0.44, 0.04, 0.04, 0.0],\n",
       " [0.0, 0.76, 0.12, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0],\n",
       " [0.0, 0.52, 0.36, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.72, 0.24, 0.0, 0.0, 0.0, 0.04],\n",
       " [0.24, 0.0, 0.28, 0.0, 0.08, 0.0, 0.36, 0.04, 0.0],\n",
       " [0.04, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.2, 0.0, 0.0, 0.16, 0.16, 0.48, 0.0, 0.0, 0.0],\n",
       " [0.6, 0.0, 0.04, 0.0, 0.04, 0.24, 0.08, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.64, 0.36, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.12, 0.32, 0.2, 0.0, 0.04, 0.12, 0.2, 0.0, 0.0],\n",
       " [0.08, 0.2, 0.36, 0.0, 0.2, 0.0, 0.12, 0.0, 0.04],\n",
       " [0.04, 0.0, 0.0, 0.68, 0.2, 0.04, 0.04, 0.0, 0.0],\n",
       " [0.64, 0.0, 0.08, 0.0, 0.0, 0.2, 0.08, 0.0, 0.0],\n",
       " [0.08, 0.08, 0.4, 0.0, 0.12, 0.0, 0.28, 0.0, 0.04],\n",
       " [0.04, 0.0, 0.0, 0.72, 0.24, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\n",
    "l1_result.index += 1\n",
    "l1_result.index.name = '#cnum'\n",
    "l1_result.columns = unique_symbols\n",
    "print(l1_result)\n",
    "chip_norm = []\n",
    "#pull the percentage for each cnum\n",
    "for x in chip_num:\n",
    "#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n",
    "    chip_norm.append(l1_result.loc[x,:].values.tolist())\n",
    "#display(chip_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Network Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_num = range(1,25)\n",
    "layer_num = range(1,4)\n",
    "\n",
    "\n",
    "shape_collection = []\n",
    "for node in node_num:\n",
    "    if node < 3:\n",
    "        shape_collection.append([node])\n",
    "\n",
    "def trickle(arr, iteration_left, check):\n",
    "    if iteration_left == 0:\n",
    "        global shape_collection\n",
    "        #running the int fxn to make sure we don't have floats\n",
    "        mp = map(int, arr)\n",
    "        x = list(mp)\n",
    "        if check == sum(x):\n",
    "            shape_collection.append(x)\n",
    "    else:\n",
    "        new_arr = [0]+ arr + [0]\n",
    "        #recursively expanding the list symmetrically\n",
    "        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n",
    "            new_arr[0] += 1\n",
    "            new_arr[1] -= 1\n",
    "            new_arr[-1] += 1\n",
    "            new_arr[-2] -= 1\n",
    "        trickle(new_arr, iteration_left - 1, check)\n",
    "\n",
    "for node in node_num:\n",
    "    for layer in layer_num:\n",
    "        if node//layer < 3:\n",
    "            continue\n",
    "        if layer%2 == 0:\n",
    "            trickle([node/2, node/2], (layer-2)/2, node)\n",
    "        else:\n",
    "            trickle([node], (layer-1)/2, node)      \n",
    "\n",
    "print(shape_collection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Number of training iterations\n",
    "num_iters = 1000\n",
    "\n",
    "#Listing out the shapes of each model\n",
    "colors_num = len(chip_norm[0])\n",
    "input_size = 3\n",
    "\n",
    "network_shapes = []\n",
    "for s in shape_collection:\n",
    "    network_shapes.append((input_size,s,colors_num))\n",
    "\n",
    "#Learning rate of the network\n",
    "rate = 0.001\n",
    "\n",
    "#Generating Training Data\n",
    "def shuffle(lab_norm, chip_norm):\n",
    "    '''\n",
    "    Applying train-test split\n",
    "    '''\n",
    "    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, shuffle = True)\n",
    "    input_train = torch.FloatTensor(lab_train)\n",
    "    output_train = torch.FloatTensor(chip_train)\n",
    "    input_test= torch.FloatTensor(lab_test)\n",
    "    output_test = torch.FloatTensor(chip_test)\n",
    "    return input_train, output_train, input_test, output_test\n",
    "\n",
    "print(network_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Array of losses over training period for each network\n",
    "num_average = 10\n",
    "output_file = {}\n",
    "for n in node_num:\n",
    "    output_file[n] = {}\n",
    "    \n",
    "\n",
    "for net_num, shape in enumerate(network_shapes):\n",
    "    print(\"Training: \",shape)\n",
    "    net_error_arr = []\n",
    "    for j in range(num_average):\n",
    "        print('Run ' + str(j+1))\n",
    "        NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n",
    "                            hiddenSize = shape[1] , learning_rate = rate)\n",
    "        error_arr = []\n",
    "        prev_error = 0\n",
    "        strike = 0\n",
    "\n",
    "        input_train, output_train, input_test, output_test = shuffle(lab_norm, chip_norm)\n",
    "\n",
    "        for i in range(num_iters):  \n",
    "            NN.train(input_train, output_train)\n",
    "            validation_error = NN.l1error(output_test, NN(input_test))\n",
    "            #Printing error\n",
    "            if i == 0: \n",
    "                dh = display(\"#\" + str(i) + \" Validation Error: \" + str(validation_error), display_id=True)\n",
    "            else:\n",
    "                dh.update(\"#\" + str(i) + \" Validation Error: \" + str(validation_error))\n",
    "            \n",
    "            #zero small error change\n",
    "            if i == 0:\n",
    "                strike = 0\n",
    "            #adding error to array\n",
    "            error_arr.append(validation_error)\n",
    "            #waiting for number 'too small' decreases or increases in validation error before ending training\n",
    "            if (prev_error < validation_error) and i > 100:\n",
    "                if strike > 5:\n",
    "                    print(\"Complete at iteration \", i, \"\\nFinal error: \", np.min(error_arr), \"\\n\")\n",
    "                    break\n",
    "                else:\n",
    "                    strike += 1\n",
    "            prev_error = validation_error\n",
    "        net_error_arr.append(np.min(error_arr))\n",
    "    output_file[sum(shape[1])][len(shape[1])] = [np.mean(net_error_arr), np.std(net_error_arr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('validation_errors.json', 'w') as f:\n",
    "    json.dump(output_file, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at minimum size of networks for each threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for size in node_num:\n",
    "    out_dict_for_size = output_file[sum(shape[1])]\n",
    "    vals = list(out_dict_for_size.values())\n",
    "    vals = np.array(vals)\n",
    "    vals = vals[:,0]\n",
    "    errors.append(np.min(vals))\n",
    "errors = np.array(errors)\n",
    "\n",
    "thresholds = np.arange(.001, 1, .001)\n",
    "\n",
    "min_sizes = []\n",
    "for threshold in thresholds:\n",
    "    idx = 0\n",
    "    for err in errors:\n",
    "        if err <= threshold:\n",
    "            break\n",
    "        idx += 1\n",
    "    min_sizes.append(size[idx])\n",
    "plt.plot(thresholds, min_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating Complexity in bits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_complexity(lnum):\n",
    "    language_data = term_data[term_data.get('#Lnum').eq(lnum)]\n",
    "    unique_terms = list(language_data['Term Abbrev'].unique())\n",
    "    l1_grouped = language_data.groupby('#cnum')['Term Abbrev'].apply(list)\n",
    "    display(l1_grouped)\n",
    "    l1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) \\\n",
    "                              for abbrev in unique_terms] for i in range(len(l1_grouped))]\n",
    "    l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\n",
    "    l1_result.index += 1\n",
    "    l1_result.index.name = '#cnum'\n",
    "    l1_result.columns = unique_terms\n",
    "\n",
    "    chip_norm = []\n",
    "\n",
    "    for x in chip_num:\n",
    "        chip_norm.append(l1_result.loc[x,:].values.tolist())\n",
    "\n",
    "    terms = unique_terms\n",
    "    chips = list(language_data['#cnum'].unique())\n",
    "\n",
    "    complexity = 0\n",
    "    prior_m = 1 / len(chips)\n",
    "    for w in terms:\n",
    "        word_prob = 0\n",
    "        for m in chips:\n",
    "            word_prob += prior_m * l1_result.at[m, w]\n",
    "        for m in chips:\n",
    "            encoder_prob = l1_result.at[m, w]\n",
    "            if encoder_prob != 0:\n",
    "                mutual_information = prior_m * encoder_prob * np.log2(encoder_prob / word_prob)\n",
    "            complexity += mutual_information\n",
    "\n",
    "    return complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#cnum\n",
       "1      [G, G, G, G, G, G, G, G, BL, P, G, BL, G, BL, ...\n",
       "2      [Y, G, G, G, G, G, G, G, G, G, G, G, G, G, G, ...\n",
       "3      [R, R, Y, Y, Y, R, R, R, Y, C, R, R, R, Y, R, ...\n",
       "4      [BL, P, P, P, P, BL, P, G, P, P, P, P, BL, P, ...\n",
       "5      [G, G, G, G, G, G, G, P, G, G, G, G, G, G, G, ...\n",
       "                             ...                        \n",
       "326    [Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, ...\n",
       "327    [W, W, W, BL, W, W, W, G, W, W, G, W, W, G, G,...\n",
       "328    [G, G, B, G, P, G, G, G, G, G, G, BL, G, G, G,...\n",
       "329    [C, C, R, Y, C, P, Y, W, C, C, Y, C, C, B, C, ...\n",
       "330    [W, W, W, G, W, W, W, W, W, G, G, W, W, G, W, ...\n",
       "Name: Term Abbrev, Length: 330, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.451688472037161\n"
     ]
    }
   ],
   "source": [
    "result = language_complexity(110)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "history": [
   {
    "code": "import sys\nimport math\nimport numpy as np\n\"\"\"\nnot working\nfrom Language_Data_Scraper.py import *\n\"\"\"\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd",
    "id": "98b63ce99ffa4f428115004fb0366917",
    "idx": 0,
    "time": "2021-02-03T02:17:02.768Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "20b0acb43c014af6866c6ee50356a0b2",
    "idx": 1,
    "time": "2021-02-03T02:17:03.459Z",
    "type": "execution"
   },
   {
    "id": "98b63ce99ffa4f428115004fb0366917",
    "time": "2021-02-03T02:17:04.551Z",
    "type": "completion"
   },
   {
    "id": "20b0acb43c014af6866c6ee50356a0b2",
    "time": "2021-02-03T02:17:04.866Z",
    "type": "completion"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "6e3071fb1dde4215810e2a80d8e471a4",
    "idx": 2,
    "time": "2021-02-03T02:17:08.061Z",
    "type": "execution"
   },
   {
    "id": "6e3071fb1dde4215810e2a80d8e471a4",
    "time": "2021-02-03T02:17:08.139Z",
    "type": "completion"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "467e3a66759c4d08a81372178f7abd45",
    "idx": 3,
    "time": "2021-02-03T02:17:11.506Z",
    "type": "execution"
   },
   {
    "id": "467e3a66759c4d08a81372178f7abd45",
    "time": "2021-02-03T02:17:11.558Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T02:17:11.748Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T02:17:11.840Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T02:17:11.958Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:17:12.253Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 5000\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [100], colors_num), (input_size, [100], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "8cc05ec00119411f8f0b62b4fa50265b",
    "idx": 6,
    "time": "2021-02-03T02:17:12.570Z",
    "type": "execution"
   },
   {
    "id": "8cc05ec00119411f8f0b62b4fa50265b",
    "time": "2021-02-03T02:17:12.619Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_train, NN(input_train))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "9277c562ac6843d08f0c0336d9745513",
    "idx": 7,
    "time": "2021-02-03T02:17:13.098Z",
    "type": "execution"
   },
   {
    "id": "9277c562ac6843d08f0c0336d9745513",
    "time": "2021-02-03T02:17:16.262Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T02:24:19.024Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:24:19.357Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(2)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T02:24:49.472Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T02:24:49.625Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T02:24:50.379Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:24:50.485Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    \nchip_norm",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T02:25:21.820Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:25:21.934Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T02:25:26.177Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T02:25:26.258Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    \nchip_norm",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T02:25:26.373Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:25:26.772Z",
    "type": "completion"
   },
   {
    "code": "chip_num",
    "id": "cf646372e8c149f695e18186263057f5",
    "idx": 4,
    "time": "2021-02-03T02:26:11.410Z",
    "type": "execution"
   },
   {
    "id": "cf646372e8c149f695e18186263057f5",
    "time": "2021-02-03T02:26:11.478Z",
    "type": "completion"
   },
   {
    "code": "lab_norm",
    "id": "cf646372e8c149f695e18186263057f5",
    "idx": 4,
    "time": "2021-02-03T02:26:22.729Z",
    "type": "execution"
   },
   {
    "id": "cf646372e8c149f695e18186263057f5",
    "time": "2021-02-03T02:26:22.830Z",
    "type": "completion"
   },
   {
    "code": "locations",
    "id": "cf646372e8c149f695e18186263057f5",
    "idx": 4,
    "time": "2021-02-03T02:26:29.288Z",
    "type": "execution"
   },
   {
    "id": "cf646372e8c149f695e18186263057f5",
    "time": "2021-02-03T02:26:29.375Z",
    "type": "completion"
   },
   {
    "code": "l1",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:28:27.321Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:28:27.377Z",
    "type": "completion"
   },
   {
    "code": "l1_grouped",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:29:19.390Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:29:19.445Z",
    "type": "completion"
   },
   {
    "code": "l1_grouped[0]",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:29:25.010Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:29:25.098Z",
    "type": "completion"
   },
   {
    "code": "l1_grouped",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:29:34.880Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:29:34.932Z",
    "type": "completion"
   },
   {
    "code": "l1_grouped[1]",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:29:45.667Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:29:45.710Z",
    "type": "completion"
   },
   {
    "code": "len(l1_grouped[1]",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:29:55.651Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:29:55.695Z",
    "type": "completion"
   },
   {
    "code": "len(l1_grouped[1])",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:29:58.486Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:29:58.586Z",
    "type": "completion"
   },
   {
    "code": "len(l1_grouped[2])",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:30:08.947Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:30:08.994Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(2)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T02:30:25.427Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T02:30:25.558Z",
    "type": "completion"
   },
   {
    "code": "(l1_grouped[2])",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:30:29.331Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:30:29.375Z",
    "type": "completion"
   },
   {
    "code": "(l1_grouped)",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:30:35.569Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:30:35.630Z",
    "type": "completion"
   },
   {
    "code": "(l1_grouped)",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:31:07.091Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:31:07.141Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    \nchip_norm",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:31:57.834Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:31:57.989Z",
    "type": "completion"
   },
   {
    "code": "unique_symbols",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:32:18.603Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:32:18.650Z",
    "type": "completion"
   },
   {
    "code": "len(unique_symbols)",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:32:31.570Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:32:31.661Z",
    "type": "completion"
   },
   {
    "code": "l1_grouped",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:32:44.502Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:32:44.556Z",
    "type": "completion"
   },
   {
    "code": "l1_grouped\nl1_chip_abbrev_percentage",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:33:02.372Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:33:03.174Z",
    "type": "completion"
   },
   {
    "code": "l1_result",
    "id": "c5f34e313dc24add925362a88193da74",
    "idx": 5,
    "time": "2021-02-03T02:34:25.660Z",
    "type": "execution"
   },
   {
    "id": "c5f34e313dc24add925362a88193da74",
    "time": "2021-02-03T02:34:25.764Z",
    "type": "completion"
   },
   {
    "code": "l1_result.loc[l1[\"#cnum\"]==1]",
    "id": "c5f34e313dc24add925362a88193da74",
    "idx": 5,
    "time": "2021-02-03T02:34:41.836Z",
    "type": "execution"
   },
   {
    "id": "c5f34e313dc24add925362a88193da74",
    "time": "2021-02-03T02:34:41.934Z",
    "type": "completion"
   },
   {
    "code": "l1_result.loc[1:]",
    "id": "c5f34e313dc24add925362a88193da74",
    "idx": 5,
    "time": "2021-02-03T02:36:01.388Z",
    "type": "execution"
   },
   {
    "id": "c5f34e313dc24add925362a88193da74",
    "time": "2021-02-03T02:36:01.493Z",
    "type": "completion"
   },
   {
    "code": "l1_result.loc[1,:]",
    "id": "c5f34e313dc24add925362a88193da74",
    "idx": 5,
    "time": "2021-02-03T02:36:16.371Z",
    "type": "execution"
   },
   {
    "id": "c5f34e313dc24add925362a88193da74",
    "time": "2021-02-03T02:36:16.426Z",
    "type": "completion"
   },
   {
    "code": "l1_result.loc[1,:].values.tolist()",
    "id": "c5f34e313dc24add925362a88193da74",
    "idx": 5,
    "time": "2021-02-03T02:37:22.747Z",
    "type": "execution"
   },
   {
    "id": "c5f34e313dc24add925362a88193da74",
    "time": "2021-02-03T02:37:22.801Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T02:40:06.885Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T02:40:06.970Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    \nchip_norm",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:40:09.847Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:40:10.221Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[1,:].values.tolist())\n    \nchip_norm",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:40:41.672Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:40:41.846Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())\n    \nchip_norm",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:40:47.442Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:40:47.600Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(2)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T02:41:07.062Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T02:41:07.197Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())\n    \nchip_norm",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:41:09.666Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:41:10.109Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())\n    \nchip_norm.shape()",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:41:27.119Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:41:27.254Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())\n    \nchip_norm.shape",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:41:39.706Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:41:39.828Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())\n    \nlen(chip_norm)\nchip_norm[0]",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:41:54.333Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:41:54.583Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())\n    \nprint(len(chip_norm))\nchip_norm[0]",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:42:07.176Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:42:07.339Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())\n    \nprint(len(chip_norm))\nprint(len(chip_norm[0]))",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:42:30.050Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:42:30.147Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:42:57.806Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:42:57.896Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 5000\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [100], colors_num), (input_size, [100], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "8cc05ec00119411f8f0b62b4fa50265b",
    "idx": 7,
    "time": "2021-02-03T02:42:58.790Z",
    "type": "execution"
   },
   {
    "id": "8cc05ec00119411f8f0b62b4fa50265b",
    "time": "2021-02-03T02:42:58.834Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_train, NN(input_train))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "9277c562ac6843d08f0c0336d9745513",
    "idx": 8,
    "time": "2021-02-03T02:44:08.571Z",
    "type": "execution"
   },
   {
    "id": "9277c562ac6843d08f0c0336d9745513",
    "time": "2021-02-03T02:45:02.125Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "21a85bde696a47ee98c20f205cad5cd4",
    "idx": 9,
    "time": "2021-02-03T02:45:57.807Z",
    "type": "execution"
   },
   {
    "id": "21a85bde696a47ee98c20f205cad5cd4",
    "time": "2021-02-03T02:45:57.924Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T02:53:48.766Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T02:53:48.849Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T02:53:48.976Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:53:49.086Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 5000\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [100], colors_num), (input_size, [100], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "8cc05ec00119411f8f0b62b4fa50265b",
    "idx": 6,
    "time": "2021-02-03T02:53:49.316Z",
    "type": "execution"
   },
   {
    "id": "8cc05ec00119411f8f0b62b4fa50265b",
    "time": "2021-02-03T02:53:49.365Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_train, NN(input_train))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "9277c562ac6843d08f0c0336d9745513",
    "idx": 7,
    "time": "2021-02-03T02:53:50.090Z",
    "type": "execution"
   },
   {
    "id": "9277c562ac6843d08f0c0336d9745513",
    "time": "2021-02-03T02:54:46.055Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "21a85bde696a47ee98c20f205cad5cd4",
    "idx": 8,
    "time": "2021-02-03T02:54:48.422Z",
    "type": "execution"
   },
   {
    "id": "21a85bde696a47ee98c20f205cad5cd4",
    "time": "2021-02-03T02:54:48.523Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "21a85bde696a47ee98c20f205cad5cd4",
    "idx": 8,
    "time": "2021-02-03T02:54:54.136Z",
    "type": "execution"
   },
   {
    "id": "21a85bde696a47ee98c20f205cad5cd4",
    "time": "2021-02-03T02:54:54.236Z",
    "type": "completion"
   },
   {
    "code": "locations",
    "id": "fd38caded7c2472d8d7f45c3be57ae56",
    "idx": 4,
    "time": "2021-02-03T04:04:36.310Z",
    "type": "execution"
   },
   {
    "id": "fd38caded7c2472d8d7f45c3be57ae56",
    "time": "2021-02-03T04:04:46.687Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\n\"\"\"\nnot working\nfrom Language_Data_Scraper.py import *\n\"\"\"\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd",
    "id": "98b63ce99ffa4f428115004fb0366917",
    "idx": 0,
    "time": "2021-02-03T04:04:53.275Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "20b0acb43c014af6866c6ee50356a0b2",
    "idx": 1,
    "time": "2021-02-03T04:04:53.503Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "6e3071fb1dde4215810e2a80d8e471a4",
    "idx": 2,
    "time": "2021-02-03T04:04:53.670Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "467e3a66759c4d08a81372178f7abd45",
    "idx": 3,
    "time": "2021-02-03T04:04:53.858Z",
    "type": "execution"
   },
   {
    "id": "98b63ce99ffa4f428115004fb0366917",
    "time": "2021-02-03T04:04:54.984Z",
    "type": "completion"
   },
   {
    "code": "locations",
    "id": "fd38caded7c2472d8d7f45c3be57ae56",
    "idx": 4,
    "time": "2021-02-03T04:04:55.008Z",
    "type": "execution"
   },
   {
    "id": "20b0acb43c014af6866c6ee50356a0b2",
    "time": "2021-02-03T04:04:55.263Z",
    "type": "completion"
   },
   {
    "id": "6e3071fb1dde4215810e2a80d8e471a4",
    "time": "2021-02-03T04:04:55.293Z",
    "type": "completion"
   },
   {
    "id": "467e3a66759c4d08a81372178f7abd45",
    "time": "2021-02-03T04:04:55.297Z",
    "type": "completion"
   },
   {
    "id": "fd38caded7c2472d8d7f45c3be57ae56",
    "time": "2021-02-03T04:04:55.316Z",
    "type": "completion"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "467e3a66759c4d08a81372178f7abd45",
    "idx": 3,
    "time": "2021-02-03T04:06:06.303Z",
    "type": "execution"
   },
   {
    "id": "467e3a66759c4d08a81372178f7abd45",
    "time": "2021-02-03T04:06:06.357Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T04:11:42.418Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T04:11:42.514Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T04:11:42.981Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T04:11:43.076Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(2)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T04:11:46.972Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T04:11:47.117Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T04:11:47.514Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T04:11:47.630Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(3)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T04:12:27.648Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T04:12:27.955Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T04:12:29.357Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T04:12:29.517Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T04:12:38.986Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T04:12:39.154Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T04:12:40.407Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T04:12:40.495Z",
    "type": "completion"
   },
   {
    "code": "term_data['#cnum']",
    "id": "466c00fff29a428ca60f9db17cd69273",
    "idx": 2,
    "time": "2021-02-03T04:38:47.778Z",
    "type": "execution"
   },
   {
    "id": "466c00fff29a428ca60f9db17cd69273",
    "time": "2021-02-03T04:38:47.976Z",
    "type": "completion"
   },
   {
    "code": "term_data['#cnum'].unique",
    "id": "466c00fff29a428ca60f9db17cd69273",
    "idx": 2,
    "time": "2021-02-03T04:38:54.487Z",
    "type": "execution"
   },
   {
    "id": "466c00fff29a428ca60f9db17cd69273",
    "time": "2021-02-03T04:38:54.529Z",
    "type": "completion"
   },
   {
    "code": "term_data['#cnum'].unique()",
    "id": "466c00fff29a428ca60f9db17cd69273",
    "idx": 2,
    "time": "2021-02-03T04:38:59.539Z",
    "type": "execution"
   },
   {
    "id": "466c00fff29a428ca60f9db17cd69273",
    "time": "2021-02-03T04:38:59.592Z",
    "type": "completion"
   },
   {
    "code": "term_data['#Lnum'].unique()",
    "id": "466c00fff29a428ca60f9db17cd69273",
    "idx": 2,
    "time": "2021-02-03T04:39:07.612Z",
    "type": "execution"
   },
   {
    "id": "466c00fff29a428ca60f9db17cd69273",
    "time": "2021-02-03T04:39:07.672Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\n\"\"\"\nnot working\nfrom Language_Data_Scraper.py import *\n\"\"\"\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd",
    "id": "0e0ac933829d471595492e4acc88ea34",
    "idx": 0,
    "time": "2021-02-06T18:06:22.906Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "4903bda737bf4f61a75993cf916b5233",
    "idx": 1,
    "time": "2021-02-06T18:06:23.110Z",
    "type": "execution"
   },
   {
    "code": "term_data['#Lnum'].unique()",
    "id": "4907e05ff41d411c8ef84c6c8eed8834",
    "idx": 2,
    "time": "2021-02-06T18:06:23.325Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "d9226fd6339f4307870bf39af7b51873",
    "idx": 3,
    "time": "2021-02-06T18:06:23.570Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "964bc2f9ed444796ade9ad151d64c2d2",
    "idx": 4,
    "time": "2021-02-06T18:06:23.889Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "idx": 5,
    "time": "2021-02-06T18:06:24.430Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "idx": 6,
    "time": "2021-02-06T18:06:25.289Z",
    "type": "execution"
   },
   {
    "id": "0e0ac933829d471595492e4acc88ea34",
    "time": "2021-02-06T18:06:25.313Z",
    "type": "completion"
   },
   {
    "id": "4903bda737bf4f61a75993cf916b5233",
    "time": "2021-02-06T18:06:25.820Z",
    "type": "completion"
   },
   {
    "id": "4907e05ff41d411c8ef84c6c8eed8834",
    "time": "2021-02-06T18:06:25.853Z",
    "type": "completion"
   },
   {
    "id": "d9226fd6339f4307870bf39af7b51873",
    "time": "2021-02-06T18:06:25.889Z",
    "type": "completion"
   },
   {
    "id": "964bc2f9ed444796ade9ad151d64c2d2",
    "time": "2021-02-06T18:06:25.908Z",
    "type": "completion"
   },
   {
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "time": "2021-02-06T18:06:25.969Z",
    "type": "completion"
   },
   {
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "time": "2021-02-06T18:06:26.066Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 5000\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [100], colors_num), (input_size, [100], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "44941f4587c5419d9f229a7472ac6520",
    "idx": 7,
    "time": "2021-02-06T18:06:27.543Z",
    "type": "execution"
   },
   {
    "id": "44941f4587c5419d9f229a7472ac6520",
    "time": "2021-02-06T18:06:27.594Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "idx": 5,
    "time": "2021-02-06T18:11:49.708Z",
    "type": "execution"
   },
   {
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "time": "2021-02-06T18:11:49.815Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "idx": 6,
    "time": "2021-02-06T18:11:49.997Z",
    "type": "execution"
   },
   {
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "time": "2021-02-06T18:11:50.122Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 5000\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [100], colors_num), (input_size, [100], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "44941f4587c5419d9f229a7472ac6520",
    "idx": 7,
    "time": "2021-02-06T18:11:54.897Z",
    "type": "execution"
   },
   {
    "id": "44941f4587c5419d9f229a7472ac6520",
    "time": "2021-02-06T18:11:54.948Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_train, NN(input_train))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "idx": 8,
    "time": "2021-02-06T18:12:17.336Z",
    "type": "execution"
   },
   {
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "time": "2021-02-06T18:13:38.420Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "idx": 9,
    "time": "2021-02-06T18:14:33.150Z",
    "type": "execution"
   },
   {
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "time": "2021-02-06T18:14:33.388Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_train, NN(input_train))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "idx": 8,
    "time": "2021-02-06T18:15:52.597Z",
    "type": "execution"
   },
   {
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "time": "2021-02-06T18:17:12.579Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "idx": 9,
    "time": "2021-02-06T18:17:15.471Z",
    "type": "execution"
   },
   {
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "time": "2021-02-06T18:17:15.790Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\n\"\"\"\nnot working\nfrom Language_Data_Scraper.py import *\n\"\"\"\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd",
    "id": "0e0ac933829d471595492e4acc88ea34",
    "idx": 0,
    "time": "2021-02-06T18:21:50.043Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "4903bda737bf4f61a75993cf916b5233",
    "idx": 1,
    "time": "2021-02-06T18:21:50.046Z",
    "type": "execution"
   },
   {
    "code": "term_data['#Lnum'].unique()",
    "id": "4907e05ff41d411c8ef84c6c8eed8834",
    "idx": 2,
    "time": "2021-02-06T18:21:50.047Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "d9226fd6339f4307870bf39af7b51873",
    "idx": 3,
    "time": "2021-02-06T18:21:50.048Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "964bc2f9ed444796ade9ad151d64c2d2",
    "idx": 4,
    "time": "2021-02-06T18:21:50.049Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "idx": 5,
    "time": "2021-02-06T18:21:50.050Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "idx": 6,
    "time": "2021-02-06T18:21:50.051Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 5000\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [100], colors_num), (input_size, [100], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "44941f4587c5419d9f229a7472ac6520",
    "idx": 7,
    "time": "2021-02-06T18:21:50.052Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_train, NN(input_train))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "idx": 8,
    "time": "2021-02-06T18:21:50.053Z",
    "type": "execution"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "idx": 9,
    "time": "2021-02-06T18:21:50.054Z",
    "type": "execution"
   },
   {
    "id": "0e0ac933829d471595492e4acc88ea34",
    "time": "2021-02-06T18:21:52.356Z",
    "type": "completion"
   },
   {
    "id": "4903bda737bf4f61a75993cf916b5233",
    "time": "2021-02-06T18:21:52.798Z",
    "type": "completion"
   },
   {
    "id": "4907e05ff41d411c8ef84c6c8eed8834",
    "time": "2021-02-06T18:21:52.820Z",
    "type": "completion"
   },
   {
    "id": "d9226fd6339f4307870bf39af7b51873",
    "time": "2021-02-06T18:21:52.867Z",
    "type": "completion"
   },
   {
    "id": "964bc2f9ed444796ade9ad151d64c2d2",
    "time": "2021-02-06T18:21:52.876Z",
    "type": "completion"
   },
   {
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "time": "2021-02-06T18:21:52.949Z",
    "type": "completion"
   },
   {
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "time": "2021-02-06T18:21:53.049Z",
    "type": "completion"
   },
   {
    "id": "44941f4587c5419d9f229a7472ac6520",
    "time": "2021-02-06T18:21:53.056Z",
    "type": "completion"
   },
   {
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "time": "2021-02-06T18:21:53.266Z",
    "type": "completion"
   },
   {
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "time": "2021-02-06T18:21:53.304Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\n\"\"\"\nnot working\nfrom Language_Data_Scraper.py import *\n\"\"\"\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd",
    "id": "0e0ac933829d471595492e4acc88ea34",
    "idx": 0,
    "time": "2021-02-06T18:22:16.243Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "4903bda737bf4f61a75993cf916b5233",
    "idx": 1,
    "time": "2021-02-06T18:22:16.244Z",
    "type": "execution"
   },
   {
    "code": "term_data['#Lnum'].unique()",
    "id": "4907e05ff41d411c8ef84c6c8eed8834",
    "idx": 2,
    "time": "2021-02-06T18:22:16.245Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "d9226fd6339f4307870bf39af7b51873",
    "idx": 3,
    "time": "2021-02-06T18:22:16.246Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "964bc2f9ed444796ade9ad151d64c2d2",
    "idx": 4,
    "time": "2021-02-06T18:22:16.248Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "idx": 5,
    "time": "2021-02-06T18:22:16.248Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "idx": 6,
    "time": "2021-02-06T18:22:16.249Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 5000\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [100], colors_num), (input_size, [100], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "44941f4587c5419d9f229a7472ac6520",
    "idx": 7,
    "time": "2021-02-06T18:22:16.250Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_train, NN(input_train))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "idx": 8,
    "time": "2021-02-06T18:22:16.251Z",
    "type": "execution"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "idx": 9,
    "time": "2021-02-06T18:22:16.252Z",
    "type": "execution"
   },
   {
    "id": "0e0ac933829d471595492e4acc88ea34",
    "time": "2021-02-06T18:22:18.731Z",
    "type": "completion"
   },
   {
    "id": "4903bda737bf4f61a75993cf916b5233",
    "time": "2021-02-06T18:22:19.173Z",
    "type": "completion"
   },
   {
    "id": "4907e05ff41d411c8ef84c6c8eed8834",
    "time": "2021-02-06T18:22:19.194Z",
    "type": "completion"
   },
   {
    "id": "d9226fd6339f4307870bf39af7b51873",
    "time": "2021-02-06T18:22:19.267Z",
    "type": "completion"
   },
   {
    "id": "964bc2f9ed444796ade9ad151d64c2d2",
    "time": "2021-02-06T18:22:19.279Z",
    "type": "completion"
   },
   {
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "time": "2021-02-06T18:22:19.422Z",
    "type": "completion"
   },
   {
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "time": "2021-02-06T18:22:19.500Z",
    "type": "completion"
   },
   {
    "id": "44941f4587c5419d9f229a7472ac6520",
    "time": "2021-02-06T18:22:19.518Z",
    "type": "completion"
   },
   {
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "time": "2021-02-06T18:23:01.889Z",
    "type": "completion"
   },
   {
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "time": "2021-02-06T18:23:01.921Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 50\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [100], colors_num), (input_size, [100], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "44941f4587c5419d9f229a7472ac6520",
    "idx": 7,
    "time": "2021-02-06T18:23:21.229Z",
    "type": "execution"
   },
   {
    "id": "44941f4587c5419d9f229a7472ac6520",
    "time": "2021-02-06T18:23:21.285Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_train, NN(input_train))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "idx": 8,
    "time": "2021-02-06T18:23:21.456Z",
    "type": "execution"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "idx": 9,
    "time": "2021-02-06T18:23:21.747Z",
    "type": "execution"
   },
   {
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "time": "2021-02-06T18:24:53.493Z",
    "type": "completion"
   },
   {
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "time": "2021-02-06T18:24:53.653Z",
    "type": "completion"
   },
   {
    "code": "chip_norm",
    "id": "42db734a766c45fba8b74a2a838c014d",
    "idx": 7,
    "time": "2021-02-06T18:26:53.230Z",
    "type": "execution"
   },
   {
    "id": "42db734a766c45fba8b74a2a838c014d",
    "time": "2021-02-06T18:26:53.399Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(2)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "idx": 5,
    "time": "2021-02-06T18:27:09.279Z",
    "type": "execution"
   },
   {
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "time": "2021-02-06T18:27:09.459Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "idx": 6,
    "time": "2021-02-06T18:27:09.735Z",
    "type": "execution"
   },
   {
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "time": "2021-02-06T18:27:09.879Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "idx": 5,
    "time": "2021-02-06T18:27:32.992Z",
    "type": "execution"
   },
   {
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "time": "2021-02-06T18:27:33.107Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "idx": 6,
    "time": "2021-02-06T18:27:33.440Z",
    "type": "execution"
   },
   {
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "time": "2021-02-06T18:27:33.576Z",
    "type": "completion"
   },
   {
    "code": "l1_result",
    "id": "42db734a766c45fba8b74a2a838c014d",
    "idx": 7,
    "time": "2021-02-06T18:27:44.399Z",
    "type": "execution"
   },
   {
    "id": "42db734a766c45fba8b74a2a838c014d",
    "time": "2021-02-06T18:27:44.499Z",
    "type": "completion"
   },
   {
    "code": "l1_result.GB",
    "id": "42db734a766c45fba8b74a2a838c014d",
    "idx": 7,
    "time": "2021-02-06T18:27:53.103Z",
    "type": "execution"
   },
   {
    "id": "42db734a766c45fba8b74a2a838c014d",
    "time": "2021-02-06T18:27:53.165Z",
    "type": "completion"
   },
   {
    "code": "l1_result.GB.unique()",
    "id": "42db734a766c45fba8b74a2a838c014d",
    "idx": 7,
    "time": "2021-02-06T18:27:57.411Z",
    "type": "execution"
   },
   {
    "id": "42db734a766c45fba8b74a2a838c014d",
    "time": "2021-02-06T18:27:57.481Z",
    "type": "completion"
   },
   {
    "code": "cnum_data[['#cnum']]",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:30:39.517Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:30:39.585Z",
    "type": "completion"
   },
   {
    "code": "cnum_data",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:30:49.443Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:30:49.514Z",
    "type": "completion"
   },
   {
    "code": "cnum_data.loc[:, ['L*', 'a*', 'b*']]",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:31:42.664Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:31:42.747Z",
    "type": "completion"
   },
   {
    "code": "cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:31:57.416Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:31:57.478Z",
    "type": "completion"
   },
   {
    "code": "debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\n# debug1['L*'] = debug1['L*'] - \nnormalized_df=(debug1-debug1.mean())/debug1.std()",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:34:07.368Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:34:07.427Z",
    "type": "completion"
   },
   {
    "code": "debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\n# debug1['L*'] = debug1['L*'] - \nnormalized_df=(debug1-debug1.mean())/debug1.std()\nnormalized_df",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:34:16.430Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:34:16.505Z",
    "type": "completion"
   },
   {
    "code": "debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\ndebug1['L*'] = (debug1['L*'] - debug1['L*'].mean) / debug1['L*'].std()\n",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:35:43.131Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:35:43.245Z",
    "type": "completion"
   },
   {
    "code": "debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\ndebug1['L*'] = (debug1['L*'] - debug1['L*'].mean) / debug1['L*'].std\n",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:36:00.211Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:36:00.325Z",
    "type": "completion"
   },
   {
    "code": "debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\ndebug1['L*'] = (debug1['L*'] - debug1['L*'].mean()) #/ debug1['L*'].std\n",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:36:06.335Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:36:06.387Z",
    "type": "completion"
   },
   {
    "code": "debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\ndebug1['L*'] = (debug1['L*'] - debug1['L*'].mean()) / debug1['L*'].std()\n",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:36:18.477Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:36:18.554Z",
    "type": "completion"
   },
   {
    "code": "debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\ndebug1['L*'] = (debug1['L*'] - debug1['L*'].mean()) / debug1['L*'].std()\ndebug1",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:36:24.120Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:36:24.192Z",
    "type": "completion"
   },
   {
    "code": "# debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\n# debug1['L*'] = (debug1['L*'] - debug1['L*'].mean()) / debug1['L*'].std()\n# debug1",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:39:11.010Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:39:11.057Z",
    "type": "completion"
   },
   {
    "code": "locattions",
    "id": "d8ce44b873434ba88decdb4f4709e46b",
    "idx": 6,
    "time": "2021-02-06T18:39:58.590Z",
    "type": "execution"
   },
   {
    "id": "d8ce44b873434ba88decdb4f4709e46b",
    "time": "2021-02-06T18:39:58.665Z",
    "type": "completion"
   },
   {
    "code": "locations",
    "id": "d8ce44b873434ba88decdb4f4709e46b",
    "idx": 6,
    "time": "2021-02-06T18:40:00.807Z",
    "type": "execution"
   },
   {
    "id": "d8ce44b873434ba88decdb4f4709e46b",
    "time": "2021-02-06T18:40:00.869Z",
    "type": "completion"
   },
   {
    "code": "chip_num",
    "id": "d8ce44b873434ba88decdb4f4709e46b",
    "idx": 6,
    "time": "2021-02-06T18:40:06.496Z",
    "type": "execution"
   },
   {
    "id": "d8ce44b873434ba88decdb4f4709e46b",
    "time": "2021-02-06T18:40:06.563Z",
    "type": "completion"
   },
   {
    "code": "len(chip_num)",
    "id": "d8ce44b873434ba88decdb4f4709e46b",
    "idx": 6,
    "time": "2021-02-06T18:40:21.037Z",
    "type": "execution"
   },
   {
    "code": "l1",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 7,
    "time": "2021-02-06T18:44:08.553Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:44:08.624Z",
    "type": "completion"
   },
   {
    "code": "NNtest = Neural_Network(inputSize = 3, outputSize = 1, hiddenSize = [3,3,3] , learning_rate = 0.001)",
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "idx": 6,
    "time": "2021-02-06T18:45:07.997Z",
    "type": "execution"
   },
   {
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "time": "2021-02-06T18:45:08.264Z",
    "type": "completion"
   },
   {
    "code": "lab_norm",
    "id": "4332d3385ba746e3b29bcac0b0c43061",
    "idx": 6,
    "time": "2021-02-06T18:45:36.177Z",
    "type": "execution"
   },
   {
    "id": "4332d3385ba746e3b29bcac0b0c43061",
    "time": "2021-02-06T18:45:36.310Z",
    "type": "completion"
   },
   {
    "code": "lab_norm.shape",
    "id": "4332d3385ba746e3b29bcac0b0c43061",
    "idx": 6,
    "time": "2021-02-06T18:45:40.341Z",
    "type": "execution"
   },
   {
    "id": "4332d3385ba746e3b29bcac0b0c43061",
    "time": "2021-02-06T18:45:40.396Z",
    "type": "completion"
   },
   {
    "code": "len(lab_norm)",
    "id": "4332d3385ba746e3b29bcac0b0c43061",
    "idx": 6,
    "time": "2021-02-06T18:45:51.499Z",
    "type": "execution"
   },
   {
    "id": "4332d3385ba746e3b29bcac0b0c43061",
    "time": "2021-02-06T18:45:51.627Z",
    "type": "completion"
   },
   {
    "code": "NNtest = Neural_Network(inputSize = 3, outputSize = 1, hiddenSize = [3,3,3] , learning_rate = 0.001)\nNNtest([[1, 1, 1]])\n",
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "idx": 6,
    "time": "2021-02-06T18:46:31.225Z",
    "type": "execution"
   },
   {
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "time": "2021-02-06T18:46:31.365Z",
    "type": "completion"
   },
   {
    "code": "NNtest = Neural_Network(inputSize = 3, outputSize = 1, hiddenSize = [3,3,3] , learning_rate = 0.001)\nNNtest(torch.FloatTensor([[1, 1, 1]]))\n",
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "idx": 6,
    "time": "2021-02-06T18:47:37.740Z",
    "type": "execution"
   },
   {
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "time": "2021-02-06T18:47:37.815Z",
    "type": "completion"
   },
   {
    "code": "NNtest = Neural_Network(inputSize = 3, outputSize = 1, hiddenSize = [3,3,3] , learning_rate = 0.001)\nNNtest(torch.FloatTensor([[1, 1, 1], [1, 1, 1]]))\n",
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "idx": 6,
    "time": "2021-02-06T18:48:01.822Z",
    "type": "execution"
   },
   {
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "time": "2021-02-06T18:48:01.883Z",
    "type": "completion"
   },
   {
    "code": "# #Neural Network Shape Test\n# NNtest = Neural_Network(inputSize = 3, outputSize = 1, hiddenSize = [3,3,3] , learning_rate = 0.001)\n# NNtest(torch.FloatTensor([[1, 1, 1], [1, 1, 1]]))\n",
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "idx": 6,
    "time": "2021-02-06T18:48:38.123Z",
    "type": "execution"
   },
   {
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "time": "2021-02-06T18:48:38.178Z",
    "type": "completion"
   },
   {
    "code": "unique_symbols",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 8,
    "time": "2021-02-06T18:49:22.578Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:49:22.633Z",
    "type": "completion"
   },
   {
    "code": "l1",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 8,
    "time": "2021-02-06T18:49:37.774Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:49:37.863Z",
    "type": "completion"
   },
   {
    "code": "l1['#Lnum']",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 8,
    "time": "2021-02-06T18:50:46.716Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:50:46.795Z",
    "type": "completion"
   },
   {
    "code": "l1['#Lnum'].unique()",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 8,
    "time": "2021-02-06T18:50:52.868Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:50:52.969Z",
    "type": "completion"
   },
   {
    "code": "unique_symbols",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 8,
    "time": "2021-02-06T18:50:58.566Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:50:58.976Z",
    "type": "completion"
   },
   {
    "code": "l1_grouped",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 8,
    "time": "2021-02-06T18:51:14.664Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:51:14.766Z",
    "type": "completion"
   },
   {
    "code": "l1_chip_abbrev_percentage",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 8,
    "time": "2021-02-06T18:52:17.986Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:52:18.155Z",
    "type": "completion"
   },
   {
    "code": "len(l1_chip_abbrev_percentage)",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 8,
    "time": "2021-02-06T18:52:31.119Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:52:31.188Z",
    "type": "completion"
   },
   {
    "code": "l1_result",
    "id": "1a980cbd8b2744578380cb27f9517068",
    "idx": 9,
    "time": "2021-02-06T18:52:53.003Z",
    "type": "execution"
   },
   {
    "id": "1a980cbd8b2744578380cb27f9517068",
    "time": "2021-02-06T18:52:53.092Z",
    "type": "completion"
   },
   {
    "code": "l1_result.shape",
    "id": "1a980cbd8b2744578380cb27f9517068",
    "idx": 9,
    "time": "2021-02-06T18:52:58.477Z",
    "type": "execution"
   },
   {
    "id": "1a980cbd8b2744578380cb27f9517068",
    "time": "2021-02-06T18:52:58.608Z",
    "type": "completion"
   },
   {
    "code": "chip_norm",
    "id": "1a980cbd8b2744578380cb27f9517068",
    "idx": 9,
    "time": "2021-02-06T18:53:26.083Z",
    "type": "execution"
   },
   {
    "id": "1a980cbd8b2744578380cb27f9517068",
    "time": "2021-02-06T18:53:26.329Z",
    "type": "completion"
   },
   {
    "code": "l1_chip_abbrev_percentage",
    "id": "6a4ca79e1c1f4970821c477347770df2",
    "idx": 9,
    "time": "2021-02-06T18:53:37.681Z",
    "type": "execution"
   },
   {
    "id": "6a4ca79e1c1f4970821c477347770df2",
    "time": "2021-02-06T18:53:37.886Z",
    "type": "completion"
   },
   {
    "code": "l1_chip_abbrev_percentage == chip_norm",
    "id": "6a4ca79e1c1f4970821c477347770df2",
    "idx": 9,
    "time": "2021-02-06T18:54:01.573Z",
    "type": "execution"
   },
   {
    "id": "6a4ca79e1c1f4970821c477347770df2",
    "time": "2021-02-06T18:54:01.634Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) \\\n                              for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "d87e693cad3e42ed8e6cc801b9ea65a0",
    "idx": 7,
    "time": "2021-02-06T18:55:04.151Z",
    "type": "execution"
   },
   {
    "id": "d87e693cad3e42ed8e6cc801b9ea65a0",
    "time": "2021-02-06T18:55:04.267Z",
    "type": "completion"
   },
   {
    "code": "range(1, 10)",
    "id": "166fd4bb85d84a50b6e68cd2a14428f6",
    "idx": 8,
    "time": "2021-02-06T18:55:50.684Z",
    "type": "execution"
   },
   {
    "id": "166fd4bb85d84a50b6e68cd2a14428f6",
    "time": "2021-02-06T18:55:50.738Z",
    "type": "completion"
   },
   {
    "code": "np.arange(1, 10)",
    "id": "166fd4bb85d84a50b6e68cd2a14428f6",
    "idx": 8,
    "time": "2021-02-06T18:55:56.492Z",
    "type": "execution"
   },
   {
    "id": "166fd4bb85d84a50b6e68cd2a14428f6",
    "time": "2021-02-06T18:55:56.559Z",
    "type": "completion"
   },
   {
    "code": "l1",
    "id": "166fd4bb85d84a50b6e68cd2a14428f6",
    "idx": 8,
    "time": "2021-02-06T18:56:08.844Z",
    "type": "execution"
   },
   {
    "id": "166fd4bb85d84a50b6e68cd2a14428f6",
    "time": "2021-02-06T18:56:08.948Z",
    "type": "completion"
   },
   {
    "code": "# #Neural Network Shape Test\nNNtest = Neural_Network(inputSize = 3, outputSize = 9, hiddenSize = [3,3,3] , learning_rate = 0.001)\nNNtest(torch.FloatTensor([[1, 1, 1], [1, 1, 1]]))\n",
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "idx": 6,
    "time": "2021-02-06T18:57:33.980Z",
    "type": "execution"
   },
   {
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "time": "2021-02-06T18:57:34.052Z",
    "type": "completion"
   },
   {
    "code": "lab_train",
    "id": "21db5e73be92486186d3fda30923ec0e",
    "idx": 10,
    "time": "2021-02-06T18:58:18.513Z",
    "type": "execution"
   },
   {
    "id": "21db5e73be92486186d3fda30923ec0e",
    "time": "2021-02-06T18:58:18.676Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 50\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [10], colors_num), (input_size, [50], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "edd0058b1003415baca0ba38b6e34e64",
    "idx": 9,
    "time": "2021-02-06T18:59:51.920Z",
    "type": "execution"
   },
   {
    "id": "edd0058b1003415baca0ba38b6e34e64",
    "time": "2021-02-06T18:59:52.152Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_test, NN(input_test))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "f6ce5e0006d345e19be7b61f8d00e067",
    "idx": 10,
    "time": "2021-02-06T18:59:52.444Z",
    "type": "execution"
   },
   {
    "id": "f6ce5e0006d345e19be7b61f8d00e067",
    "time": "2021-02-06T19:01:15.324Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "idx": 11,
    "time": "2021-02-06T19:01:24.056Z",
    "type": "execution"
   },
   {
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "time": "2021-02-06T19:01:24.162Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "idx": 11,
    "time": "2021-02-06T19:02:43.827Z",
    "type": "execution"
   },
   {
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "time": "2021-02-06T19:02:43.934Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    print(shape[0], shape[1], shape[2])\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "idx": 11,
    "time": "2021-02-06T19:02:58.448Z",
    "type": "execution"
   },
   {
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "time": "2021-02-06T19:02:58.545Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    print(net-num)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "idx": 11,
    "time": "2021-02-06T19:03:45.357Z",
    "type": "execution"
   },
   {
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "time": "2021-02-06T19:03:45.458Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    print(net_num)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "idx": 11,
    "time": "2021-02-06T19:03:51.700Z",
    "type": "execution"
   },
   {
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "time": "2021-02-06T19:03:51.777Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        if check == sum(x):\n            shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "64d8564a7b884a1984405f392fabb21e",
    "idx": 12,
    "time": "2021-02-06T19:07:47.762Z",
    "type": "execution"
   },
   {
    "id": "64d8564a7b884a1984405f392fabb21e",
    "time": "2021-02-06T19:07:47.842Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "d910e0e2e334411483adc407dcd04424",
    "idx": 13,
    "time": "2021-02-06T19:07:49.283Z",
    "type": "execution"
   },
   {
    "id": "d910e0e2e334411483adc407dcd04424",
    "time": "2021-02-06T19:07:49.330Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "818fef46e19e4960a9b9281a596a52f7",
    "idx": 14,
    "time": "2021-02-06T19:08:00.217Z",
    "type": "execution"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\n\"\"\"\nnot working\nfrom Language_Data_Scraper.py import *\n\"\"\"\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd",
    "id": "0e403ae3e9b4423785f11c3d7b46a36c",
    "idx": 0,
    "time": "2021-02-06T22:44:54.280Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "49e9b39ffb2944d48b6ccdc09010b658",
    "idx": 1,
    "time": "2021-02-06T22:44:54.450Z",
    "type": "execution"
   },
   {
    "code": "term_data['#Lnum'].unique()",
    "id": "5121391a3fb9447c863f9b6a21ed2789",
    "idx": 2,
    "time": "2021-02-06T22:44:54.610Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "03a37b7ff7ce43208b74d0bbc44eea00",
    "idx": 3,
    "time": "2021-02-06T22:44:54.767Z",
    "type": "execution"
   },
   {
    "code": "# debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\n# debug1['L*'] = (debug1['L*'] - debug1['L*'].mean()) / debug1['L*'].std()\n# debug1",
    "id": "3b364c7fbfe047e990150f2894e5d207",
    "idx": 4,
    "time": "2021-02-06T22:44:54.922Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "90feda1a5c5c41a68e68ac9822b47500",
    "idx": 5,
    "time": "2021-02-06T22:44:55.585Z",
    "type": "execution"
   },
   {
    "code": "# #Neural Network Shape Test\n# NNtest = Neural_Network(inputSize = 3, outputSize = 9, hiddenSize = [3,3,3] , learning_rate = 0.001)\n# NNtest(torch.FloatTensor([[1, 1, 1], [1, 1, 1]]))\n",
    "id": "c4306a3ea3554ad2a7b64ce9a660fc86",
    "idx": 6,
    "time": "2021-02-06T22:44:56.042Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) \\\n                              for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "70e03cca165945b78f53df7ac997e754",
    "idx": 7,
    "time": "2021-02-06T22:44:56.541Z",
    "type": "execution"
   },
   {
    "id": "0e403ae3e9b4423785f11c3d7b46a36c",
    "time": "2021-02-06T22:44:56.693Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "fc21542794344e2781ed7c8323668401",
    "idx": 8,
    "time": "2021-02-06T22:44:56.814Z",
    "type": "execution"
   },
   {
    "id": "49e9b39ffb2944d48b6ccdc09010b658",
    "time": "2021-02-06T22:44:57.172Z",
    "type": "completion"
   },
   {
    "id": "5121391a3fb9447c863f9b6a21ed2789",
    "time": "2021-02-06T22:44:57.181Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 50\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [10], colors_num), (input_size, [50], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "01693c63b3c54f879d46abdd5696ea35",
    "idx": 9,
    "time": "2021-02-06T22:44:57.196Z",
    "type": "execution"
   },
   {
    "id": "03a37b7ff7ce43208b74d0bbc44eea00",
    "time": "2021-02-06T22:44:57.225Z",
    "type": "completion"
   },
   {
    "id": "3b364c7fbfe047e990150f2894e5d207",
    "time": "2021-02-06T22:44:57.241Z",
    "type": "completion"
   },
   {
    "id": "90feda1a5c5c41a68e68ac9822b47500",
    "time": "2021-02-06T22:44:57.257Z",
    "type": "completion"
   },
   {
    "id": "c4306a3ea3554ad2a7b64ce9a660fc86",
    "time": "2021-02-06T22:44:57.272Z",
    "type": "completion"
   },
   {
    "id": "70e03cca165945b78f53df7ac997e754",
    "time": "2021-02-06T22:44:57.373Z",
    "type": "completion"
   },
   {
    "id": "fc21542794344e2781ed7c8323668401",
    "time": "2021-02-06T22:44:57.444Z",
    "type": "completion"
   },
   {
    "id": "01693c63b3c54f879d46abdd5696ea35",
    "time": "2021-02-06T22:44:57.445Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_test, NN(input_test))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "04811196407b40048e92e59766e727ff",
    "idx": 10,
    "time": "2021-02-06T22:44:57.481Z",
    "type": "execution"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    print(net_num)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "5fcbbc9de1f3451b8c81b54e170f3d99",
    "idx": 11,
    "time": "2021-02-06T22:44:58.224Z",
    "type": "execution"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        if check == sum(x):\n            shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "2d09069ca66742728f94f15c244e85cf",
    "idx": 12,
    "time": "2021-02-06T22:44:59.542Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "0fe8dc7016284b4f8380932c7b98392f",
    "idx": 13,
    "time": "2021-02-06T22:44:59.786Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "81206a89a4654a4cb6e7e785c66e9a56",
    "idx": 14,
    "time": "2021-02-06T22:45:00.167Z",
    "type": "execution"
   },
   {
    "id": "04811196407b40048e92e59766e727ff",
    "time": "2021-02-06T22:45:51.868Z",
    "type": "completion"
   },
   {
    "id": "5fcbbc9de1f3451b8c81b54e170f3d99",
    "time": "2021-02-06T22:45:51.869Z",
    "type": "completion"
   },
   {
    "id": "2d09069ca66742728f94f15c244e85cf",
    "time": "2021-02-06T22:45:51.870Z",
    "type": "completion"
   },
   {
    "id": "0fe8dc7016284b4f8380932c7b98392f",
    "time": "2021-02-06T22:45:51.871Z",
    "type": "completion"
   },
   {
    "id": "81206a89a4654a4cb6e7e785c66e9a56",
    "time": "2021-02-06T22:45:51.875Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        if check == sum(x):\n            shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "2d09069ca66742728f94f15c244e85cf",
    "idx": 12,
    "time": "2021-02-06T22:46:01.870Z",
    "type": "execution"
   },
   {
    "id": "2d09069ca66742728f94f15c244e85cf",
    "time": "2021-02-06T22:46:01.938Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "0fe8dc7016284b4f8380932c7b98392f",
    "idx": 13,
    "time": "2021-02-06T22:46:02.509Z",
    "type": "execution"
   },
   {
    "id": "0fe8dc7016284b4f8380932c7b98392f",
    "time": "2021-02-06T22:46:02.561Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "81206a89a4654a4cb6e7e785c66e9a56",
    "idx": 14,
    "time": "2021-02-06T22:46:03.018Z",
    "type": "execution"
   },
   {
    "code": "# #Array of losses over training period for each network\n# for net_num, shape in enumerate(network_shapes):\n#     print('Network Shape:',flush = True)\n#     print('Input Size = ' + str(shape[0]), flush = True)\n#     print('Hidden Size = ' + str(shape[1]), flush = True)\n#     print('Output Size = ' + str(shape[2]), flush = True)\n#     NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n#     for i in range(num_iters):\n#         #Calculating l1 error\n#         error = NN.l1error(output_test, NN(input_test))\n#         if i == 0: \n#             dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n#         else:\n#             dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n#         NN.train(input_train, output_train)\n#     #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n#     NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "04811196407b40048e92e59766e727ff",
    "idx": 10,
    "time": "2021-02-06T22:46:39.071Z",
    "type": "execution"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "277px",
    "left": "1061px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
