{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "import Language_Data_Scraper as LD\n",
    "sys.path.insert(0, '..')\n",
    "from net_framework import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1], [2], [3], [4], [5], [6], [3, 3], [7], [8], [4, 4], [9], [3, 3, 3], [10], [5, 5], [3, 4, 3], [11], [3, 5, 3], [12], [6, 6], [4, 4, 4], [13], [4, 5, 4], [14], [7, 7], [4, 6, 4], [15], [5, 5, 5], [16], [8, 8], [5, 6, 5], [17], [5, 7, 5], [18], [9, 9], [6, 6, 6], [19], [6, 7, 6], [20], [10, 10], [6, 8, 6], [21], [7, 7, 7], [22], [11, 11], [7, 8, 7], [23], [7, 9, 7], [24], [12, 12], [8, 8, 8]]\n"
     ]
    }
   ],
   "source": [
    "node_num = range(1,25)\n",
    "layer_num = range(1,4)\n",
    "\n",
    "\n",
    "shape_collection = []\n",
    "for node in node_num:\n",
    "    if node < 3:\n",
    "        shape_collection.append([node])\n",
    "\n",
    "def trickle(arr, iteration_left, check):\n",
    "    if iteration_left == 0:\n",
    "        global shape_collection\n",
    "        #running the int fxn to make sure we don't have floats\n",
    "        mp = map(int, arr)\n",
    "        x = list(mp)\n",
    "        if check == sum(x):\n",
    "            shape_collection.append(x)\n",
    "    else:\n",
    "        new_arr = [0]+ arr + [0]\n",
    "        #recursively expanding the list symmetrically\n",
    "        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n",
    "            new_arr[0] += 1\n",
    "            new_arr[1] -= 1\n",
    "            new_arr[-1] += 1\n",
    "            new_arr[-2] -= 1\n",
    "        trickle(new_arr, iteration_left - 1, check)\n",
    "\n",
    "for node in node_num:\n",
    "    for layer in layer_num:\n",
    "        if node//layer < 3:\n",
    "            continue\n",
    "        if layer%2 == 0:\n",
    "            trickle([node/2, node/2], (layer-2)/2, node)\n",
    "        else:\n",
    "            trickle([node], (layer-1)/2, node)      \n",
    "\n",
    "print(shape_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingHelper:\n",
    "\n",
    "    def __init__(self, language_number):\n",
    "        self.num_iters = 1000\n",
    "        self.input_size = 3\n",
    "        self.rate = 0.001\n",
    "        self.language = LD.LanguageData(language_number)\n",
    "        self.colors_num = self.language.colors_num()\n",
    "        self.network_shapes = [(self.input_size, s, self.colors_num) for s in shape_collection]\n",
    "\n",
    "    def shuffle(self):\n",
    "        lab_train, lab_test, chip_train, chip_test = train_test_split(self.language.lab_norm, self.language.chip_norm(), test_size=0.2,\n",
    "        shuffle = True)\n",
    "        input_train = torch.FloatTensor(lab_train)\n",
    "        output_train = torch.FloatTensor(chip_train)\n",
    "        input_test= torch.FloatTensor(lab_test)\n",
    "        output_test = torch.FloatTensor(chip_test)\n",
    "        return input_train, output_train, input_test, output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(3, [1], 9), (3, [2], 9), (3, [3], 9), (3, [4], 9), (3, [5], 9), (3, [6], 9), (3, [3, 3], 9), (3, [7], 9), (3, [8], 9), (3, [4, 4], 9), (3, [9], 9), (3, [3, 3, 3], 9), (3, [10], 9), (3, [5, 5], 9), (3, [3, 4, 3], 9), (3, [11], 9), (3, [3, 5, 3], 9), (3, [12], 9), (3, [6, 6], 9), (3, [4, 4, 4], 9), (3, [13], 9), (3, [4, 5, 4], 9), (3, [14], 9), (3, [7, 7], 9), (3, [4, 6, 4], 9), (3, [15], 9), (3, [5, 5, 5], 9), (3, [16], 9), (3, [8, 8], 9), (3, [5, 6, 5], 9), (3, [17], 9), (3, [5, 7, 5], 9), (3, [18], 9), (3, [9, 9], 9), (3, [6, 6, 6], 9), (3, [19], 9), (3, [6, 7, 6], 9), (3, [20], 9), (3, [10, 10], 9), (3, [6, 8, 6], 9), (3, [21], 9), (3, [7, 7, 7], 9), (3, [22], 9), (3, [11, 11], 9), (3, [7, 8, 7], 9), (3, [23], 9), (3, [7, 9, 7], 9), (3, [24], 9), (3, [12, 12], 9), (3, [8, 8, 8], 9)]\n"
     ]
    }
   ],
   "source": [
    "print(TrainingHelper(1).network_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "\n",
    "    def __init__(self, language_number):\n",
    "        #Array of losses over training period for each network\n",
    "        self.num_average = 10\n",
    "        self.output_file = {}\n",
    "        for n in node_num:\n",
    "            self.output_file[n] = {}\n",
    "        self.th = TrainingHelper(language_number)\n",
    "\n",
    "    def trainer(self):\n",
    "        for net_num, shape in enumerate(self.th.network_shapes):\n",
    "            print(\"Training: \",shape)\n",
    "            net_error_arr = []\n",
    "            for j in range(self.num_average):\n",
    "                print('Run ' + str(j+1))\n",
    "                NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n",
    "                                    hiddenSize = shape[1] , learning_rate = self.th.rate)\n",
    "                error_arr = []\n",
    "                prev_error = 0\n",
    "                strike = 0\n",
    "\n",
    "                input_train, output_train, input_test, output_test = self.th.shuffle()\n",
    "\n",
    "                for i in range(self.th.num_iters):  \n",
    "                    NN.train(input_train, output_train)\n",
    "                    validation_error = NN.l1error(output_test, NN(input_test))\n",
    "                    #Printing error\n",
    "                    if i == 0: \n",
    "                        dh = display(\"#\" + str(i) + \" Validation Error: \" + str(validation_error), display_id=True)\n",
    "                    else:\n",
    "                        dh.update(\"#\" + str(i) + \" Validation Error: \" + str(validation_error))\n",
    "                    \n",
    "                    #zero small error change\n",
    "                    if i == 0:\n",
    "                        strike = 0\n",
    "                    #adding error to array\n",
    "                    error_arr.append(validation_error)\n",
    "                    #waiting for number 'too small' decreases or increases in validation error before ending training\n",
    "                    if (prev_error < validation_error) and i > 100:\n",
    "                        if strike > 5:\n",
    "                            print(\"Complete at iteration \", i, \"\\nFinal error: \", np.min(error_arr), \"\\n\")\n",
    "                            break\n",
    "                        else:\n",
    "                            strike += 1\n",
    "                    prev_error = validation_error\n",
    "                net_error_arr.append(np.min(error_arr))\n",
    "            output_file[sum(shape[1])][len(shape[1])] = [np.mean(net_error_arr), np.std(net_error_arr)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input number here\n",
    "Train(56).trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('validation_errors.json', 'w') as f:\n",
    "            json.dump(output_file, f)"
   ]
  }
 ]
}