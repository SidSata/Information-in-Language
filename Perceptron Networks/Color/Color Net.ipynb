{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "sys.path.insert(0, '..')\n",
    "from net_framework import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Lnum</th>\n",
       "      <th>#snum</th>\n",
       "      <th>#cnum</th>\n",
       "      <th>Term Abbrev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>LE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>WK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>LF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #Lnum  #snum  #cnum Term Abbrev\n",
       "0      1      1      1          LB\n",
       "1      1      1      2          LB\n",
       "2      1      1      3          LE\n",
       "3      1      1      4          WK\n",
       "4      1      1      5          LF"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\n",
    "term_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\n",
    "term_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-f811cbef4bec>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  locations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\n",
      "<ipython-input-22-f811cbef4bec>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  locations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\n",
      "<ipython-input-22-f811cbef4bec>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  locations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#cnum</th>\n",
       "      <th>Normalized-L</th>\n",
       "      <th>Normalized-a</th>\n",
       "      <th>Normalized-b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025675</td>\n",
       "      <td>-0.138822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>274</td>\n",
       "      <td>0.876301</td>\n",
       "      <td>-0.025504</td>\n",
       "      <td>-0.138822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129</td>\n",
       "      <td>0.876301</td>\n",
       "      <td>0.070445</td>\n",
       "      <td>-0.106039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230</td>\n",
       "      <td>0.876301</td>\n",
       "      <td>0.070101</td>\n",
       "      <td>-0.089951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>302</td>\n",
       "      <td>0.876301</td>\n",
       "      <td>0.070617</td>\n",
       "      <td>-0.072041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #cnum  Normalized-L  Normalized-a  Normalized-b\n",
       "0    141      1.000000     -0.025675     -0.138822\n",
       "1    274      0.876301     -0.025504     -0.138822\n",
       "2    129      0.876301      0.070445     -0.106039\n",
       "3    230      0.876301      0.070101     -0.089951\n",
       "4    302      0.876301      0.070617     -0.072041"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\n",
    "locations = cnum_data[['#cnum']]\n",
    "locations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\n",
    "locations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\n",
    "locations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\n",
    "display(locations.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = locations.sort_values('#cnum')\n",
    "chip_num = list(locations['#cnum'])\n",
    "lab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = term_data[term_data.get('#Lnum').eq(1)]\n",
    "unique_symbols = list(l1['Term Abbrev'].unique())\n",
    "l1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\n",
    "l1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         LB    LE    WK    LF     F     G     S   GB    FU\n",
      "#cnum                                                     \n",
      "1      0.36  0.00  0.00  0.04  0.08  0.52  0.00  0.0  0.00\n",
      "2      0.12  0.00  0.04  0.12  0.60  0.04  0.00  0.0  0.08\n",
      "3      0.00  0.96  0.04  0.00  0.00  0.00  0.00  0.0  0.00\n",
      "4      0.28  0.00  0.40  0.00  0.08  0.04  0.20  0.0  0.00\n",
      "5      0.04  0.00  0.00  0.20  0.68  0.08  0.00  0.0  0.00\n",
      "...     ...   ...   ...   ...   ...   ...   ...  ...   ...\n",
      "326    0.08  0.20  0.36  0.00  0.20  0.00  0.12  0.0  0.04\n",
      "327    0.04  0.00  0.00  0.68  0.20  0.04  0.04  0.0  0.00\n",
      "328    0.64  0.00  0.08  0.00  0.00  0.20  0.08  0.0  0.00\n",
      "329    0.08  0.08  0.40  0.00  0.12  0.00  0.28  0.0  0.04\n",
      "330    0.04  0.00  0.00  0.72  0.24  0.00  0.00  0.0  0.00\n",
      "\n",
      "[330 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\n",
    "l1_result.index += 1\n",
    "l1_result.index.name = '#cnum'\n",
    "l1_result.columns = unique_symbols\n",
    "print(l1_result)\n",
    "chip_norm = []\n",
    "#pull the percentage for each cnum\n",
    "for x in chip_num:\n",
    "    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_num = range(10, 130, 10)\n",
    "layer_num = range(1,6)\n",
    "# checking if the array is symmetrical\n",
    "\n",
    "shape_collection = []\n",
    "\n",
    "def trickle(arr, iteration_left):\n",
    "    if iteration_left == 0:\n",
    "        global shape_collection\n",
    "        #running the int fxn to make sure we don't have floats\n",
    "        mp = map(int, arr)\n",
    "        shape_collection.append(list(mp))\n",
    "    else:\n",
    "        new_arr = [0]+ arr + [0]\n",
    "        #recursively expanding the list symmetrically\n",
    "        while new_arr[0] < new_arr[1]-1 and new_arr[-1] < new_arr[-2]-1:\n",
    "            new_arr[0] += 1\n",
    "            new_arr[1] -= 1\n",
    "            new_arr[-1] += 1\n",
    "            new_arr[-2] -= 1\n",
    "        trickle(new_arr, iteration_left - 1)\n",
    "\n",
    "for node in node_num:\n",
    "    for layer in layer_num:\n",
    "        if node//layer < 3:\n",
    "            continue\n",
    "        if layer%2 == 0:\n",
    "            trickle([node/2, node/2], (layer-2)/2)\n",
    "        else:\n",
    "            trickle([node], (layer-1)/2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "print(shape_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Originally designed to find every single permutation\n",
    "#Oops misread directions\n",
    "node_num = range(10, 130, 10)\n",
    "layer_num = range(1,6)\n",
    "\n",
    "def check(arr):\n",
    "    for a in arr:\n",
    "        if a < 3:\n",
    "            return False\n",
    "    inverse = arr[::-1]\n",
    "    if arr == inverse:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "shape_collection = []\n",
    "\n",
    "def trickle_down(node, layer):\n",
    "    if layer > 1:\n",
    "        for i in range(0, node[-1]):\n",
    "            new_arr = node[:-1] + [node[-1]-i] + [i]\n",
    "            trickle_down(new_arr,layer-1)\n",
    "    else:\n",
    "        if check(node):\n",
    "            global shape_collection\n",
    "            shape_collection.append([sum(node), len(node),node])\n",
    "\n",
    "for node in node_num:\n",
    "    for layer in layer_num:\n",
    "        trickle_down([node], layer)\n",
    "\n",
    "print(shape_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of training iterations\n",
    "num_iters = 500\n",
    "\n",
    "#Listing out the shapes of each model\n",
    "colors_num = len(chip_norm[0])\n",
    "input_size = 3\n",
    "\n",
    "network_shapes = []\n",
    "for s in shape_collection:\n",
    "    network_shapes.append((input_size,s,colors_num))\n",
    "\n",
    "#Learning rate of the network\n",
    "rate = 0.001\n",
    "\n",
    "#Generating Training Data\n",
    "input_train, output_train, input_test, output_test = 0, 0, 0, 0\n",
    "def shuffle():\n",
    "    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n",
    "    #test run on whole dataset\n",
    "    #lab_train, lab_test = lab_norm, lab_norm\n",
    "    #chip_train, chip_test = chip_norm, chip_norm\n",
    "    \n",
    "    global input_train, output_train, input_test, output_test\n",
    "    input_train = torch.FloatTensor(lab_train)\n",
    "    output_train = torch.FloatTensor(chip_train)\n",
    "    input_test= torch.FloatTensor(lab_test)\n",
    "    output_test = torch.FloatTensor(chip_test)\n",
    "\n",
    "print(network_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array of losses over training period for each network\n",
    "output_file = {}\n",
    "for n in node_num:\n",
    "    output_file[n] = {}\n",
    "    \n",
    "for net_num, shape in enumerate(network_shapes):\n",
    "    shuffle()\n",
    "    print(\"Training: \",shape)\n",
    "    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n",
    "                        hiddenSize = shape[1] , learning_rate = rate)\n",
    "    error_arr = []\n",
    "    prev_error = 0\n",
    "    strike = 0\n",
    "    \n",
    "    for i in range(num_iters):       \n",
    "        NN.train(input_train, output_train)\n",
    "        validation_error = NN.l1error(output_test, NN(input_test))\n",
    "        #zero mistake counter at new training\n",
    "        if i == 0:\n",
    "            strike = 0\n",
    "        #adding error to array\n",
    "        error_arr.append(validation_error)\n",
    "        #wait for them to grow up\n",
    "        if prev_error < validation_error and i > 100:\n",
    "            if strike > 3:\n",
    "                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n",
    "                break\n",
    "            else:\n",
    "                strike += 1\n",
    "        prev_error = validation_error\n",
    "\n",
    "    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n",
    "    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n",
    "    output_file[sum(shape[1])][len(shape[1])] = error_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('validation_errors.json', 'w') as f:\n",
    "    json.dump(output_file, f)"
   ]
  }
 ],
 "metadata": {
  "history": [
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "25416a549cd04f4487e7bd5f8618d338",
    "idx": 0,
    "time": "2021-01-20T18:41:05.939Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "85a33b45f61648a08e12c86fe4413df7",
    "idx": 1,
    "time": "2021-01-20T18:41:05.946Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "4fd00db43d74435dba582a93ac732f39",
    "idx": 2,
    "time": "2021-01-20T18:41:05.948Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "1d186efc421a4fff8972cbc8e044bfdb",
    "idx": 3,
    "time": "2021-01-20T18:41:05.950Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "797406f9da5c4e788dd5f6fe416ecec9",
    "idx": 4,
    "time": "2021-01-20T18:41:05.953Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "e7dff5ca819048459a6ae624e6d6ce7b",
    "idx": 5,
    "time": "2021-01-20T18:41:05.954Z",
    "type": "execution"
   },
   {
    "id": "25416a549cd04f4487e7bd5f8618d338",
    "time": "2021-01-20T18:41:11.110Z",
    "type": "completion"
   },
   {
    "id": "85a33b45f61648a08e12c86fe4413df7",
    "time": "2021-01-20T18:41:11.504Z",
    "type": "completion"
   },
   {
    "id": "4fd00db43d74435dba582a93ac732f39",
    "time": "2021-01-20T18:41:11.585Z",
    "type": "completion"
   },
   {
    "id": "1d186efc421a4fff8972cbc8e044bfdb",
    "time": "2021-01-20T18:41:11.587Z",
    "type": "completion"
   },
   {
    "id": "797406f9da5c4e788dd5f6fe416ecec9",
    "time": "2021-01-20T18:41:11.595Z",
    "type": "completion"
   },
   {
    "id": "e7dff5ca819048459a6ae624e6d6ce7b",
    "time": "2021-01-20T18:41:11.871Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "25416a549cd04f4487e7bd5f8618d338",
    "idx": 0,
    "time": "2021-01-20T18:42:10.430Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "85a33b45f61648a08e12c86fe4413df7",
    "idx": 1,
    "time": "2021-01-20T18:42:10.435Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "4fd00db43d74435dba582a93ac732f39",
    "idx": 2,
    "time": "2021-01-20T18:42:10.436Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "1d186efc421a4fff8972cbc8e044bfdb",
    "idx": 3,
    "time": "2021-01-20T18:42:10.437Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "797406f9da5c4e788dd5f6fe416ecec9",
    "idx": 4,
    "time": "2021-01-20T18:42:10.438Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "e7dff5ca819048459a6ae624e6d6ce7b",
    "idx": 5,
    "time": "2021-01-20T18:42:10.439Z",
    "type": "execution"
   },
   {
    "code": "#Originally designed to find every single permutation\n#Oops misread directions\nnode_num = range(10, 130, 10)\nlayer_num = range(1,6)\n\ndef check(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\nshape_collection = []\n\ndef trickle_down(node, layer):\n    if layer > 1:\n        for i in range(0, node[-1]):\n            new_arr = node[:-1] + [node[-1]-i] + [i]\n            trickle_down(new_arr,layer-1)\n    else:\n        if check(node):\n            global shape_collection\n            shape_collection.append(sum(node), len(node),node)\n\nfor node in node_num:\n    for layer in layer_num:\n        trickle_down([node], layer)\n\nprint(shape_collection)",
    "id": "413ad261276748d89dd277b8fabceed0",
    "idx": 7,
    "time": "2021-01-20T18:42:10.446Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "2881ec8a2c8b4d1c89126e11a80dfb8b",
    "idx": 8,
    "time": "2021-01-20T18:42:10.447Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "82de63d5a1c94978ab0eb3b95bcb444d",
    "idx": 9,
    "time": "2021-01-20T18:42:10.448Z",
    "type": "execution"
   },
   {
    "code": "with open('validation_errors.json', 'w') as f:\n    json.dump(output_file, f)",
    "id": "2941038e293a41a39f99076758f6b743",
    "idx": 10,
    "time": "2021-01-20T18:42:10.449Z",
    "type": "execution"
   },
   {
    "id": "25416a549cd04f4487e7bd5f8618d338",
    "time": "2021-01-20T18:42:10.515Z",
    "type": "completion"
   },
   {
    "id": "85a33b45f61648a08e12c86fe4413df7",
    "time": "2021-01-20T18:42:10.777Z",
    "type": "completion"
   },
   {
    "id": "4fd00db43d74435dba582a93ac732f39",
    "time": "2021-01-20T18:42:10.813Z",
    "type": "completion"
   },
   {
    "id": "1d186efc421a4fff8972cbc8e044bfdb",
    "time": "2021-01-20T18:42:10.815Z",
    "type": "completion"
   },
   {
    "id": "797406f9da5c4e788dd5f6fe416ecec9",
    "time": "2021-01-20T18:42:10.859Z",
    "type": "completion"
   },
   {
    "id": "e7dff5ca819048459a6ae624e6d6ce7b",
    "time": "2021-01-20T18:42:11.109Z",
    "type": "completion"
   },
   {
    "id": "413ad261276748d89dd277b8fabceed0",
    "time": "2021-01-20T18:42:11.246Z",
    "type": "completion"
   },
   {
    "id": "2881ec8a2c8b4d1c89126e11a80dfb8b",
    "time": "2021-01-20T18:42:11.291Z",
    "type": "completion"
   },
   {
    "id": "82de63d5a1c94978ab0eb3b95bcb444d",
    "time": "2021-01-20T18:42:11.292Z",
    "type": "completion"
   },
   {
    "id": "2941038e293a41a39f99076758f6b743",
    "time": "2021-01-20T18:42:11.293Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "25416a549cd04f4487e7bd5f8618d338",
    "idx": 0,
    "time": "2021-01-20T18:42:23.296Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "85a33b45f61648a08e12c86fe4413df7",
    "idx": 1,
    "time": "2021-01-20T18:42:23.298Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "4fd00db43d74435dba582a93ac732f39",
    "idx": 2,
    "time": "2021-01-20T18:42:23.299Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "1d186efc421a4fff8972cbc8e044bfdb",
    "idx": 3,
    "time": "2021-01-20T18:42:23.300Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "797406f9da5c4e788dd5f6fe416ecec9",
    "idx": 4,
    "time": "2021-01-20T18:42:23.300Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "e7dff5ca819048459a6ae624e6d6ce7b",
    "idx": 5,
    "time": "2021-01-20T18:42:23.301Z",
    "type": "execution"
   },
   {
    "id": "25416a549cd04f4487e7bd5f8618d338",
    "time": "2021-01-20T18:42:23.351Z",
    "type": "completion"
   },
   {
    "id": "85a33b45f61648a08e12c86fe4413df7",
    "time": "2021-01-20T18:42:23.680Z",
    "type": "completion"
   },
   {
    "id": "4fd00db43d74435dba582a93ac732f39",
    "time": "2021-01-20T18:42:23.754Z",
    "type": "completion"
   },
   {
    "id": "1d186efc421a4fff8972cbc8e044bfdb",
    "time": "2021-01-20T18:42:23.760Z",
    "type": "completion"
   },
   {
    "id": "797406f9da5c4e788dd5f6fe416ecec9",
    "time": "2021-01-20T18:42:23.775Z",
    "type": "completion"
   },
   {
    "id": "e7dff5ca819048459a6ae624e6d6ce7b",
    "time": "2021-01-20T18:42:24.101Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport json",
    "id": "25416a549cd04f4487e7bd5f8618d338",
    "idx": 0,
    "time": "2021-01-20T18:42:40.686Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "85a33b45f61648a08e12c86fe4413df7",
    "idx": 1,
    "time": "2021-01-20T18:42:40.689Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "4fd00db43d74435dba582a93ac732f39",
    "idx": 2,
    "time": "2021-01-20T18:42:40.692Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "1d186efc421a4fff8972cbc8e044bfdb",
    "idx": 3,
    "time": "2021-01-20T18:42:40.694Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "797406f9da5c4e788dd5f6fe416ecec9",
    "idx": 4,
    "time": "2021-01-20T18:42:40.696Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "e7dff5ca819048459a6ae624e6d6ce7b",
    "idx": 5,
    "time": "2021-01-20T18:42:40.697Z",
    "type": "execution"
   },
   {
    "id": "25416a549cd04f4487e7bd5f8618d338",
    "time": "2021-01-20T18:42:40.763Z",
    "type": "completion"
   },
   {
    "id": "85a33b45f61648a08e12c86fe4413df7",
    "time": "2021-01-20T18:42:41.031Z",
    "type": "completion"
   },
   {
    "id": "4fd00db43d74435dba582a93ac732f39",
    "time": "2021-01-20T18:42:41.060Z",
    "type": "completion"
   },
   {
    "id": "1d186efc421a4fff8972cbc8e044bfdb",
    "time": "2021-01-20T18:42:41.062Z",
    "type": "completion"
   },
   {
    "id": "797406f9da5c4e788dd5f6fe416ecec9",
    "time": "2021-01-20T18:42:41.096Z",
    "type": "completion"
   },
   {
    "id": "e7dff5ca819048459a6ae624e6d6ce7b",
    "time": "2021-01-20T18:42:41.368Z",
    "type": "completion"
   },
   {
    "code": "#Originally designed to find every single permutation\n#Oops misread directions\nnode_num = range(10, 130, 10)\nlayer_num = range(1,6)\n\ndef check(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\nshape_collection = []\n\ndef trickle_down(node, layer):\n    if layer > 1:\n        for i in range(0, node[-1]):\n            new_arr = node[:-1] + [node[-1]-i] + [i]\n            trickle_down(new_arr,layer-1)\n    else:\n        if check(node):\n            global shape_collection\n            shape_collection.append([sum(node), len(node),node])\n\nfor node in node_num:\n    for layer in layer_num:\n        trickle_down([node], layer)\n\nprint(shape_collection)",
    "id": "413ad261276748d89dd277b8fabceed0",
    "idx": 7,
    "time": "2021-01-20T18:42:44.847Z",
    "type": "execution"
   },
   {
    "id": "413ad261276748d89dd277b8fabceed0",
    "time": "2021-01-20T18:43:04.794Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\ndef isSym(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\n#mirroring an array\ndef mirror(arr, desired_length):\n    if desired_length/len(arr) == 2:\n        return arr + arr[::-1]\n    else:\n        return arr[:-1] + arr[::-1]\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        shape_collection.append(arr)\n    else:\n        new_arr = [0]+ arr + [0]\n        while new_arr[0] < newarr[1] and new_arr[-1] < new_arr[-2]:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "idx": 6,
    "time": "2021-01-21T06:22:13.009Z",
    "type": "execution"
   },
   {
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "time": "2021-01-21T06:22:13.170Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\ndef isSym(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\n#mirroring an array\ndef mirror(arr, desired_length):\n    if desired_length/len(arr) == 2:\n        return arr + arr[::-1]\n    else:\n        return arr[:-1] + arr[::-1]\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        shape_collection.append(arr)\n    else:\n        new_arr = [0]+ arr + [0]\n        while new_arr[0] < new_arr[1] and new_arr[-1] < new_arr[-2]:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "idx": 6,
    "time": "2021-01-21T06:22:34.197Z",
    "type": "execution"
   },
   {
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "time": "2021-01-21T06:22:34.257Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\ndef isSym(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\n#mirroring an array\ndef mirror(arr, desired_length):\n    if desired_length/len(arr) == 2:\n        return arr + arr[::-1]\n    else:\n        return arr[:-1] + arr[::-1]\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        shape_collection.append(int(arr))\n    else:\n        new_arr = [0]+ arr + [0]\n        while new_arr[0] < new_arr[1] and new_arr[-1] < new_arr[-2]:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "idx": 6,
    "time": "2021-01-21T06:22:46.609Z",
    "type": "execution"
   },
   {
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "time": "2021-01-21T06:22:46.699Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\ndef isSym(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\n#mirroring an array\ndef mirror(arr, desired_length):\n    if desired_length/len(arr) == 2:\n        return arr + arr[::-1]\n    else:\n        return arr[:-1] + arr[::-1]\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        shape_collection.append(map(int, arr))\n    else:\n        new_arr = [0]+ arr + [0]\n        while new_arr[0] < new_arr[1] and new_arr[-1] < new_arr[-2]:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "idx": 6,
    "time": "2021-01-21T06:23:28.317Z",
    "type": "execution"
   },
   {
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "time": "2021-01-21T06:23:28.409Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\ndef isSym(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\n#mirroring an array\ndef mirror(arr, desired_length):\n    if desired_length/len(arr) == 2:\n        return arr + arr[::-1]\n    else:\n        return arr[:-1] + arr[::-1]\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        shape_collection.append(map(int(), arr))\n    else:\n        new_arr = [0]+ arr + [0]\n        while new_arr[0] < new_arr[1] and new_arr[-1] < new_arr[-2]:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "idx": 6,
    "time": "2021-01-21T06:24:13.934Z",
    "type": "execution"
   },
   {
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "time": "2021-01-21T06:24:14.009Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\ndef isSym(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\n#mirroring an array\ndef mirror(arr, desired_length):\n    if desired_length/len(arr) == 2:\n        return arr + arr[::-1]\n    else:\n        return arr[:-1] + arr[::-1]\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        shape_collection.append(int() for a in arr)\n    else:\n        new_arr = [0]+ arr + [0]\n        while new_arr[0] < new_arr[1] and new_arr[-1] < new_arr[-2]:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "idx": 6,
    "time": "2021-01-21T06:24:36.786Z",
    "type": "execution"
   },
   {
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "time": "2021-01-21T06:24:36.857Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\ndef isSym(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\n#mirroring an array\ndef mirror(arr, desired_length):\n    if desired_length/len(arr) == 2:\n        return arr + arr[::-1]\n    else:\n        return arr[:-1] + arr[::-1]\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        shape_collection.append([int() for a in arr])\n    else:\n        new_arr = [0]+ arr + [0]\n        while new_arr[0] < new_arr[1] and new_arr[-1] < new_arr[-2]:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "idx": 6,
    "time": "2021-01-21T06:24:46.105Z",
    "type": "execution"
   },
   {
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "time": "2021-01-21T06:24:46.210Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\ndef isSym(arr):\n    for a in arr:\n        if a < 3:\n            return False\n    inverse = arr[::-1]\n    if arr == inverse:\n        return True\n    else:\n        return False\n\n#mirroring an array\ndef mirror(arr, desired_length):\n    if desired_length/len(arr) == 2:\n        return arr + arr[::-1]\n    else:\n        return arr[:-1] + arr[::-1]\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        mp = map(int, arr)\n        shape_collection.append(list(mp))\n    else:\n        new_arr = [0]+ arr + [0]\n        while new_arr[0] < new_arr[1] and new_arr[-1] < new_arr[-2]:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "idx": 6,
    "time": "2021-01-21T06:26:20.137Z",
    "type": "execution"
   },
   {
    "id": "b7af04b8481d454c986fca4090fb1c49",
    "time": "2021-01-21T06:26:20.217Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(10, 130, 10)\nlayer_num = range(1,6)\n# checking if the array is symmetrical\n\nshape_collection = []\n\ndef trickle(arr, iteration_left):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        shape_collection.append(list(mp))\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-1 and new_arr[-1] < new_arr[-2]-1:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2)\n        else:\n            trickle([node], (layer-1)/2)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "778aa23fe0964fc68f4f15bb72d70497",
    "idx": 6,
    "time": "2021-01-21T06:28:14.496Z",
    "type": "execution"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "277px",
    "left": "800px",
    "right": "20px",
    "top": "69px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
