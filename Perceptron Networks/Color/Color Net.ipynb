{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "not working\n",
    "from Language_Data_Scraper.py import *\n",
    "\"\"\"\n",
    "sys.path.insert(0, '..')\n",
    "from net_framework import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   #Lnum  #snum  #cnum Term Abbrev\n",
       "0      1      1      1          LB\n",
       "1      1      1      2          LB\n",
       "2      1      1      3          LE\n",
       "3      1      1      4          WK\n",
       "4      1      1      5          LF"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>#Lnum</th>\n      <th>#snum</th>\n      <th>#cnum</th>\n      <th>Term Abbrev</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>LB</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>LB</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>LE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>WK</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>LF</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\n",
    "term_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\n",
    "term_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "   #cnum  Normalized-L  Normalized-a  Normalized-b\n0    141      1.000000     -0.025675     -0.138822\n1    274      0.876301     -0.025504     -0.138822\n2    129      0.876301      0.070445     -0.106039\n3    230      0.876301      0.070101     -0.089951\n4    302      0.876301      0.070617     -0.072041",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>#cnum</th>\n      <th>Normalized-L</th>\n      <th>Normalized-a</th>\n      <th>Normalized-b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>141</td>\n      <td>1.000000</td>\n      <td>-0.025675</td>\n      <td>-0.138822</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>274</td>\n      <td>0.876301</td>\n      <td>-0.025504</td>\n      <td>-0.138822</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>129</td>\n      <td>0.876301</td>\n      <td>0.070445</td>\n      <td>-0.106039</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>230</td>\n      <td>0.876301</td>\n      <td>0.070101</td>\n      <td>-0.089951</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>302</td>\n      <td>0.876301</td>\n      <td>0.070617</td>\n      <td>-0.072041</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\n",
    "locations = cnum_data[['#cnum']]\n",
    "locations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\n",
    "locations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\n",
    "locations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\n",
    "display(locations.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330]\n"
     ]
    }
   ],
   "source": [
    "locations = locations.sort_values('#cnum')\n",
    "chip_num = list(locations['#cnum'])\n",
    "lab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]\n",
    "print(chip_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = term_data[term_data.get('#Lnum').eq(1)]\n",
    "unique_symbols = list(l1['Term Abbrev'].unique())\n",
    "l1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\n",
    "l1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         LB    LE    WK    LF     F     G     S   GB    FU\n#cnum                                                     \n1      0.36  0.00  0.00  0.04  0.08  0.52  0.00  0.0  0.00\n2      0.12  0.00  0.04  0.12  0.60  0.04  0.00  0.0  0.08\n3      0.00  0.96  0.04  0.00  0.00  0.00  0.00  0.0  0.00\n4      0.28  0.00  0.40  0.00  0.08  0.04  0.20  0.0  0.00\n5      0.04  0.00  0.00  0.20  0.68  0.08  0.00  0.0  0.00\n...     ...   ...   ...   ...   ...   ...   ...  ...   ...\n326    0.08  0.20  0.36  0.00  0.20  0.00  0.12  0.0  0.04\n327    0.04  0.00  0.00  0.68  0.20  0.04  0.04  0.0  0.00\n328    0.64  0.00  0.08  0.00  0.00  0.20  0.08  0.0  0.00\n329    0.08  0.08  0.40  0.00  0.12  0.00  0.28  0.0  0.04\n330    0.04  0.00  0.00  0.72  0.24  0.00  0.00  0.0  0.00\n\n[330 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\n",
    "l1_result.index += 1\n",
    "l1_result.index.name = '#cnum'\n",
    "l1_result.columns = unique_symbols\n",
    "print(l1_result)\n",
    "chip_norm = []\n",
    "#pull the percentage for each cnum\n",
    "for x in chip_num:\n",
    "    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.0, 0.44, 0.28, 0.0, 0.16, 0.0, 0.12, 0.0, 0.0], [0.0, 0.28, 0.36, 0.04, 0.28, 0.0, 0.04, 0.0, 0.0], [0.0, 0.0, 0.0, 0.56, 0.4, 0.0, 0.0, 0.0, 0.04], [0.68, 0.0, 0.0, 0.0, 0.0, 0.28, 0.04, 0.0, 0.0], [0.0, 0.0, 0.0, 0.64, 0.28, 0.0, 0.04, 0.0, 0.04], [0.44, 0.0, 0.0, 0.04, 0.0, 0.52, 0.0, 0.0, 0.0], [0.32, 0.0, 0.0, 0.0, 0.04, 0.6, 0.04, 0.0, 0.0], [0.64, 0.0, 0.0, 0.0, 0.0, 0.32, 0.04, 0.0, 0.0], [0.12, 0.0, 0.04, 0.04, 0.36, 0.4, 0.04, 0.0, 0.0], [0.12, 0.0, 0.0, 0.08, 0.36, 0.36, 0.04, 0.0, 0.04], [0.56, 0.0, 0.0, 0.0, 0.08, 0.28, 0.08, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12, 0.0, 0.08, 0.12, 0.24, 0.36, 0.04, 0.0, 0.04], [0.0, 0.4, 0.4, 0.0, 0.12, 0.0, 0.08, 0.0, 0.0], [0.96, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.08, 0.52, 0.4, 0.0, 0.0, 0.0, 0.0], [0.04, 0.0, 0.0, 0.2, 0.68, 0.08, 0.0, 0.0, 0.0], [0.36, 0.04, 0.2, 0.0, 0.28, 0.0, 0.12, 0.0, 0.0], [0.0, 0.4, 0.28, 0.0, 0.12, 0.0, 0.2, 0.0, 0.0], [0.12, 0.08, 0.12, 0.0, 0.12, 0.44, 0.08, 0.0, 0.04], [0.0, 0.0, 0.0, 0.72, 0.2, 0.0, 0.04, 0.0, 0.04], [0.88, 0.0, 0.04, 0.0, 0.04, 0.04, 0.0, 0.0, 0.0], [0.8, 0.0, 0.0, 0.0, 0.0, 0.12, 0.08, 0.0, 0.0], [0.0, 0.24, 0.44, 0.04, 0.2, 0.0, 0.08, 0.0, 0.0], [0.0, 0.36, 0.28, 0.0, 0.28, 0.0, 0.08, 0.0, 0.0], [0.72, 0.0, 0.04, 0.0, 0.04, 0.16, 0.0, 0.04, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.88, 0.12, 0.0, 0.0, 0.0, 0.0], [0.6, 0.0, 0.04, 0.0, 0.04, 0.24, 0.08, 0.0, 0.0], [0.12, 0.0, 0.12, 0.08, 0.4, 0.12, 0.12, 0.0, 0.04], [0.08, 0.0, 0.04, 0.16, 0.64, 0.0, 0.08, 0.0, 0.0], [0.36, 0.0, 0.0, 0.0, 0.12, 0.48, 0.0, 0.0, 0.04], [0.0, 0.04, 0.24, 0.16, 0.4, 0.0, 0.12, 0.0, 0.04], [0.08, 0.0, 0.4, 0.0, 0.12, 0.04, 0.24, 0.04, 0.08], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.36, 0.28, 0.0, 0.24, 0.0, 0.12, 0.0, 0.0], [0.56, 0.0, 0.0, 0.0, 0.04, 0.36, 0.04, 0.0, 0.0], [0.0, 0.0, 0.04, 0.28, 0.36, 0.16, 0.12, 0.0, 0.04], [0.0, 0.96, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12, 0.0, 0.2, 0.0, 0.04, 0.44, 0.16, 0.0, 0.04], [0.44, 0.0, 0.0, 0.0, 0.04, 0.52, 0.0, 0.0, 0.0], [0.8, 0.0, 0.0, 0.04, 0.04, 0.08, 0.04, 0.0, 0.0], [0.88, 0.0, 0.0, 0.0, 0.0, 0.04, 0.08, 0.0, 0.0], [0.96, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.36, 0.0, 0.0, 0.0, 0.0, 0.64, 0.0, 0.0, 0.0], [0.08, 0.0, 0.08, 0.2, 0.56, 0.04, 0.04, 0.0, 0.0], [0.52, 0.0, 0.04, 0.0, 0.04, 0.36, 0.0, 0.0, 0.04], [0.0, 0.96, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.24, 0.36, 0.04, 0.28, 0.0, 0.08, 0.0, 0.0], [0.8, 0.0, 0.04, 0.04, 0.0, 0.08, 0.04, 0.0, 0.0], [0.0, 0.28, 0.44, 0.0, 0.24, 0.0, 0.04, 0.0, 0.0], [0.36, 0.0, 0.04, 0.0, 0.0, 0.52, 0.08, 0.0, 0.0], [0.0, 0.92, 0.04, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0], [0.08, 0.2, 0.36, 0.0, 0.2, 0.0, 0.12, 0.0, 0.04], [0.24, 0.0, 0.04, 0.04, 0.32, 0.2, 0.16, 0.0, 0.0], [0.88, 0.0, 0.0, 0.0, 0.0, 0.04, 0.04, 0.04, 0.0], [0.04, 0.32, 0.36, 0.0, 0.16, 0.0, 0.08, 0.0, 0.04], [0.12, 0.2, 0.2, 0.0, 0.2, 0.0, 0.28, 0.0, 0.0], [0.0, 0.0, 0.0, 0.76, 0.24, 0.0, 0.0, 0.0, 0.0], [0.12, 0.0, 0.08, 0.2, 0.48, 0.0, 0.08, 0.0, 0.04], [0.04, 0.0, 0.0, 0.72, 0.24, 0.0, 0.0, 0.0, 0.0], [0.76, 0.0, 0.0, 0.0, 0.04, 0.08, 0.12, 0.0, 0.0], [0.24, 0.4, 0.12, 0.0, 0.04, 0.04, 0.16, 0.0, 0.0], [0.68, 0.0, 0.08, 0.0, 0.04, 0.0, 0.2, 0.0, 0.0], [0.48, 0.0, 0.0, 0.04, 0.08, 0.28, 0.12, 0.0, 0.0], [0.0, 0.0, 0.16, 0.28, 0.44, 0.0, 0.12, 0.0, 0.0], [0.28, 0.0, 0.08, 0.0, 0.24, 0.32, 0.04, 0.0, 0.04], [0.88, 0.0, 0.04, 0.0, 0.04, 0.04, 0.0, 0.0, 0.0], [0.08, 0.0, 0.0, 0.28, 0.52, 0.08, 0.04, 0.0, 0.0], [0.64, 0.0, 0.04, 0.0, 0.12, 0.12, 0.08, 0.0, 0.0], [0.0, 0.96, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0], [0.0, 0.56, 0.36, 0.0, 0.04, 0.0, 0.04, 0.0, 0.0], [0.2, 0.0, 0.2, 0.0, 0.24, 0.2, 0.12, 0.0, 0.04], [0.52, 0.0, 0.0, 0.0, 0.0, 0.48, 0.0, 0.0, 0.0], [0.08, 0.0, 0.0, 0.48, 0.2, 0.12, 0.08, 0.0, 0.04], [0.0, 0.0, 0.08, 0.12, 0.36, 0.24, 0.16, 0.0, 0.04], [0.92, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0], [0.16, 0.0, 0.12, 0.0, 0.16, 0.32, 0.24, 0.0, 0.0], [0.0, 0.88, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04], [0.0, 0.0, 0.0, 0.48, 0.32, 0.0, 0.16, 0.0, 0.04], [0.32, 0.0, 0.12, 0.08, 0.12, 0.2, 0.12, 0.0, 0.04], [0.72, 0.04, 0.0, 0.0, 0.08, 0.08, 0.08, 0.0, 0.0], [0.56, 0.0, 0.04, 0.0, 0.04, 0.28, 0.08, 0.0, 0.0], [0.08, 0.08, 0.4, 0.0, 0.12, 0.0, 0.28, 0.0, 0.04], [0.04, 0.0, 0.08, 0.28, 0.4, 0.12, 0.08, 0.0, 0.0], [0.12, 0.0, 0.0, 0.32, 0.32, 0.2, 0.04, 0.0, 0.0], [0.92, 0.0, 0.0, 0.0, 0.04, 0.0, 0.04, 0.0, 0.0], [0.0, 0.2, 0.4, 0.0, 0.24, 0.0, 0.16, 0.0, 0.0], [0.0, 0.28, 0.28, 0.0, 0.32, 0.0, 0.08, 0.0, 0.04], [0.28, 0.0, 0.2, 0.0, 0.04, 0.24, 0.24, 0.0, 0.0], [0.6, 0.0, 0.0, 0.0, 0.12, 0.24, 0.04, 0.0, 0.0], [0.6, 0.0, 0.0, 0.0, 0.04, 0.32, 0.04, 0.0, 0.0], [0.04, 0.0, 0.0, 0.8, 0.04, 0.0, 0.08, 0.0, 0.04], [0.04, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4, 0.0, 0.0, 0.04, 0.04, 0.52, 0.0, 0.0, 0.0], [0.0, 0.96, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.64, 0.28, 0.0, 0.08, 0.0, 0.0], [0.04, 0.48, 0.32, 0.0, 0.12, 0.0, 0.04, 0.0, 0.0], [0.24, 0.28, 0.12, 0.0, 0.08, 0.12, 0.12, 0.0, 0.04], [0.16, 0.0, 0.08, 0.0, 0.24, 0.28, 0.2, 0.0, 0.04], [0.32, 0.0, 0.08, 0.08, 0.28, 0.16, 0.04, 0.0, 0.04], [0.04, 0.0, 0.12, 0.12, 0.64, 0.04, 0.0, 0.0, 0.04], [0.04, 0.6, 0.2, 0.0, 0.08, 0.0, 0.08, 0.0, 0.0], [0.04, 0.72, 0.12, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0], [0.0, 0.0, 0.0, 0.56, 0.36, 0.0, 0.04, 0.0, 0.04], [0.0, 0.96, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24, 0.0, 0.0, 0.08, 0.12, 0.4, 0.16, 0.0, 0.0], [0.0, 0.0, 0.04, 0.88, 0.08, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.4, 0.44, 0.0, 0.12, 0.0, 0.04], [0.12, 0.0, 0.2, 0.04, 0.36, 0.12, 0.16, 0.0, 0.0], [0.36, 0.0, 0.0, 0.04, 0.08, 0.52, 0.0, 0.0, 0.0], [0.16, 0.28, 0.28, 0.0, 0.24, 0.0, 0.04, 0.0, 0.0], [0.0, 0.0, 0.0, 0.72, 0.24, 0.0, 0.0, 0.0, 0.04], [0.48, 0.0, 0.0, 0.0, 0.0, 0.52, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.44, 0.48, 0.0, 0.04, 0.0, 0.04], [0.0, 0.0, 0.0, 0.64, 0.36, 0.0, 0.0, 0.0, 0.0], [0.0, 0.4, 0.32, 0.0, 0.12, 0.0, 0.12, 0.0, 0.04], [0.04, 0.88, 0.0, 0.0, 0.04, 0.0, 0.04, 0.0, 0.0], [0.0, 0.72, 0.24, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0], [0.0, 0.56, 0.32, 0.0, 0.04, 0.0, 0.08, 0.0, 0.0], [0.4, 0.0, 0.0, 0.0, 0.08, 0.48, 0.04, 0.0, 0.0], [0.56, 0.0, 0.0, 0.0, 0.04, 0.36, 0.04, 0.0, 0.0], [0.84, 0.0, 0.0, 0.0, 0.08, 0.0, 0.08, 0.0, 0.0], [0.0, 0.16, 0.08, 0.16, 0.56, 0.0, 0.0, 0.0, 0.04], [0.04, 0.0, 0.08, 0.44, 0.44, 0.0, 0.0, 0.0, 0.0], [0.48, 0.0, 0.0, 0.0, 0.04, 0.44, 0.04, 0.0, 0.0], [0.96, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0], [0.84, 0.0, 0.04, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0], [0.68, 0.0, 0.0, 0.0, 0.04, 0.28, 0.0, 0.0, 0.0], [0.44, 0.0, 0.12, 0.0, 0.24, 0.0, 0.2, 0.0, 0.0], [0.32, 0.04, 0.32, 0.0, 0.04, 0.08, 0.2, 0.0, 0.0], [0.16, 0.04, 0.16, 0.0, 0.4, 0.12, 0.08, 0.0, 0.04], [0.0, 0.76, 0.16, 0.0, 0.04, 0.0, 0.04, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.04, 0.56, 0.32, 0.0, 0.04, 0.0, 0.04], [0.04, 0.0, 0.0, 0.84, 0.12, 0.0, 0.0, 0.0, 0.0], [0.64, 0.0, 0.0, 0.0, 0.04, 0.28, 0.04, 0.0, 0.0], [0.48, 0.0, 0.0, 0.0, 0.04, 0.48, 0.0, 0.0, 0.0], [0.44, 0.0, 0.08, 0.0, 0.12, 0.32, 0.04, 0.0, 0.0], [0.0, 0.6, 0.16, 0.0, 0.08, 0.0, 0.16, 0.0, 0.0], [0.0, 0.0, 0.0, 0.4, 0.44, 0.04, 0.08, 0.0, 0.04], [0.32, 0.0, 0.04, 0.04, 0.24, 0.24, 0.08, 0.0, 0.04], [0.0, 0.08, 0.16, 0.16, 0.48, 0.0, 0.12, 0.0, 0.0], [0.24, 0.0, 0.28, 0.0, 0.08, 0.0, 0.36, 0.04, 0.0], [0.0, 0.0, 0.0, 0.84, 0.16, 0.0, 0.0, 0.0, 0.0], [0.12, 0.0, 0.2, 0.0, 0.44, 0.0, 0.16, 0.04, 0.04], [0.0, 0.04, 0.28, 0.0, 0.52, 0.0, 0.16, 0.0, 0.0], [0.04, 0.84, 0.08, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0], [0.36, 0.0, 0.28, 0.0, 0.12, 0.0, 0.2, 0.0, 0.04], [0.16, 0.04, 0.08, 0.16, 0.28, 0.16, 0.08, 0.0, 0.04], [0.0, 0.04, 0.12, 0.32, 0.44, 0.0, 0.08, 0.0, 0.0], [0.04, 0.28, 0.44, 0.0, 0.04, 0.04, 0.16, 0.0, 0.0], [0.88, 0.0, 0.0, 0.0, 0.0, 0.08, 0.04, 0.0, 0.0], [0.16, 0.0, 0.0, 0.12, 0.32, 0.36, 0.04, 0.0, 0.0], [0.8, 0.0, 0.0, 0.0, 0.04, 0.12, 0.04, 0.0, 0.0], [0.12, 0.0, 0.04, 0.12, 0.6, 0.04, 0.0, 0.0, 0.08], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08, 0.0, 0.04, 0.2, 0.28, 0.36, 0.04, 0.0, 0.0], [0.0, 0.0, 0.0, 0.68, 0.28, 0.0, 0.04, 0.0, 0.0], [0.16, 0.0, 0.12, 0.12, 0.32, 0.08, 0.16, 0.0, 0.04], [0.04, 0.0, 0.04, 0.6, 0.24, 0.04, 0.04, 0.0, 0.0], [0.56, 0.0, 0.16, 0.0, 0.08, 0.04, 0.16, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04, 0.0, 0.12, 0.48, 0.28, 0.0, 0.04, 0.0, 0.04], [0.0, 0.08, 0.12, 0.12, 0.64, 0.04, 0.0, 0.0, 0.0], [0.36, 0.0, 0.08, 0.0, 0.04, 0.44, 0.04, 0.04, 0.0], [0.48, 0.0, 0.04, 0.0, 0.04, 0.4, 0.04, 0.0, 0.0], [0.32, 0.04, 0.04, 0.04, 0.36, 0.04, 0.12, 0.0, 0.04], [0.12, 0.04, 0.12, 0.0, 0.4, 0.0, 0.32, 0.0, 0.0], [0.08, 0.0, 0.08, 0.08, 0.24, 0.4, 0.12, 0.0, 0.0], [0.28, 0.0, 0.0, 0.0, 0.08, 0.44, 0.16, 0.0, 0.04], [0.0, 0.36, 0.36, 0.0, 0.24, 0.0, 0.04, 0.0, 0.0], [0.08, 0.08, 0.44, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0], [0.6, 0.0, 0.0, 0.0, 0.04, 0.32, 0.04, 0.0, 0.0], [0.36, 0.0, 0.08, 0.0, 0.24, 0.28, 0.0, 0.0, 0.04], [0.52, 0.0, 0.0, 0.0, 0.0, 0.36, 0.08, 0.04, 0.0], [0.28, 0.0, 0.0, 0.04, 0.12, 0.48, 0.04, 0.0, 0.04], [0.0, 0.0, 0.0, 0.72, 0.2, 0.0, 0.04, 0.0, 0.04], [0.2, 0.24, 0.28, 0.0, 0.08, 0.04, 0.16, 0.0, 0.0], [0.0, 0.04, 0.28, 0.0, 0.48, 0.0, 0.12, 0.04, 0.04], [0.4, 0.0, 0.08, 0.08, 0.24, 0.04, 0.16, 0.0, 0.0], [0.6, 0.04, 0.12, 0.0, 0.08, 0.08, 0.08, 0.0, 0.0], [0.36, 0.04, 0.24, 0.0, 0.04, 0.2, 0.12, 0.0, 0.0], [0.2, 0.0, 0.28, 0.0, 0.28, 0.04, 0.2, 0.0, 0.0], [0.36, 0.0, 0.12, 0.04, 0.16, 0.0, 0.24, 0.04, 0.04], [0.88, 0.0, 0.0, 0.0, 0.0, 0.08, 0.04, 0.0, 0.0], [0.16, 0.0, 0.08, 0.08, 0.24, 0.44, 0.0, 0.0, 0.0], [0.68, 0.0, 0.0, 0.04, 0.0, 0.28, 0.0, 0.0, 0.0], [0.0, 0.72, 0.08, 0.0, 0.12, 0.0, 0.08, 0.0, 0.0], [0.0, 0.96, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0], [0.48, 0.0, 0.04, 0.0, 0.04, 0.36, 0.08, 0.0, 0.0], [0.2, 0.0, 0.12, 0.0, 0.08, 0.48, 0.12, 0.0, 0.0], [0.48, 0.0, 0.0, 0.0, 0.04, 0.44, 0.04, 0.0, 0.0], [0.44, 0.0, 0.0, 0.0, 0.04, 0.52, 0.0, 0.0, 0.0], [0.32, 0.12, 0.24, 0.0, 0.12, 0.0, 0.2, 0.0, 0.0], [0.0, 0.12, 0.28, 0.04, 0.44, 0.0, 0.08, 0.0, 0.04], [0.04, 0.16, 0.36, 0.04, 0.24, 0.0, 0.16, 0.0, 0.0], [0.0, 0.0, 0.0, 0.68, 0.28, 0.0, 0.04, 0.0, 0.0], [0.04, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04, 0.16, 0.24, 0.0, 0.28, 0.08, 0.2, 0.0, 0.0], [0.0, 0.0, 0.12, 0.12, 0.6, 0.0, 0.12, 0.0, 0.04], [0.24, 0.0, 0.12, 0.0, 0.32, 0.12, 0.12, 0.0, 0.08], [0.32, 0.0, 0.0, 0.0, 0.0, 0.68, 0.0, 0.0, 0.0], [0.96, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0], [0.0, 0.56, 0.32, 0.0, 0.04, 0.0, 0.08, 0.0, 0.0], [0.0, 0.0, 0.04, 0.72, 0.24, 0.0, 0.0, 0.0, 0.0], [0.04, 0.04, 0.32, 0.0, 0.4, 0.0, 0.2, 0.0, 0.0], [0.48, 0.0, 0.0, 0.0, 0.0, 0.44, 0.04, 0.04, 0.0], [0.96, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6, 0.0, 0.04, 0.0, 0.12, 0.24, 0.0, 0.0, 0.0], [0.76, 0.0, 0.08, 0.0, 0.08, 0.04, 0.04, 0.0, 0.0], [0.48, 0.0, 0.04, 0.0, 0.0, 0.48, 0.0, 0.0, 0.0], [0.0, 0.12, 0.36, 0.0, 0.4, 0.0, 0.08, 0.0, 0.04], [0.52, 0.0, 0.04, 0.0, 0.12, 0.28, 0.04, 0.0, 0.0], [0.0, 0.0, 0.12, 0.32, 0.52, 0.0, 0.04, 0.0, 0.0], [0.28, 0.0, 0.08, 0.0, 0.28, 0.12, 0.2, 0.0, 0.04], [0.0, 0.36, 0.36, 0.0, 0.24, 0.0, 0.04, 0.0, 0.0], [0.2, 0.0, 0.0, 0.16, 0.16, 0.48, 0.0, 0.0, 0.0], [0.72, 0.0, 0.04, 0.0, 0.0, 0.24, 0.0, 0.0, 0.0], [0.0, 0.0, 0.04, 0.16, 0.68, 0.0, 0.08, 0.0, 0.04], [0.0, 0.0, 0.0, 0.96, 0.04, 0.0, 0.0, 0.0, 0.0], [0.92, 0.0, 0.0, 0.0, 0.04, 0.0, 0.04, 0.0, 0.0], [0.0, 0.0, 0.28, 0.0, 0.56, 0.0, 0.08, 0.0, 0.08], [0.04, 0.44, 0.4, 0.0, 0.08, 0.0, 0.04, 0.0, 0.0], [0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0], [0.04, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.44, 0.0, 0.08, 0.0, 0.2, 0.24, 0.0, 0.0, 0.04], [0.44, 0.0, 0.04, 0.0, 0.16, 0.24, 0.08, 0.04, 0.0], [0.04, 0.04, 0.48, 0.0, 0.2, 0.0, 0.2, 0.0, 0.04], [0.92, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0], [0.48, 0.0, 0.0, 0.0, 0.04, 0.4, 0.08, 0.0, 0.0], [0.4, 0.0, 0.08, 0.0, 0.04, 0.4, 0.04, 0.04, 0.0], [0.08, 0.04, 0.36, 0.0, 0.36, 0.0, 0.16, 0.0, 0.0], [0.04, 0.0, 0.12, 0.36, 0.44, 0.0, 0.04, 0.0, 0.0], [0.44, 0.0, 0.0, 0.0, 0.0, 0.56, 0.0, 0.0, 0.0], [0.12, 0.0, 0.0, 0.28, 0.36, 0.16, 0.08, 0.0, 0.0], [0.0, 0.0, 0.0, 0.56, 0.4, 0.0, 0.04, 0.0, 0.0], [0.0, 0.4, 0.36, 0.0, 0.12, 0.0, 0.08, 0.0, 0.04], [0.04, 0.0, 0.0, 0.72, 0.2, 0.0, 0.04, 0.0, 0.0], [0.24, 0.16, 0.2, 0.0, 0.08, 0.08, 0.2, 0.04, 0.0], [0.48, 0.0, 0.2, 0.0, 0.12, 0.0, 0.16, 0.0, 0.04], [0.0, 0.32, 0.4, 0.04, 0.12, 0.0, 0.08, 0.0, 0.04], [0.04, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04, 0.0, 0.0, 0.52, 0.36, 0.0, 0.04, 0.0, 0.04], [0.0, 0.56, 0.24, 0.0, 0.12, 0.0, 0.08, 0.0, 0.0], [0.0, 0.0, 0.2, 0.08, 0.48, 0.04, 0.12, 0.0, 0.08], [0.04, 0.0, 0.08, 0.28, 0.36, 0.12, 0.08, 0.0, 0.04], [0.0, 0.96, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.08, 0.48, 0.4, 0.0, 0.04, 0.0, 0.0], [0.04, 0.0, 0.44, 0.0, 0.24, 0.0, 0.28, 0.0, 0.0], [0.12, 0.08, 0.16, 0.0, 0.36, 0.12, 0.16, 0.0, 0.0], [0.32, 0.0, 0.04, 0.0, 0.32, 0.28, 0.04, 0.0, 0.0], [0.04, 0.0, 0.0, 0.64, 0.28, 0.0, 0.04, 0.0, 0.0], [0.0, 0.4, 0.2, 0.0, 0.24, 0.0, 0.16, 0.0, 0.0], [0.04, 0.44, 0.32, 0.0, 0.08, 0.0, 0.12, 0.0, 0.0], [0.6, 0.0, 0.0, 0.0, 0.12, 0.24, 0.04, 0.0, 0.0], [0.8, 0.0, 0.08, 0.0, 0.04, 0.0, 0.08, 0.0, 0.0], [0.36, 0.0, 0.0, 0.0, 0.04, 0.6, 0.0, 0.0, 0.0], [0.12, 0.04, 0.32, 0.04, 0.2, 0.12, 0.12, 0.0, 0.04], [0.92, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0], [0.0, 0.24, 0.2, 0.08, 0.28, 0.0, 0.2, 0.0, 0.0], [0.32, 0.0, 0.0, 0.0, 0.04, 0.6, 0.0, 0.04, 0.0], [0.04, 0.68, 0.2, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0], [0.0, 0.16, 0.44, 0.0, 0.28, 0.0, 0.12, 0.0, 0.0], [0.04, 0.52, 0.32, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0], [0.0, 0.0, 0.0, 0.72, 0.28, 0.0, 0.0, 0.0, 0.0], [0.24, 0.12, 0.24, 0.0, 0.12, 0.0, 0.24, 0.0, 0.04], [0.28, 0.08, 0.28, 0.0, 0.16, 0.0, 0.16, 0.0, 0.04], [0.0, 0.84, 0.08, 0.04, 0.04, 0.0, 0.0, 0.0, 0.0], [0.0, 0.44, 0.16, 0.04, 0.28, 0.0, 0.08, 0.0, 0.0], [0.32, 0.0, 0.08, 0.0, 0.2, 0.24, 0.12, 0.0, 0.04], [0.0, 0.0, 0.04, 0.48, 0.4, 0.0, 0.04, 0.0, 0.04], [0.24, 0.0, 0.24, 0.0, 0.36, 0.0, 0.16, 0.0, 0.0], [0.24, 0.0, 0.44, 0.0, 0.0, 0.0, 0.28, 0.04, 0.0], [0.12, 0.32, 0.2, 0.0, 0.04, 0.12, 0.2, 0.0, 0.0], [0.0, 0.0, 0.08, 0.44, 0.36, 0.0, 0.08, 0.0, 0.04], [0.04, 0.0, 0.28, 0.0, 0.44, 0.0, 0.16, 0.04, 0.04], [0.08, 0.08, 0.28, 0.0, 0.12, 0.16, 0.24, 0.04, 0.0], [0.0, 0.28, 0.2, 0.04, 0.24, 0.04, 0.2, 0.0, 0.0], [0.0, 0.76, 0.12, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0], [0.12, 0.0, 0.12, 0.0, 0.32, 0.2, 0.2, 0.0, 0.04], [0.36, 0.0, 0.04, 0.0, 0.12, 0.36, 0.08, 0.0, 0.04], [0.0, 0.52, 0.2, 0.0, 0.2, 0.0, 0.08, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.32, 0.0, 0.08, 0.12, 0.32, 0.04, 0.12, 0.0, 0.0], [0.44, 0.0, 0.08, 0.0, 0.2, 0.0, 0.28, 0.0, 0.0], [0.28, 0.0, 0.0, 0.0, 0.0, 0.68, 0.04, 0.0, 0.0], [0.24, 0.28, 0.2, 0.0, 0.12, 0.04, 0.12, 0.0, 0.0], [0.4, 0.0, 0.0, 0.04, 0.0, 0.56, 0.0, 0.0, 0.0], [0.8, 0.0, 0.04, 0.0, 0.0, 0.08, 0.08, 0.0, 0.0], [0.04, 0.0, 0.0, 0.68, 0.2, 0.04, 0.04, 0.0, 0.0], [0.64, 0.0, 0.08, 0.0, 0.0, 0.2, 0.08, 0.0, 0.0], [0.0, 0.08, 0.12, 0.16, 0.6, 0.0, 0.0, 0.0, 0.04], [0.44, 0.0, 0.08, 0.0, 0.08, 0.36, 0.04, 0.0, 0.0], [0.08, 0.0, 0.0, 0.4, 0.32, 0.12, 0.04, 0.0, 0.04], [0.0, 0.84, 0.08, 0.0, 0.0, 0.0, 0.04, 0.0, 0.04], [0.0, 0.16, 0.48, 0.08, 0.2, 0.0, 0.08, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "#Number of training iterations\n",
    "num_iters = 5000\n",
    "#Size of training dataset\n",
    "num_examples = 2000\n",
    "#Listing out the shapes of each model\n",
    "colors_num = len(chip_norm[0])\n",
    "input_size = 3\n",
    "\n",
    "network_shapes = [(input_size, [100], colors_num), (input_size, [100], colors_num), (input_size, [100], colors_num)]\n",
    "#Learning rate of the network\n",
    "rate = 0.001\n",
    "\n",
    "#Generating Training Data\n",
    "lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1)\n",
    "\n",
    "input_train = torch.FloatTensor(lab_train)\n",
    "output_train = torch.FloatTensor(chip_train)\n",
    "input_test= torch.FloatTensor(lab_test)\n",
    "output_test = torch.FloatTensor(chip_test)\n",
    "\n",
    "print(chip_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network Shape:\n",
      "Input Size = 3\n",
      "Hidden Size = [100]\n",
      "Output Size = 9\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'#0 Error: 0.3937310576438904'"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network Shape:\n",
      "Input Size = 3\n",
      "Hidden Size = [100]\n",
      "Output Size = 9\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'#0 Error: 0.4112871289253235'"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network Shape:\n",
      "Input Size = 3\n",
      "Hidden Size = [100]\n",
      "Output Size = 9\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'#0 Error: 0.3904036581516266'"
     },
     "metadata": {}
    }
   ],
   "source": [
    "#Array of losses over training period for each network\n",
    "for net_num, shape in enumerate(network_shapes):\n",
    "    print('Network Shape:',flush = True)\n",
    "    print('Input Size = ' + str(shape[0]), flush = True)\n",
    "    print('Hidden Size = ' + str(shape[1]), flush = True)\n",
    "    print('Output Size = ' + str(shape[2]), flush = True)\n",
    "    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        #Calculating l1 error\n",
    "        error = NN.l1error(output_train, NN(input_train))\n",
    "        if i == 0: \n",
    "            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n",
    "        else:\n",
    "            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n",
    "            \n",
    "        NN.train(input_train, output_train)\n",
    "    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n",
    "    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network Shape:\n",
      "Input Size = 3\n",
      "Hidden Size = [100]\n",
      "Output Size = 9\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Neural_Network:\n\tUnexpected key(s) in state_dict: \"model.4.weight\", \"model.4.bias\", \"model.6.weight\", \"model.6.bias\". \n\tsize mismatch for model.0.weight: copying a param with shape torch.Size([3, 3]) from checkpoint, the shape in current model is torch.Size([100, 3]).\n\tsize mismatch for model.0.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for model.2.weight: copying a param with shape torch.Size([3, 3]) from checkpoint, the shape in current model is torch.Size([9, 100]).\n\tsize mismatch for model.2.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([9]).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-70ff175db6a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n\u001b[1;32m      9\u001b[0m                         hiddenSize = shape[1] , learning_rate = rate)\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved_networks/Net \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1045\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Neural_Network:\n\tUnexpected key(s) in state_dict: \"model.4.weight\", \"model.4.bias\", \"model.6.weight\", \"model.6.bias\". \n\tsize mismatch for model.0.weight: copying a param with shape torch.Size([3, 3]) from checkpoint, the shape in current model is torch.Size([100, 3]).\n\tsize mismatch for model.0.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for model.2.weight: copying a param with shape torch.Size([3, 3]) from checkpoint, the shape in current model is torch.Size([9, 100]).\n\tsize mismatch for model.2.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([9])."
     ]
    }
   ],
   "source": [
    "for net_num, shape in enumerate(network_shapes):\n",
    "    print('Network Shape:',flush = True)\n",
    "    print('Input Size = ' + str(shape[0]), flush = True)\n",
    "    print('Hidden Size = ' + str(shape[1]), flush = True)\n",
    "    print('Output Size = ' + str(shape[2]), flush = True)\n",
    "    # Loading the network we trained in the prev. section\n",
    "    \n",
    "    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n",
    "                        hiddenSize = shape[1] , learning_rate = rate)\n",
    "    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n",
    "    \n",
    "    validation_error = NN.l1error(output_test, NN(input_test))\n",
    "    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n",
    "    print(\"The validation error is: \" + str(validation_error), flush = True)  \n",
    "    print(\"The maximum error is:\" + str(max_error), flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "277px",
    "left": "1061px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}